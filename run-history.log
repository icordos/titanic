  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=200 -> avg_log_loss=0.4097 (folds: loss=0.4133/acc=0.7920, loss=0.4144/acc=0.7839, loss=0.3756/acc=0.8172, loss=0.4207/acc=0.7943, loss=0.4201/acc=0.8000, loss=0.3978/acc=0.8182, loss=0.3973/
acc=0.8055, loss=0.4234/acc=0.7952, loss=0.4122/acc=0.8076, loss=0.4221/acc=0.7972)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4053 (folds: loss=0.4123/acc=0.8034, loss=0.4027/acc=0.7862, loss=0.3765/acc=0.8149, loss=0.4258/acc=0.8103, loss=0.4141/acc=0.8023, loss=0.3938/acc
=0.8193, loss=0.3885/acc=0.8147, loss=0.4171/acc=0.8021, loss=0.4083/acc=0.8007, loss=0.4138/acc=0.8076)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4020 (folds: loss=0.4093/acc=0.8000, loss=0.4004/acc=0.7908, loss=0.3711/acc=0.8161, loss=0.4201/acc=0.8069, loss=0.4100/acc=0.8034, loss=0.3915/acc
=0.8205, loss=0.3856/acc=0.8113, loss=0.4150/acc=0.7998, loss=0.4073/acc=0.7984, loss=0.4097/acc=0.8134)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3961 (folds: loss=0.4031/acc=0.8000, loss=0.3944/acc=0.7908, loss=0.3647/acc=0.8115, loss=0.4153/acc=0.8092, loss=0.4037/acc=0.8092, loss=0.3855/acc
=0.8193, loss=0.3810/acc=0.8159, loss=0.4088/acc=0.8055, loss=0.4041/acc=0.8018, loss=0.4006/acc=0.8260)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4148 (folds: loss=0.4206/acc=0.7966, loss=0.4126/acc=0.7862, loss=0.3878/acc=0.8161, loss=0.4345/acc=0.8023, loss=0.4253/acc=0.7897, loss=0.4043/acc
=0.8113, loss=0.3959/acc=0.8101, loss=0.4269/acc=0.7940, loss=0.4159/acc=0.8007, loss=0.4239/acc=0.8030)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.3919 (folds: loss=0.3971/acc=0.8011, loss=0.3910/acc=0.7908, loss=0.3594/acc=0.8138, loss=0.4111/acc=0.8126, loss=0.4021/acc=0.8069, loss=0.3817/acc
=0.8124, loss=0.3768/acc=0.8205, loss=0.4027/acc=0.8055, loss=0.4005/acc=0.8030, loss=0.3968/acc=0.8099)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4045 (folds: loss=0.4117/acc=0.7989, loss=0.4028/acc=0.7897, loss=0.3749/acc=0.8161, loss=0.4235/acc=0.8103, loss=0.4138/acc=0.7989, loss=0.3938/acc
=0.8216, loss=0.3876/acc=0.8136, loss=0.4160/acc=0.7975, loss=0.4088/acc=0.8007, loss=0.4121/acc=0.8122)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4050 (folds: loss=0.4116/acc=0.8011, loss=0.4032/acc=0.7885, loss=0.3755/acc=0.8161, loss=0.4246/acc=0.8092, loss=0.4138/acc=0.8023, loss=0.3944/acc
=0.8205, loss=0.3874/acc=0.8101, loss=0.4165/acc=0.7986, loss=0.4093/acc=0.8018, loss=0.4140/acc=0.8088)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4022 (folds: loss=0.4084/acc=0.7977, loss=0.4009/acc=0.7874, loss=0.3710/acc=0.8138, loss=0.4200/acc=0.8126, loss=0.4105/acc=0.8057, loss=0.3918/acc
=0.8193, loss=0.3853/acc=0.8136, loss=0.4157/acc=0.7998, loss=0.4086/acc=0.8030, loss=0.4099/acc=0.8168)
  [gbstump] gbstump estimators=100 lr=0.1 min_leaf=100 -> avg_log_loss=0.4133 (folds: loss=0.4166/acc=0.7897, loss=0.4158/acc=0.7828, loss=0.3818/acc=0.8161, loss=0.4236/acc=0.7908, loss=0.4231/acc=0.7977, loss=0.4048/acc=0.8170, loss=0.4002/
acc=0.8067, loss=0.4273/acc=0.7940, loss=0.4152/acc=0.8099, loss=0.4249/acc=0.7995)
  [extratrees] extratrees trees=100 depth=8 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.3910 (folds: loss=0.3990/acc=0.7989, loss=0.3911/acc=0.7920, loss=0.3570/acc=0.8149, loss=0.4093/acc=0.8080, loss=0.3993/acc=0.8057, loss=0.3807/acc
=0.8182, loss=0.3773/acc=0.8159, loss=0.3993/acc=0.8113, loss=0.4024/acc=0.7926, loss=0.3946/acc=0.8168)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.3985 (folds: loss=0.4041/acc=0.8011, loss=0.3975/acc=0.7908, loss=0.3658/acc=0.8161, loss=0.4183/acc=0.8069, loss=0.4074/acc=0.8034, loss=0.3881/acc
=0.8182, loss=0.3826/acc=0.8147, loss=0.4103/acc=0.8078, loss=0.4051/acc=0.7984, loss=0.4056/acc=0.8134)
  [extratrees] extratrees trees=400 depth=6 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.3950 (folds: loss=0.4012/acc=0.7989, loss=0.3943/acc=0.7943, loss=0.3614/acc=0.8161, loss=0.4129/acc=0.8092, loss=0.4030/acc=0.8103, loss=0.3839/acc
=0.8193, loss=0.3807/acc=0.8124, loss=0.4085/acc=0.8032, loss=0.4049/acc=0.8007, loss=0.3993/acc=0.8249)
  [rf] rf trees=400 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4080 (folds: loss=0.4114/acc=0.7954, loss=0.4113/acc=0.7816, loss=0.3721/acc=0.8161, loss=0.4242/acc=0.7874, loss=0.4148/acc=0.8011, loss=0.3993/acc=0.8101, loss=0.
3952/acc=0.8021, loss=0.4214/acc=0.7929, loss=0.4130/acc=0.8065, loss=0.4173/acc=0.8007)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.3915 (folds: loss=0.3973/acc=0.8023, loss=0.3911/acc=0.7885, loss=0.3587/acc=0.8172, loss=0.4106/acc=0.8138, loss=0.4010/acc=0.8092, loss=0.3812/acc
=0.8159, loss=0.3772/acc=0.8205, loss=0.4011/acc=0.8090, loss=0.4012/acc=0.8007, loss=0.3960/acc=0.8168)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4004 (folds: loss=0.4070/acc=0.7989, loss=0.3990/acc=0.7931, loss=0.3674/acc=0.8126, loss=0.4184/acc=0.8080, loss=0.4079/acc=0.8092, loss=0.3903/acc
=0.8182, loss=0.3851/acc=0.8136, loss=0.4130/acc=0.8021, loss=0.4085/acc=0.7995, loss=0.4075/acc=0.8203)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4010 (folds: loss=0.4079/acc=0.7989, loss=0.3998/acc=0.7897, loss=0.3680/acc=0.8218, loss=0.4183/acc=0.8138, loss=0.4093/acc=0.8080, loss=0.3899/acc
=0.8170, loss=0.3858/acc=0.8090, loss=0.4146/acc=0.8009, loss=0.4091/acc=0.7972, loss=0.4077/acc=0.8134)
  [mlp] layers=[64, 32, 128, 64, 16, 256] act=gelu lr=0.0047 batch=128 epochs=10 -> avg_log_loss=0.3994 (folds: loss=0.4174/acc=0.7862, loss=0.3869/acc=0.7920, loss=0.3672/acc=0.8230, loss=0.4238/acc=0.7897, loss=0.4091/acc=0.7943, loss=0.381
8/acc=0.8136, loss=0.3849/acc=0.8009, loss=0.4136/acc=0.8090, loss=0.4099/acc=0.8018, loss=0.3998/acc=0.8180)
  [gbstump] gbstump estimators=150 lr=0.3 min_leaf=50 -> avg_log_loss=0.4109 (folds: loss=0.4167/acc=0.7851, loss=0.4174/acc=0.7851, loss=0.3746/acc=0.8092, loss=0.4210/acc=0.7977, loss=0.4204/acc=0.7920, loss=0.3965/acc=0.8147, loss=0.4018/a
cc=0.8032, loss=0.4243/acc=0.7986, loss=0.4123/acc=0.8134, loss=0.4235/acc=0.8007)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.3926 (folds: loss=0.4006/acc=0.8023, loss=0.3914/acc=0.7920, loss=0.3574/acc=0.8138, loss=0.4114/acc=0.8103, loss=0.4011/acc=0.8080, loss=0.3826/acc
=0.8159, loss=0.3790/acc=0.8216, loss=0.4048/acc=0.8067, loss=0.4008/acc=0.8076, loss=0.3974/acc=0.8180)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4007 (folds: loss=0.4082/acc=0.7989, loss=0.3978/acc=0.7943, loss=0.3694/acc=0.8161, loss=0.4184/acc=0.8115, loss=0.4078/acc=0.8034, loss=0.3903/acc
=0.8193, loss=0.3865/acc=0.8136, loss=0.4137/acc=0.8044, loss=0.4064/acc=0.8041, loss=0.4089/acc=0.8157)
  best avg log loss this gen: 0.3887
Generation 40/40
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.3887 (folds: loss=0.3950/acc=0.7977, loss=0.3882/acc=0.7966, loss=0.3536/acc=0.8195, loss=0.4066/acc=0.8080, loss=0.3981/acc=0.8161, loss=0.3772/acc
=0.8205, loss=0.3757/acc=0.8147, loss=0.3995/acc=0.8113, loss=0.3993/acc=0.7995, loss=0.3938/acc=0.8168)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3889 (folds: loss=0.3950/acc=0.7977, loss=0.3889/acc=0.7989, loss=0.3535/acc=0.8195, loss=0.4073/acc=0.8161, loss=0.3981/acc=0.8149, loss=0.3791/acc
=0.8170, loss=0.3753/acc=0.8193, loss=0.3998/acc=0.8090, loss=0.3984/acc=0.7984, loss=0.3936/acc=0.8099)
  [rf] rf trees=200 depth=6 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4004 (folds: loss=0.4053/acc=0.7920, loss=0.4052/acc=0.7874, loss=0.3634/acc=0.8184, loss=0.4154/acc=0.7908, loss=0.4049/acc=0.8057, loss=0.3908/acc=0.8136, loss=0.
3888/acc=0.8067, loss=0.4127/acc=0.7952, loss=0.4061/acc=0.7995, loss=0.4114/acc=0.8065)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.3950 (folds: loss=0.4022/acc=0.8000, loss=0.3937/acc=0.7966, loss=0.3619/acc=0.8138, loss=0.4127/acc=0.8103, loss=0.4028/acc=0.8092, loss=0.3834/acc
=0.8182, loss=0.3807/acc=0.8147, loss=0.4078/acc=0.8032, loss=0.4039/acc=0.7984, loss=0.4013/acc=0.8214)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4147 (folds: loss=0.4205/acc=0.7943, loss=0.4118/acc=0.7862, loss=0.3872/acc=0.8138, loss=0.4339/acc=0.8011, loss=0.4262/acc=0.7908, loss=0.4037/acc
=0.8124, loss=0.3969/acc=0.8090, loss=0.4274/acc=0.7906, loss=0.4154/acc=0.8007, loss=0.4243/acc=0.8018)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4014 (folds: loss=0.4070/acc=0.7989, loss=0.4005/acc=0.7931, loss=0.3692/acc=0.8184, loss=0.4170/acc=0.8080, loss=0.4100/acc=0.8046, loss=0.3913/acc
=0.8228, loss=0.3876/acc=0.8101, loss=0.4132/acc=0.8067, loss=0.4085/acc=0.8007, loss=0.4093/acc=0.8203)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4018 (folds: loss=0.4084/acc=0.8011, loss=0.4000/acc=0.7885, loss=0.3713/acc=0.8172, loss=0.4208/acc=0.8080, loss=0.4091/acc=0.8023, loss=0.3916/acc
=0.8193, loss=0.3859/acc=0.8147, loss=0.4142/acc=0.8021, loss=0.4078/acc=0.8007, loss=0.4084/acc=0.8249)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.3929 (folds: loss=0.3996/acc=0.7943, loss=0.3930/acc=0.7931, loss=0.3587/acc=0.8126, loss=0.4103/acc=0.8115, loss=0.4012/acc=0.8103, loss=0.3834/acc
=0.8147, loss=0.3793/acc=0.8159, loss=0.4035/acc=0.8055, loss=0.4019/acc=0.8007, loss=0.3985/acc=0.8134)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.3919 (folds: loss=0.3967/acc=0.8000, loss=0.3899/acc=0.7977, loss=0.3572/acc=0.8172, loss=0.4113/acc=0.8069, loss=0.4003/acc=0.8138, loss=0.3819/acc
=0.8124, loss=0.3788/acc=0.8251, loss=0.4028/acc=0.8124, loss=0.4030/acc=0.7995, loss=0.3973/acc=0.8191)
  [rf] rf trees=300 depth=4 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4079 (folds: loss=0.4111/acc=0.7943, loss=0.4117/acc=0.7816, loss=0.3722/acc=0.8138, loss=0.4237/acc=0.7931, loss=0.4147/acc=0.8069, loss=0.3990/acc=0.8090, loss=0.
3950/acc=0.8032, loss=0.4208/acc=0.7963, loss=0.4125/acc=0.8088, loss=0.4181/acc=0.8018)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4022 (folds: loss=0.4098/acc=0.8034, loss=0.4007/acc=0.7943, loss=0.3708/acc=0.8207, loss=0.4203/acc=0.8080, loss=0.4106/acc=0.8034, loss=0.3914/acc
=0.8193, loss=0.3866/acc=0.8124, loss=0.4143/acc=0.8009, loss=0.4077/acc=0.8041, loss=0.4103/acc=0.8180)
  [extratrees] extratrees trees=200 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4080 (folds: loss=0.4140/acc=0.7931, loss=0.4069/acc=0.7885, loss=0.3775/acc=0.8184, loss=0.4251/acc=0.8046, loss=0.4159/acc=0.7977, loss=0.3972/acc
=0.8136, loss=0.3919/acc=0.8101, loss=0.4209/acc=0.7986, loss=0.4135/acc=0.8007, loss=0.4171/acc=0.8076)
  [mlp] layers=[16, 16] act=relu lr=0.0050 batch=256 epochs=20 -> avg_log_loss=0.4028 (folds: loss=0.4132/acc=0.7828, loss=0.3978/acc=0.7966, loss=0.3750/acc=0.8069, loss=0.4165/acc=0.7954, loss=0.4059/acc=0.8080, loss=0.3909/acc=0.7940, loss
=0.3894/acc=0.8113, loss=0.4120/acc=0.8009, loss=0.4253/acc=0.7915, loss=0.4026/acc=0.8099)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.3940 (folds: loss=0.3985/acc=0.7966, loss=0.3931/acc=0.7920, loss=0.3621/acc=0.8115, loss=0.4137/acc=0.8103, loss=0.4040/acc=0.8080, loss=0.3853/acc
=0.8170, loss=0.3790/acc=0.8205, loss=0.4056/acc=0.8067, loss=0.4002/acc=0.8018, loss=0.3987/acc=0.8122)
  [extratrees] extratrees trees=100 depth=6 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.3956 (folds: loss=0.4035/acc=0.7989, loss=0.3952/acc=0.7977, loss=0.3630/acc=0.8172, loss=0.4137/acc=0.8092, loss=0.4043/acc=0.8080, loss=0.3835/acc
=0.8170, loss=0.3813/acc=0.8124, loss=0.4072/acc=0.8078, loss=0.4040/acc=0.7995, loss=0.4008/acc=0.8214)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.3931 (folds: loss=0.3995/acc=0.8023, loss=0.3939/acc=0.7862, loss=0.3596/acc=0.8138, loss=0.4130/acc=0.8080, loss=0.4015/acc=0.8080, loss=0.3834/acc
=0.8170, loss=0.3789/acc=0.8216, loss=0.4034/acc=0.8067, loss=0.4016/acc=0.8018, loss=0.3960/acc=0.8145)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4009 (folds: loss=0.4072/acc=0.7977, loss=0.3996/acc=0.7908, loss=0.3690/acc=0.8161, loss=0.4185/acc=0.8103, loss=0.4086/acc=0.8080, loss=0.3904/acc
=0.8205, loss=0.3869/acc=0.8113, loss=0.4127/acc=0.8055, loss=0.4082/acc=0.7995, loss=0.4075/acc=0.8168)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.3941 (folds: loss=0.3994/acc=0.8057, loss=0.3944/acc=0.7908, loss=0.3620/acc=0.8126, loss=0.4121/acc=0.8092, loss=0.4035/acc=0.8069, loss=0.3841/acc
=0.8182, loss=0.3782/acc=0.8170, loss=0.4060/acc=0.8067, loss=0.4010/acc=0.7995, loss=0.4005/acc=0.8203)
  [rf] rf trees=300 depth=5 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4038 (folds: loss=0.4096/acc=0.7989, loss=0.4078/acc=0.7839, loss=0.3685/acc=0.8184, loss=0.4197/acc=0.7908, loss=0.4087/acc=0.8023, loss=0.3942/acc=0.8078, loss=0.
3909/acc=0.8067, loss=0.4165/acc=0.7975, loss=0.4102/acc=0.8041, loss=0.4118/acc=0.8076)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.3934 (folds: loss=0.3999/acc=0.8000, loss=0.3937/acc=0.7931, loss=0.3607/acc=0.8126, loss=0.4113/acc=0.8092, loss=0.4013/acc=0.8023, loss=0.3821/acc
=0.8205, loss=0.3792/acc=0.8182, loss=0.4042/acc=0.8067, loss=0.4043/acc=0.8007, loss=0.3977/acc=0.8168)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4148 (folds: loss=0.4200/acc=0.7989, loss=0.4129/acc=0.7874, loss=0.3883/acc=0.8092, loss=0.4342/acc=0.8000, loss=0.4240/acc=0.7966, loss=0.4042/acc
=0.8136, loss=0.3961/acc=0.8101, loss=0.4266/acc=0.7929, loss=0.4169/acc=0.7984, loss=0.4248/acc=0.8018)
  [extratrees] extratrees trees=100 depth=6 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.3981 (folds: loss=0.4029/acc=0.8011, loss=0.3970/acc=0.7885, loss=0.3657/acc=0.8161, loss=0.4173/acc=0.8126, loss=0.4081/acc=0.8034, loss=0.3876/acc
=0.8251, loss=0.3815/acc=0.8159, loss=0.4106/acc=0.8009, loss=0.4050/acc=0.8018, loss=0.4053/acc=0.8157)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4008 (folds: loss=0.4068/acc=0.7989, loss=0.3989/acc=0.7954, loss=0.3687/acc=0.8184, loss=0.4185/acc=0.8080, loss=0.4083/acc=0.8069, loss=0.3900/acc
=0.8216, loss=0.3861/acc=0.8090, loss=0.4142/acc=0.8067, loss=0.4079/acc=0.7995, loss=0.4083/acc=0.8157)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4106 (folds: loss=0.4170/acc=0.7954, loss=0.4083/acc=0.7885, loss=0.3814/acc=0.8149, loss=0.4299/acc=0.8034, loss=0.4197/acc=0.7954, loss=0.3983/acc
=0.8159, loss=0.3935/acc=0.8101, loss=0.4242/acc=0.7917, loss=0.4139/acc=0.7984, loss=0.4192/acc=0.8018)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.3892 (folds: loss=0.3962/acc=0.7966, loss=0.3898/acc=0.7989, loss=0.3524/acc=0.8195, loss=0.4073/acc=0.8115, loss=0.3997/acc=0.8115, loss=0.3773/acc
=0.8159, loss=0.3756/acc=0.8205, loss=0.4007/acc=0.8101, loss=0.3996/acc=0.7984, loss=0.3928/acc=0.8168)
  [extratrees] extratrees trees=100 depth=6 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.3950 (folds: loss=0.4009/acc=0.7966, loss=0.3948/acc=0.7931, loss=0.3626/acc=0.8126, loss=0.4122/acc=0.8126, loss=0.4040/acc=0.8080, loss=0.3852/acc
=0.8113, loss=0.3813/acc=0.8136, loss=0.4064/acc=0.8067, loss=0.4033/acc=0.7984, loss=0.3996/acc=0.8180)
  [gbstump] gbstump estimators=150 lr=0.1 min_leaf=150 -> avg_log_loss=0.4109 (folds: loss=0.4142/acc=0.7977, loss=0.4141/acc=0.7839, loss=0.3781/acc=0.8126, loss=0.4211/acc=0.7954, loss=0.4214/acc=0.7954, loss=0.4007/acc=0.8182, loss=0.3982/
acc=0.8055, loss=0.4247/acc=0.7940, loss=0.4135/acc=0.8088, loss=0.4232/acc=0.8007)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3925 (folds: loss=0.4007/acc=0.7943, loss=0.3918/acc=0.7943, loss=0.3592/acc=0.8115, loss=0.4113/acc=0.8069, loss=0.4020/acc=0.8126, loss=0.3806/acc
=0.8170, loss=0.3765/acc=0.8216, loss=0.4025/acc=0.8055, loss=0.4023/acc=0.7961, loss=0.3983/acc=0.8145)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4081 (folds: loss=0.4142/acc=0.7954, loss=0.4064/acc=0.7874, loss=0.3781/acc=0.8172, loss=0.4260/acc=0.8057, loss=0.4156/acc=0.7977, loss=0.3971/acc
=0.8159, loss=0.3928/acc=0.8078, loss=0.4214/acc=0.7963, loss=0.4125/acc=0.8018, loss=0.4174/acc=0.8041)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4016 (folds: loss=0.4083/acc=0.8011, loss=0.3994/acc=0.7920, loss=0.3717/acc=0.8126, loss=0.4204/acc=0.8057, loss=0.4100/acc=0.8034, loss=0.3904/acc
=0.8228, loss=0.3851/acc=0.8090, loss=0.4147/acc=0.8021, loss=0.4079/acc=0.7984, loss=0.4085/acc=0.8145)
  best avg log loss this gen: 0.3887
Retraining top genome #1 (extratrees) with val_log_loss=0.3887
Saved model to models/ga_nn_1_1768344940.pt and submission to submissions/ga_nn_1_1768344940.csv
Retraining top genome #2 (gbstump) with val_log_loss=0.3887
Saved model to models/ga_nn_2_1768344941.pt and submission to submissions/ga_nn_2_1768344941.csv
Retraining top genome #3 (extratrees) with val_log_loss=0.3887
Saved model to models/ga_nn_3_1768344941.pt and submission to submissions/ga_nn_3_1768344941.csv
Retraining top genome #4 (extratrees) with val_log_loss=0.3887
Saved model to models/ga_nn_4_1768344941.pt and submission to submissions/ga_nn_4_1768344941.csv
Retraining top genome #5 (gbstump) with val_log_loss=0.3887
Saved model to models/ga_nn_5_1768344943.pt and submission to submissions/ga_nn_5_1768344943.csv
Retraining top genome #6 (extratrees) with val_log_loss=0.3887
Saved model to models/ga_nn_6_1768344943.pt and submission to submissions/ga_nn_6_1768344943.csv
Retraining top genome #7 (extratrees) with val_log_loss=0.3889
Saved model to models/ga_nn_7_1768344943.pt and submission to submissions/ga_nn_7_1768344943.csv
Retraining top genome #8 (extratrees) with val_log_loss=0.3889
Saved model to models/ga_nn_8_1768344944.pt and submission to submissions/ga_nn_8_1768344944.csv
Retraining top genome #9 (extratrees) with val_log_loss=0.3889
Saved model to models/ga_nn_9_1768344944.pt and submission to submissions/ga_nn_9_1768344944.csv
Retraining top genome #10 (rf) with val_log_loss=0.3889
Saved model to models/ga_nn_10_1768344944.pt and submission to submissions/ga_nn_10_1768344944.csv
Backed up previous summary to models/ga_search_summary_20260113-225544.json
Wrote GA summary to models/ga_search_summary.json

real    123m56.087s
user    419m51.341s
sys     10m48.730s
root@6a432f30b2e3:/workspace# pwd
/workspace
root@6a432f30b2e3:/workspace# ls
AGENTS.md  README.md  data  models  notebooks  report  requirements.txt  results  run_all.sh  scripts  specs  submissions
root@6a432f30b2e3:/workspace#   kaggle competitions submit -c spaceship-titanic \
    -f submissions/ga_nn_1_1768344940.csv \
    -m "GA rank 1 ts 1768344940 logloss 0.3887"

  kaggle competitions submit -c spaceship-titanic \
    -f submissions/ga_nn_2_1768344941.csv \
    -m "GA rank 2 ts 1768344941 logloss 0.3887"

  kaggle competitions submit -c spaceship-titanic \
    -f submissions/ga_nn_3_1768344941.csv \
    -m "GA rank 3 ts 1768344941 logloss 0.3887"

  kaggle competitions submit -c spaceship-titanic \
    -f submissions/ga_nn_4_1768344941.csv \
    -m "GA rank 4 ts 1768344941 logloss 0.3887"

  kaggle competitions submit -c spaceship-titanic \
    -f submissions/ga_nn_5_1768344943.csv \
    -m "GA rank 5 ts 1768344943 logloss 0.3887"
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.3k/56.3k [00:00<00:00, 89.0kB/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.3k/56.3k [00:00<00:00, 83.6kB/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.3k/56.3k [00:00<00:00, 98.9kB/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.3k/56.3k [00:00<00:00, 96.4kB/s]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.3k/56.3k [00:00<00:00, 98.5kB/s]
Successfully submitted to Spackaggle competitions submissions -c spaceship-titanicmpetitions submissions -c spaceship-titanic
Traceback (most recent call last):
  File "/usr/local/bin/kaggle", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/kaggle/cli.py", line 70, in main
    out = args.func(**command_args)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py", line 1471, in competition_submissions_cli
    submissions = self.competition_submissions(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: KaggleApi.competition_submissions() got an unexpected keyword argument 'page_number'
root@6a432f30b2e3:/workspace# git pull
fatal: detected dubious ownership in repository at '/workspace'
To add an exception for this directory, call:

        git config --global --add safe.directory /workspace
root@6a432f30b2e3:/workspace# ls
AGENTS.md  README.md  data  models  notebooks  report  requirements.txt  results  run_all.sh  scripts  specs  submissions
root@6a432f30b2e3:/workspace# scripts/run_day1_day2.py --stack-folds 5 --stack-top-n 12 --ga-generations 15 --ga-population 15 --ga-cv-folds 8 --ga-top-k 3
[day12] $ python scripts/prepare_data.py --input data/train.csv --output data/train_prepared.csv --exclude-columns Transported
Wrote processed data to data/train_prepared.csv
[day12] $ python scripts/prepare_data.py --input data/test.csv --output data/test_prepared.csv
Wrote processed data to data/test_prepared.csv
[day12] $ python scripts/add_baseline_predictions.py --train data/train_prepared.csv --test data/test_prepared.csv --target-column Transported
[baseline] Training logistic regression on device cpu.
[baseline] Training gradient boosting stumps.
Wrote augmented training data to data/train_prepared.csv
Wrote augmented test data to data/test_prepared.csv
[day12] $ python scripts/select_top_submissions.py --summaries-dir models --pattern ga_search_summary*.json --top 12 --output models/day1_top_configs.json --include-missing
Wrote top 11 entries to models/day1_top_configs.json
[day12] $ python scripts/build_stack_meta.py --train data/train_prepared.csv --test data/test_prepared.csv --raw-test data/test.csv --top-configs models/day1_top_configs.json --folds 5 --seed 2025 --output-dir models/day1_stack
[stack] Loaded 11 configs from models/day1_top_configs.json
[stack] Generating OOF predictions for ga_rank_1_ts_1768344940_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_2_ts_1768344941_logloss_0_3887 (gbstump)
[stack] Generating OOF predictions for ga_rank_3_ts_1768344941_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_4_ts_1768344941_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_5_ts_1768344943_logloss_0_3887 (gbstump)
[stack] Generating OOF predictions for ga_rank_6_ts_1768344943_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_7_ts_1768344943_logloss_0_3889 (extratrees)
[stack] Generating OOF predictions for ga_rank_8_ts_1768344944_logloss_0_3889 (extratrees)
[stack] Generating OOF predictions for ga_rank_9_ts_1768344944_logloss_0_3889 (extratrees)
[stack] Generating OOF predictions for ga_rank_10_ts_1768344944_logloss_0_3889 (rf)
[stack] Generating OOF predictions for ga_rank_1_ts_1768337411_logloss_0_3960 (extratrees)
[stack] Saved train features to models/day1_stack/stack_train_features.csv
[stack] Saved test features to models/day1_stack/stack_test_features.csv
[stack] Saved meta-model to models/day1_stack/stack_meta_model.joblib
[stack] Saved submission preview to models/day1_stack/stack_meta_submission.csv
[day12] $ python scripts/train_ga_nn.py --train-prepared data/train_prepared.csv --test-prepared data/test_prepared.csv --raw-test data/test.csv --generations 15 --population 15 --top-k 3 --cv-folds 8 --resume-summary models/day1_top_configs.
json --resume-top-n 12 --seed 1347 --no-kaggle
Using stratified 8-fold CV for GA fitness.
Using device: cpu
Loaded 11 genomes from models/day1_top_configs.json to seed the population.
Generation 1/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4008 (folds: loss=0.3831/acc=0.8382, loss=0.3827/acc=0.8070, loss=0.4065/acc=0.8040, loss=0.3941/acc=0.8020, loss=0.3796/acc=0.8223, loss=0.4082/acc
=0.8066, loss=0.4473/acc=0.7772, loss=0.4049/acc=0.7901)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=50 -> avg_log_loss=0.4399 (folds: loss=0.4307/acc=0.7987, loss=0.4250/acc=0.7941, loss=0.4471/acc=0.8096, loss=0.4263/acc=0.8029, loss=0.4289/acc=0.8140, loss=0.4444/acc=0.7855, loss=0.4763/a
cc=0.7634, loss=0.4402/acc=0.7753)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4065 (folds: loss=0.3924/acc=0.8244, loss=0.3867/acc=0.8116, loss=0.4129/acc=0.8096, loss=0.4005/acc=0.8085, loss=0.3773/acc=0.8177, loss=0.4138/acc
=0.8002, loss=0.4557/acc=0.7698, loss=0.4124/acc=0.7901)
  [extratrees] extratrees trees=300 depth=7 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4025 (folds: loss=0.3881/acc=0.8290, loss=0.3849/acc=0.8097, loss=0.4087/acc=0.8068, loss=0.3958/acc=0.8039, loss=0.3762/acc=0.8168, loss=0.4103/acc
=0.8020, loss=0.4484/acc=0.7744, loss=0.4072/acc=0.7965)
  [gbstump] gbstump estimators=150 lr=0.2 min_leaf=150 -> avg_log_loss=0.4399 (folds: loss=0.4248/acc=0.8070, loss=0.4260/acc=0.7914, loss=0.4508/acc=0.8022, loss=0.4262/acc=0.8029, loss=0.4302/acc=0.8076, loss=0.4428/acc=0.7901, loss=0.4743/
acc=0.7698, loss=0.4436/acc=0.7827)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4142 (folds: loss=0.4012/acc=0.8153, loss=0.3943/acc=0.8125, loss=0.4223/acc=0.8077, loss=0.4070/acc=0.8066, loss=0.3852/acc=0.8223, loss=0.4195/acc
=0.8002, loss=0.4642/acc=0.7597, loss=0.4195/acc=0.7891)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4003 (folds: loss=0.3835/acc=0.8392, loss=0.3832/acc=0.8143, loss=0.4056/acc=0.8086, loss=0.3942/acc=0.8029, loss=0.3773/acc=0.8241, loss=0.4082/acc
=0.8057, loss=0.4466/acc=0.7762, loss=0.4037/acc=0.7928)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4044 (folds: loss=0.3899/acc=0.8300, loss=0.3853/acc=0.8116, loss=0.4112/acc=0.8077, loss=0.3973/acc=0.8085, loss=0.3778/acc=0.8158, loss=0.4111/acc
=0.7993, loss=0.4528/acc=0.7726, loss=0.4098/acc=0.7937)
  [extratrees] extratrees trees=200 depth=4 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4157 (folds: loss=0.4027/acc=0.8180, loss=0.3964/acc=0.8079, loss=0.4229/acc=0.8086, loss=0.4095/acc=0.8076, loss=0.3868/acc=0.8195, loss=0.4199/acc
=0.7937, loss=0.4662/acc=0.7597, loss=0.4209/acc=0.7882)
  [rf] rf trees=200 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4341 (folds: loss=0.4185/acc=0.8061, loss=0.4169/acc=0.7886, loss=0.4389/acc=0.8013, loss=0.4223/acc=0.8002, loss=0.4264/acc=0.8103, loss=0.4338/acc=0.7873, loss=0.
4745/acc=0.7716, loss=0.4413/acc=0.7818)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4026 (folds: loss=0.3880/acc=0.8290, loss=0.3850/acc=0.8097, loss=0.4094/acc=0.8086, loss=0.3965/acc=0.8085, loss=0.3758/acc=0.8195, loss=0.4112/acc
=0.8020, loss=0.4486/acc=0.7753, loss=0.4059/acc=0.7965)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4021 (folds: loss=0.3863/acc=0.8346, loss=0.3831/acc=0.8125, loss=0.4082/acc=0.8123, loss=0.3951/acc=0.8048, loss=0.3785/acc=0.8241, loss=0.4100/acc
=0.8020, loss=0.4498/acc=0.7781, loss=0.4062/acc=0.8002)
  [histgb] histgb trees=300 depth=6 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4251 (folds: loss=0.4064/acc=0.8079, loss=0.4153/acc=0.7923, loss=0.4332/acc=0.8096, loss=0.4158/acc=0.8039, loss=0.4129/acc=0.8140, loss=0.4321/acc=0.7910,
 loss=0.4590/acc=0.7716, loss=0.4264/acc=0.7827)
  [rf] rf trees=200 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4194 (folds: loss=0.4066/acc=0.7987, loss=0.4008/acc=0.7914, loss=0.4220/acc=0.8068, loss=0.4044/acc=0.8002, loss=0.4074/acc=0.8149, loss=0.4258/acc=0.7891, loss=0.
4628/acc=0.7753, loss=0.4258/acc=0.7864)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4142 (folds: loss=0.4018/acc=0.8162, loss=0.3924/acc=0.8097, loss=0.4214/acc=0.8096, loss=0.4074/acc=0.8076, loss=0.3856/acc=0.8223, loss=0.4201/acc
=0.7993, loss=0.4648/acc=0.7615, loss=0.4202/acc=0.7873)
  best avg log loss this gen: 0.4003
Generation 2/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4006 (folds: loss=0.3840/acc=0.8364, loss=0.3822/acc=0.8088, loss=0.4060/acc=0.8096, loss=0.3942/acc=0.8020, loss=0.3780/acc=0.8250, loss=0.4079/acc
=0.8029, loss=0.4473/acc=0.7744, loss=0.4051/acc=0.7919)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4004 (folds: loss=0.3829/acc=0.8382, loss=0.3818/acc=0.8097, loss=0.4059/acc=0.8077, loss=0.3933/acc=0.8020, loss=0.3796/acc=0.8223, loss=0.4071/acc
=0.8066, loss=0.4470/acc=0.7744, loss=0.4059/acc=0.7937)
  [histgb] histgb trees=200 depth=8 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4423 (folds: loss=0.4283/acc=0.7950, loss=0.4357/acc=0.7960, loss=0.4506/acc=0.8050, loss=0.4284/acc=0.7947, loss=0.4346/acc=0.8039, loss=0.4502/acc=0.7864,
 loss=0.4735/acc=0.7652, loss=0.4371/acc=0.7855)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4009 (folds: loss=0.3848/acc=0.8364, loss=0.3820/acc=0.8088, loss=0.4062/acc=0.8105, loss=0.3945/acc=0.8039, loss=0.3785/acc=0.8232, loss=0.4095/acc
=0.8011, loss=0.4474/acc=0.7753, loss=0.4046/acc=0.7965)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4079 (folds: loss=0.3937/acc=0.8281, loss=0.3868/acc=0.8125, loss=0.4162/acc=0.8096, loss=0.4001/acc=0.8103, loss=0.3814/acc=0.8204, loss=0.4148/acc
=0.7983, loss=0.4570/acc=0.7670, loss=0.4132/acc=0.7910)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4011 (folds: loss=0.3842/acc=0.8355, loss=0.3839/acc=0.8125, loss=0.4068/acc=0.8050, loss=0.3954/acc=0.8066, loss=0.3761/acc=0.8223, loss=0.4093/acc
=0.8020, loss=0.4474/acc=0.7753, loss=0.4056/acc=0.7956)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4010 (folds: loss=0.3849/acc=0.8355, loss=0.3834/acc=0.8088, loss=0.4065/acc=0.8068, loss=0.3943/acc=0.8076, loss=0.3765/acc=0.8204, loss=0.4091/acc
=0.8048, loss=0.4475/acc=0.7762, loss=0.4060/acc=0.7937)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4017 (folds: loss=0.3863/acc=0.8355, loss=0.3824/acc=0.8079, loss=0.4084/acc=0.8096, loss=0.3942/acc=0.8085, loss=0.3784/acc=0.8204, loss=0.4094/acc
=0.8039, loss=0.4489/acc=0.7753, loss=0.4059/acc=0.7965)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4083 (folds: loss=0.3956/acc=0.8235, loss=0.3879/acc=0.8125, loss=0.4162/acc=0.8123, loss=0.4017/acc=0.8085, loss=0.3794/acc=0.8214, loss=0.4149/acc
=0.7993, loss=0.4573/acc=0.7698, loss=0.4132/acc=0.7910)
  [extratrees] extratrees trees=400 depth=6 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4063 (folds: loss=0.3926/acc=0.8254, loss=0.3871/acc=0.8079, loss=0.4132/acc=0.8096, loss=0.4001/acc=0.8103, loss=0.3772/acc=0.8177, loss=0.4145/acc
=0.7974, loss=0.4543/acc=0.7716, loss=0.4111/acc=0.7937)
  [gbstump] gbstump estimators=300 lr=0.2 min_leaf=100 -> avg_log_loss=0.4445 (folds: loss=0.4299/acc=0.8006, loss=0.4322/acc=0.7886, loss=0.4555/acc=0.8004, loss=0.4314/acc=0.7974, loss=0.4371/acc=0.8039, loss=0.4507/acc=0.7772, loss=0.4748/
acc=0.7698, loss=0.4442/acc=0.7799)
  [rf] rf trees=300 depth=7 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4329 (folds: loss=0.4192/acc=0.8107, loss=0.4139/acc=0.7895, loss=0.4364/acc=0.8077, loss=0.4197/acc=0.8029, loss=0.4289/acc=0.8140, loss=0.4353/acc=0.7864, loss=0.
4712/acc=0.7744, loss=0.4383/acc=0.7799)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4154 (folds: loss=0.4029/acc=0.8143, loss=0.3965/acc=0.8107, loss=0.4232/acc=0.8086, loss=0.4087/acc=0.8085, loss=0.3854/acc=0.8186, loss=0.4197/acc
=0.7983, loss=0.4656/acc=0.7615, loss=0.4209/acc=0.7891)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4024 (folds: loss=0.3868/acc=0.8309, loss=0.3838/acc=0.8107, loss=0.4087/acc=0.8105, loss=0.3960/acc=0.8066, loss=0.3773/acc=0.8177, loss=0.4102/acc
=0.7993, loss=0.4501/acc=0.7735, loss=0.4063/acc=0.7965)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4099 (folds: loss=0.3969/acc=0.8208, loss=0.3911/acc=0.8097, loss=0.4175/acc=0.8077, loss=0.4045/acc=0.8085, loss=0.3811/acc=0.8204, loss=0.4156/acc
=0.8011, loss=0.4584/acc=0.7661, loss=0.4141/acc=0.7910)
  best avg log loss this gen: 0.4004
Generation 3/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4007 (folds: loss=0.3833/acc=0.8373, loss=0.3830/acc=0.8070, loss=0.4062/acc=0.8086, loss=0.3933/acc=0.8039, loss=0.3800/acc=0.8223, loss=0.4078/acc
=0.8011, loss=0.4467/acc=0.7772, loss=0.4056/acc=0.7956)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4007 (folds: loss=0.3833/acc=0.8382, loss=0.3841/acc=0.8134, loss=0.4061/acc=0.8077, loss=0.3939/acc=0.8066, loss=0.3779/acc=0.8278, loss=0.4078/acc
=0.8076, loss=0.4473/acc=0.7753, loss=0.4050/acc=0.7947)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4002 (folds: loss=0.3831/acc=0.8373, loss=0.3820/acc=0.8061, loss=0.4069/acc=0.8050, loss=0.3928/acc=0.8011, loss=0.3771/acc=0.8260, loss=0.4069/acc
=0.8066, loss=0.4480/acc=0.7762, loss=0.4051/acc=0.7937)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4080 (folds: loss=0.3941/acc=0.8244, loss=0.3869/acc=0.8107, loss=0.4167/acc=0.8123, loss=0.3997/acc=0.8066, loss=0.3804/acc=0.8204, loss=0.4137/acc
=0.7965, loss=0.4571/acc=0.7716, loss=0.4150/acc=0.7901)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4020 (folds: loss=0.3870/acc=0.8318, loss=0.3838/acc=0.8070, loss=0.4085/acc=0.8105, loss=0.3942/acc=0.8076, loss=0.3770/acc=0.8214, loss=0.4093/acc
=0.8011, loss=0.4496/acc=0.7762, loss=0.4066/acc=0.7956)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4085 (folds: loss=0.3954/acc=0.8254, loss=0.3876/acc=0.8162, loss=0.4160/acc=0.8123, loss=0.4006/acc=0.8140, loss=0.3809/acc=0.8232, loss=0.4157/acc
=0.8020, loss=0.4577/acc=0.7698, loss=0.4143/acc=0.7919)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4016 (folds: loss=0.3850/acc=0.8364, loss=0.3837/acc=0.8079, loss=0.4075/acc=0.8031, loss=0.3951/acc=0.8029, loss=0.3771/acc=0.8177, loss=0.4097/acc
=0.8039, loss=0.4490/acc=0.7744, loss=0.4058/acc=0.7937)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4011 (folds: loss=0.3849/acc=0.8309, loss=0.3835/acc=0.8125, loss=0.4078/acc=0.8050, loss=0.3952/acc=0.8029, loss=0.3757/acc=0.8250, loss=0.4087/acc
=0.8057, loss=0.4472/acc=0.7726, loss=0.4058/acc=0.7910)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4027 (folds: loss=0.3881/acc=0.8318, loss=0.3845/acc=0.8107, loss=0.4082/acc=0.8059, loss=0.3978/acc=0.8057, loss=0.3749/acc=0.8250, loss=0.4114/acc
=0.8029, loss=0.4499/acc=0.7726, loss=0.4065/acc=0.7974)
  [mlp] layers=[256, 64, 128, 256, 256, 256] act=gelu lr=0.0043 batch=256 epochs=20 -> avg_log_loss=0.5797 (folds: loss=0.5206/acc=0.7996, loss=0.5237/acc=0.8153, loss=0.5600/acc=0.7930, loss=0.6124/acc=0.7928, loss=0.5551/acc=0.8048, loss=0.
5793/acc=0.7965, loss=0.7076/acc=0.7597, loss=0.5791/acc=0.7827)
  [gbstump] gbstump estimators=150 lr=0.2 min_leaf=50 -> avg_log_loss=0.4423 (folds: loss=0.4329/acc=0.8015, loss=0.4276/acc=0.7923, loss=0.4519/acc=0.8059, loss=0.4276/acc=0.7956, loss=0.4311/acc=0.8112, loss=0.4484/acc=0.7873, loss=0.4768/a
cc=0.7587, loss=0.4425/acc=0.7744)
  [histgb] histgb trees=300 depth=4 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4373 (folds: loss=0.4220/acc=0.8042, loss=0.4261/acc=0.7941, loss=0.4429/acc=0.7976, loss=0.4277/acc=0.7937, loss=0.4269/acc=0.8020, loss=0.4497/acc=0.7901,
 loss=0.4647/acc=0.7680, loss=0.4382/acc=0.7753)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4101 (folds: loss=0.3968/acc=0.8235, loss=0.3903/acc=0.8079, loss=0.4174/acc=0.8114, loss=0.4039/acc=0.8076, loss=0.3804/acc=0.8241, loss=0.4172/acc
=0.8020, loss=0.4604/acc=0.7634, loss=0.4144/acc=0.7891)
  [gbstump] gbstump estimators=150 lr=0.1 min_leaf=200 -> avg_log_loss=0.4357 (folds: loss=0.4217/acc=0.8143, loss=0.4200/acc=0.7950, loss=0.4473/acc=0.8068, loss=0.4226/acc=0.8066, loss=0.4233/acc=0.8204, loss=0.4366/acc=0.7891, loss=0.4774/
acc=0.7624, loss=0.4364/acc=0.7864)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4034 (folds: loss=0.3873/acc=0.8318, loss=0.3829/acc=0.8061, loss=0.4102/acc=0.8114, loss=0.3973/acc=0.8103, loss=0.3787/acc=0.8223, loss=0.4108/acc
=0.7974, loss=0.4517/acc=0.7762, loss=0.4079/acc=0.7983)
  best avg log loss this gen: 0.4002
Generation 4/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4007 (folds: loss=0.3839/acc=0.8401, loss=0.3832/acc=0.8097, loss=0.4061/acc=0.8059, loss=0.3922/acc=0.8057, loss=0.3792/acc=0.8195, loss=0.4075/acc
=0.8066, loss=0.4486/acc=0.7744, loss=0.4049/acc=0.7956)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4004 (folds: loss=0.3838/acc=0.8392, loss=0.3832/acc=0.8116, loss=0.4071/acc=0.8050, loss=0.3926/acc=0.8066, loss=0.3773/acc=0.8260, loss=0.4073/acc
=0.8057, loss=0.4463/acc=0.7781, loss=0.4054/acc=0.7910)
  [histgb] histgb trees=400 depth=5 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4442 (folds: loss=0.4282/acc=0.7969, loss=0.4339/acc=0.7904, loss=0.4575/acc=0.7939, loss=0.4344/acc=0.7956, loss=0.4320/acc=0.7983, loss=0.4641/acc=0.7643,
 loss=0.4642/acc=0.7670, loss=0.4397/acc=0.7864)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4063 (folds: loss=0.3923/acc=0.8254, loss=0.3859/acc=0.8097, loss=0.4128/acc=0.8160, loss=0.3999/acc=0.8085, loss=0.3793/acc=0.8232, loss=0.4139/acc
=0.7947, loss=0.4549/acc=0.7753, loss=0.4110/acc=0.7947)
  [histgb] histgb trees=100 depth=6 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4407 (folds: loss=0.4277/acc=0.7996, loss=0.4329/acc=0.7960, loss=0.4486/acc=0.7976, loss=0.4277/acc=0.8029, loss=0.4326/acc=0.8094, loss=0.4496/acc=0.7864,
 loss=0.4702/acc=0.7689, loss=0.4366/acc=0.7855)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4097 (folds: loss=0.3963/acc=0.8244, loss=0.3903/acc=0.8097, loss=0.4167/acc=0.8105, loss=0.4033/acc=0.8057, loss=0.3815/acc=0.8195, loss=0.4162/acc
=0.7993, loss=0.4587/acc=0.7661, loss=0.4147/acc=0.7919)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4085 (folds: loss=0.3942/acc=0.8272, loss=0.3875/acc=0.8125, loss=0.4157/acc=0.8132, loss=0.4014/acc=0.8103, loss=0.3808/acc=0.8214, loss=0.4155/acc
=0.7974, loss=0.4583/acc=0.7698, loss=0.4145/acc=0.7891)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4054 (folds: loss=0.3921/acc=0.8309, loss=0.3881/acc=0.8107, loss=0.4103/acc=0.8096, loss=0.4001/acc=0.8048, loss=0.3767/acc=0.8195, loss=0.4138/acc
=0.7983, loss=0.4525/acc=0.7716, loss=0.4097/acc=0.7937)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4042 (folds: loss=0.3892/acc=0.8318, loss=0.3836/acc=0.8097, loss=0.4113/acc=0.8151, loss=0.3966/acc=0.8076, loss=0.3796/acc=0.8177, loss=0.4102/acc
=0.7956, loss=0.4530/acc=0.7762, loss=0.4098/acc=0.7965)
  [gbstump] gbstump estimators=200 lr=0.1 min_leaf=50 -> avg_log_loss=0.4392 (folds: loss=0.4318/acc=0.7950, loss=0.4237/acc=0.7950, loss=0.4472/acc=0.8096, loss=0.4250/acc=0.8029, loss=0.4267/acc=0.8168, loss=0.4424/acc=0.7919, loss=0.4770/a
cc=0.7597, loss=0.4397/acc=0.7772)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4013 (folds: loss=0.3857/acc=0.8346, loss=0.3835/acc=0.8107, loss=0.4069/acc=0.8050, loss=0.3943/acc=0.8039, loss=0.3768/acc=0.8269, loss=0.4081/acc
=0.8076, loss=0.4483/acc=0.7707, loss=0.4066/acc=0.7956)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4024 (folds: loss=0.3870/acc=0.8327, loss=0.3846/acc=0.8088, loss=0.4088/acc=0.8077, loss=0.3965/acc=0.8094, loss=0.3752/acc=0.8214, loss=0.4105/acc
=0.8048, loss=0.4496/acc=0.7781, loss=0.4069/acc=0.7947)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4059 (folds: loss=0.3920/acc=0.8254, loss=0.3873/acc=0.8143, loss=0.4130/acc=0.8068, loss=0.3996/acc=0.8112, loss=0.3772/acc=0.8195, loss=0.4140/acc
=0.7974, loss=0.4531/acc=0.7689, loss=0.4110/acc=0.7974)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4024 (folds: loss=0.3869/acc=0.8327, loss=0.3826/acc=0.8070, loss=0.4090/acc=0.8105, loss=0.3967/acc=0.8066, loss=0.3773/acc=0.8223, loss=0.4101/acc
=0.8039, loss=0.4492/acc=0.7726, loss=0.4070/acc=0.7928)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4026 (folds: loss=0.3867/acc=0.8290, loss=0.3846/acc=0.8107, loss=0.4088/acc=0.8077, loss=0.3957/acc=0.8066, loss=0.3772/acc=0.8195, loss=0.4104/acc
=0.7965, loss=0.4498/acc=0.7716, loss=0.4079/acc=0.7974)
  best avg log loss this gen: 0.4004
Generation 5/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4004 (folds: loss=0.3847/acc=0.8401, loss=0.3835/acc=0.8088, loss=0.4052/acc=0.8105, loss=0.3937/acc=0.8048, loss=0.3777/acc=0.8250, loss=0.4072/acc
=0.8039, loss=0.4463/acc=0.7735, loss=0.4052/acc=0.7974)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4005 (folds: loss=0.3839/acc=0.8392, loss=0.3822/acc=0.8088, loss=0.4053/acc=0.8105, loss=0.3933/acc=0.8020, loss=0.3795/acc=0.8223, loss=0.4067/acc
=0.8085, loss=0.4480/acc=0.7753, loss=0.4048/acc=0.7965)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4033 (folds: loss=0.3885/acc=0.8290, loss=0.3848/acc=0.8079, loss=0.4101/acc=0.8040, loss=0.3972/acc=0.8094, loss=0.3763/acc=0.8177, loss=0.4120/acc
=0.7993, loss=0.4500/acc=0.7790, loss=0.4073/acc=0.7947)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4007 (folds: loss=0.3838/acc=0.8382, loss=0.3822/acc=0.8097, loss=0.4052/acc=0.8059, loss=0.3920/acc=0.8039, loss=0.3801/acc=0.8278, loss=0.4077/acc
=0.8076, loss=0.4482/acc=0.7716, loss=0.4064/acc=0.7928)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4085 (folds: loss=0.3951/acc=0.8208, loss=0.3890/acc=0.8125, loss=0.4163/acc=0.8096, loss=0.4010/acc=0.8094, loss=0.3799/acc=0.8232, loss=0.4154/acc
=0.8002, loss=0.4577/acc=0.7661, loss=0.4135/acc=0.7891)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4043 (folds: loss=0.3905/acc=0.8281, loss=0.3848/acc=0.8125, loss=0.4114/acc=0.8096, loss=0.3973/acc=0.8112, loss=0.3768/acc=0.8177, loss=0.4112/acc
=0.7993, loss=0.4525/acc=0.7753, loss=0.4098/acc=0.7956)
  [histgb] histgb trees=300 depth=8 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4499 (folds: loss=0.4376/acc=0.7840, loss=0.4420/acc=0.7886, loss=0.4691/acc=0.7737, loss=0.4366/acc=0.7845, loss=0.4332/acc=0.8011, loss=0.4741/acc=0.7643,
 loss=0.4658/acc=0.7652, loss=0.4410/acc=0.7808)
  [extratrees] extratrees trees=400 depth=6 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4045 (folds: loss=0.3901/acc=0.8254, loss=0.3847/acc=0.8097, loss=0.4114/acc=0.8096, loss=0.3974/acc=0.8094, loss=0.3786/acc=0.8177, loss=0.4121/acc
=0.8002, loss=0.4527/acc=0.7707, loss=0.4095/acc=0.7919)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4075 (folds: loss=0.3933/acc=0.8235, loss=0.3865/acc=0.8107, loss=0.4150/acc=0.8105, loss=0.3996/acc=0.8131, loss=0.3807/acc=0.8204, loss=0.4140/acc
=0.7965, loss=0.4574/acc=0.7661, loss=0.4139/acc=0.7910)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4154 (folds: loss=0.4036/acc=0.8171, loss=0.3953/acc=0.8088, loss=0.4226/acc=0.8096, loss=0.4094/acc=0.8057, loss=0.3862/acc=0.8223, loss=0.4208/acc
=0.8011, loss=0.4640/acc=0.7624, loss=0.4209/acc=0.7882)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4101 (folds: loss=0.3990/acc=0.8226, loss=0.3906/acc=0.8079, loss=0.4171/acc=0.8096, loss=0.4037/acc=0.8076, loss=0.3816/acc=0.8204, loss=0.4160/acc
=0.8002, loss=0.4581/acc=0.7689, loss=0.4148/acc=0.7882)
  [mlp] layers=[256] act=gelu lr=0.0014 batch=128 epochs=15 -> avg_log_loss=0.4048 (folds: loss=0.3797/acc=0.8226, loss=0.3808/acc=0.8180, loss=0.4036/acc=0.8059, loss=0.4039/acc=0.7928, loss=0.3847/acc=0.8158, loss=0.4161/acc=0.8011, loss=0.
4589/acc=0.7735, loss=0.4106/acc=0.7983)
  [rf] rf trees=400 depth=8 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4240 (folds: loss=0.4094/acc=0.8033, loss=0.4076/acc=0.7886, loss=0.4269/acc=0.8105, loss=0.4122/acc=0.8020, loss=0.4155/acc=0.8168, loss=0.4257/acc=0.7919, loss=0.
4636/acc=0.7716, loss=0.4307/acc=0.7845)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4006 (folds: loss=0.3839/acc=0.8373, loss=0.3827/acc=0.8070, loss=0.4055/acc=0.8068, loss=0.3949/acc=0.8020, loss=0.3775/acc=0.8232, loss=0.4091/acc
=0.8085, loss=0.4465/acc=0.7772, loss=0.4048/acc=0.7937)
  [extratrees] extratrees trees=100 depth=8 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4010 (folds: loss=0.3846/acc=0.8401, loss=0.3817/acc=0.8088, loss=0.4055/acc=0.8086, loss=0.3953/acc=0.8076, loss=0.3785/acc=0.8223, loss=0.4085/acc
=0.8029, loss=0.4487/acc=0.7762, loss=0.4053/acc=0.7937)
  best avg log loss this gen: 0.4004
Generation 6/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4005 (folds: loss=0.3838/acc=0.8401, loss=0.3841/acc=0.8070, loss=0.4066/acc=0.8059, loss=0.3929/acc=0.8066, loss=0.3775/acc=0.8204, loss=0.4081/acc
=0.8048, loss=0.4464/acc=0.7716, loss=0.4047/acc=0.7928)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4000 (folds: loss=0.3831/acc=0.8382, loss=0.3818/acc=0.8079, loss=0.4044/acc=0.8123, loss=0.3933/acc=0.8029, loss=0.3787/acc=0.8186, loss=0.4075/acc
=0.8048, loss=0.4474/acc=0.7762, loss=0.4041/acc=0.7928)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4085 (folds: loss=0.3946/acc=0.8263, loss=0.3875/acc=0.8116, loss=0.4163/acc=0.8123, loss=0.4015/acc=0.8085, loss=0.3804/acc=0.8214, loss=0.4151/acc
=0.7974, loss=0.4577/acc=0.7661, loss=0.4147/acc=0.7873)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4078 (folds: loss=0.3940/acc=0.8263, loss=0.3869/acc=0.8097, loss=0.4153/acc=0.8151, loss=0.4005/acc=0.8094, loss=0.3803/acc=0.8214, loss=0.4149/acc
=0.7983, loss=0.4566/acc=0.7707, loss=0.4138/acc=0.7891)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4030 (folds: loss=0.3884/acc=0.8281, loss=0.3846/acc=0.8079, loss=0.4078/acc=0.8040, loss=0.3980/acc=0.8057, loss=0.3771/acc=0.8204, loss=0.4115/acc
=0.8002, loss=0.4494/acc=0.7735, loss=0.4073/acc=0.7965)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4012 (folds: loss=0.3854/acc=0.8346, loss=0.3842/acc=0.8125, loss=0.4067/acc=0.8059, loss=0.3955/acc=0.8066, loss=0.3759/acc=0.8204, loss=0.4090/acc
=0.8020, loss=0.4473/acc=0.7726, loss=0.4053/acc=0.7937)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4085 (folds: loss=0.3959/acc=0.8226, loss=0.3886/acc=0.8143, loss=0.4162/acc=0.8114, loss=0.4024/acc=0.8057, loss=0.3794/acc=0.8232, loss=0.4147/acc
=0.7983, loss=0.4573/acc=0.7670, loss=0.4137/acc=0.7901)
  [extratrees] extratrees trees=300 depth=7 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4028 (folds: loss=0.3881/acc=0.8318, loss=0.3836/acc=0.8107, loss=0.4090/acc=0.8105, loss=0.3969/acc=0.8039, loss=0.3758/acc=0.8177, loss=0.4124/acc
=0.7965, loss=0.4499/acc=0.7762, loss=0.4069/acc=0.7974)
  [extratrees] extratrees trees=100 depth=8 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4026 (folds: loss=0.3882/acc=0.8336, loss=0.3844/acc=0.8079, loss=0.4087/acc=0.8086, loss=0.3954/acc=0.8076, loss=0.3779/acc=0.8232, loss=0.4107/acc
=0.8011, loss=0.4505/acc=0.7753, loss=0.4055/acc=0.7993)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4060 (folds: loss=0.3923/acc=0.8244, loss=0.3861/acc=0.8153, loss=0.4131/acc=0.8059, loss=0.4002/acc=0.8112, loss=0.3777/acc=0.8204, loss=0.4129/acc
=0.8002, loss=0.4540/acc=0.7726, loss=0.4113/acc=0.7937)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4041 (folds: loss=0.3901/acc=0.8244, loss=0.3852/acc=0.8125, loss=0.4093/acc=0.8050, loss=0.3980/acc=0.8076, loss=0.3793/acc=0.8168, loss=0.4128/acc
=0.7983, loss=0.4503/acc=0.7726, loss=0.4079/acc=0.7937)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4109 (folds: loss=0.3983/acc=0.8208, loss=0.3919/acc=0.8079, loss=0.4184/acc=0.8096, loss=0.4049/acc=0.8094, loss=0.3805/acc=0.8214, loss=0.4172/acc
=0.8002, loss=0.4595/acc=0.7652, loss=0.4164/acc=0.7891)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=50 -> avg_log_loss=0.4399 (folds: loss=0.4307/acc=0.7987, loss=0.4250/acc=0.7941, loss=0.4471/acc=0.8096, loss=0.4263/acc=0.8029, loss=0.4289/acc=0.8140, loss=0.4444/acc=0.7855, loss=0.4763/a
cc=0.7634, loss=0.4402/acc=0.7753)
  [gbstump] gbstump estimators=150 lr=0.3 min_leaf=150 -> avg_log_loss=0.4438 (folds: loss=0.4274/acc=0.8024, loss=0.4319/acc=0.7923, loss=0.4551/acc=0.8040, loss=0.4305/acc=0.8011, loss=0.4339/acc=0.8076, loss=0.4489/acc=0.7864, loss=0.4758/
acc=0.7716, loss=0.4470/acc=0.7799)
  [extratrees] extratrees trees=100 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4026 (folds: loss=0.3880/acc=0.8272, loss=0.3855/acc=0.8107, loss=0.4061/acc=0.8077, loss=0.3949/acc=0.8094, loss=0.3802/acc=0.8241, loss=0.4099/acc
=0.8057, loss=0.4499/acc=0.7689, loss=0.4063/acc=0.7993)
  best avg log loss this gen: 0.4000
Generation 7/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4003 (folds: loss=0.3837/acc=0.8382, loss=0.3817/acc=0.8125, loss=0.4055/acc=0.8068, loss=0.3929/acc=0.8011, loss=0.3790/acc=0.8241, loss=0.4072/acc
=0.8029, loss=0.4473/acc=0.7772, loss=0.4052/acc=0.7947)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4003 (folds: loss=0.3834/acc=0.8346, loss=0.3834/acc=0.8088, loss=0.4051/acc=0.8040, loss=0.3934/acc=0.8085, loss=0.3781/acc=0.8177, loss=0.4085/acc
=0.8048, loss=0.4469/acc=0.7772, loss=0.4039/acc=0.7937)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4029 (folds: loss=0.3888/acc=0.8327, loss=0.3853/acc=0.8079, loss=0.4096/acc=0.8059, loss=0.3962/acc=0.8057, loss=0.3782/acc=0.8168, loss=0.4099/acc
=0.8011, loss=0.4489/acc=0.7680, loss=0.4065/acc=0.7910)
  [histgb] histgb trees=300 depth=5 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4450 (folds: loss=0.4287/acc=0.7960, loss=0.4366/acc=0.7978, loss=0.4541/acc=0.7930, loss=0.4338/acc=0.7993, loss=0.4347/acc=0.8103, loss=0.4555/acc=0.7818,
 loss=0.4757/acc=0.7569, loss=0.4411/acc=0.7919)
  [extratrees] extratrees trees=400 depth=6 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4059 (folds: loss=0.3926/acc=0.8226, loss=0.3874/acc=0.8116, loss=0.4118/acc=0.8068, loss=0.4005/acc=0.8094, loss=0.3768/acc=0.8186, loss=0.4128/acc
=0.7993, loss=0.4540/acc=0.7735, loss=0.4110/acc=0.7928)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4007 (folds: loss=0.3841/acc=0.8364, loss=0.3833/acc=0.8134, loss=0.4059/acc=0.8077, loss=0.3938/acc=0.8029, loss=0.3767/acc=0.8241, loss=0.4087/acc
=0.8085, loss=0.4474/acc=0.7744, loss=0.4056/acc=0.7965)
  [extratrees] extratrees trees=400 depth=6 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4059 (folds: loss=0.3920/acc=0.8235, loss=0.3873/acc=0.8097, loss=0.4130/acc=0.8096, loss=0.3995/acc=0.8103, loss=0.3777/acc=0.8177, loss=0.4124/acc
=0.8029, loss=0.4543/acc=0.7716, loss=0.4113/acc=0.7937)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4091 (folds: loss=0.3960/acc=0.8244, loss=0.3888/acc=0.8116, loss=0.4169/acc=0.8096, loss=0.4019/acc=0.8094, loss=0.3805/acc=0.8214, loss=0.4160/acc
=0.7974, loss=0.4589/acc=0.7680, loss=0.4140/acc=0.7901)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4033 (folds: loss=0.3880/acc=0.8336, loss=0.3846/acc=0.8051, loss=0.4092/acc=0.8114, loss=0.3970/acc=0.8048, loss=0.3779/acc=0.8223, loss=0.4108/acc
=0.8002, loss=0.4509/acc=0.7790, loss=0.4080/acc=0.7965)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4044 (folds: loss=0.3890/acc=0.8318, loss=0.3830/acc=0.8070, loss=0.4118/acc=0.8096, loss=0.3973/acc=0.8122, loss=0.3792/acc=0.8195, loss=0.4123/acc
=0.7974, loss=0.4528/acc=0.7762, loss=0.4100/acc=0.7937)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4154 (folds: loss=0.4034/acc=0.8162, loss=0.3958/acc=0.8061, loss=0.4232/acc=0.8068, loss=0.4098/acc=0.8057, loss=0.3852/acc=0.8158, loss=0.4207/acc
=0.7974, loss=0.4644/acc=0.7652, loss=0.4204/acc=0.7882)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4154 (folds: loss=0.4030/acc=0.8171, loss=0.3965/acc=0.8097, loss=0.4235/acc=0.8077, loss=0.4086/acc=0.8094, loss=0.3858/acc=0.8168, loss=0.4206/acc
=0.7974, loss=0.4648/acc=0.7615, loss=0.4206/acc=0.7873)
  [extratrees] extratrees trees=200 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4140 (folds: loss=0.4004/acc=0.8171, loss=0.3934/acc=0.8116, loss=0.4220/acc=0.8086, loss=0.4066/acc=0.8066, loss=0.3853/acc=0.8204, loss=0.4201/acc
=0.7965, loss=0.4645/acc=0.7624, loss=0.4196/acc=0.7882)
  [extratrees] extratrees trees=100 depth=6 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4066 (folds: loss=0.3934/acc=0.8272, loss=0.3865/acc=0.8097, loss=0.4144/acc=0.8068, loss=0.4009/acc=0.8066, loss=0.3799/acc=0.8177, loss=0.4132/acc
=0.8011, loss=0.4539/acc=0.7689, loss=0.4109/acc=0.7937)
  [histgb] histgb trees=400 depth=7 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4435 (folds: loss=0.4235/acc=0.8006, loss=0.4381/acc=0.7904, loss=0.4501/acc=0.7976, loss=0.4347/acc=0.7983, loss=0.4334/acc=0.8076, loss=0.4590/acc=0.7772,
 loss=0.4675/acc=0.7634, loss=0.4418/acc=0.7864)
  best avg log loss this gen: 0.4003
Generation 8/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4002 (folds: loss=0.3834/acc=0.8364, loss=0.3825/acc=0.8097, loss=0.4054/acc=0.8077, loss=0.3930/acc=0.8020, loss=0.3780/acc=0.8223, loss=0.4074/acc
=0.8085, loss=0.4476/acc=0.7716, loss=0.4045/acc=0.7947)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4001 (folds: loss=0.3836/acc=0.8382, loss=0.3827/acc=0.8153, loss=0.4063/acc=0.8086, loss=0.3940/acc=0.8048, loss=0.3766/acc=0.8214, loss=0.4076/acc
=0.8085, loss=0.4455/acc=0.7744, loss=0.4043/acc=0.7947)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4051 (folds: loss=0.3906/acc=0.8244, loss=0.3850/acc=0.8097, loss=0.4119/acc=0.8105, loss=0.3980/acc=0.8131, loss=0.3785/acc=0.8149, loss=0.4126/acc
=0.7965, loss=0.4534/acc=0.7716, loss=0.4111/acc=0.7937)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4177 (folds: loss=0.4072/acc=0.8134, loss=0.4007/acc=0.8051, loss=0.4238/acc=0.8059, loss=0.4138/acc=0.8048, loss=0.3872/acc=0.8204, loss=0.4206/acc
=0.8020, loss=0.4675/acc=0.7624, loss=0.4211/acc=0.7855)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=150 -> avg_log_loss=0.4377 (folds: loss=0.4243/acc=0.8097, loss=0.4228/acc=0.7914, loss=0.4465/acc=0.8086, loss=0.4252/acc=0.8039, loss=0.4281/acc=0.8149, loss=0.4398/acc=0.7882, loss=0.4752/
acc=0.7661, loss=0.4394/acc=0.7836)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4052 (folds: loss=0.3902/acc=0.8290, loss=0.3847/acc=0.8070, loss=0.4125/acc=0.8142, loss=0.3978/acc=0.8094, loss=0.3787/acc=0.8241, loss=0.4134/acc
=0.7947, loss=0.4541/acc=0.7744, loss=0.4099/acc=0.7891)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4049 (folds: loss=0.3905/acc=0.8309, loss=0.3864/acc=0.8107, loss=0.4116/acc=0.8077, loss=0.3985/acc=0.8085, loss=0.3778/acc=0.8186, loss=0.4134/acc
=0.7965, loss=0.4521/acc=0.7753, loss=0.4092/acc=0.7947)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4017 (folds: loss=0.3872/acc=0.8300, loss=0.3838/acc=0.8088, loss=0.4084/acc=0.8059, loss=0.3962/acc=0.8020, loss=0.3763/acc=0.8223, loss=0.4092/acc
=0.8020, loss=0.4469/acc=0.7744, loss=0.4054/acc=0.7910)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4152 (folds: loss=0.4039/acc=0.8199, loss=0.3948/acc=0.8070, loss=0.4226/acc=0.8096, loss=0.4087/acc=0.8076, loss=0.3856/acc=0.8232, loss=0.4202/acc
=0.8002, loss=0.4651/acc=0.7652, loss=0.4206/acc=0.7864)
  [mlp] layers=[256] act=relu lr=0.0039 batch=512 epochs=30 -> avg_log_loss=0.4069 (folds: loss=0.3785/acc=0.8281, loss=0.3828/acc=0.8134, loss=0.4045/acc=0.8050, loss=0.4057/acc=0.7919, loss=0.3862/acc=0.8029, loss=0.4166/acc=0.8029, loss=0.
4661/acc=0.7744, loss=0.4146/acc=0.7947)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4105 (folds: loss=0.3970/acc=0.8217, loss=0.3917/acc=0.8079, loss=0.4198/acc=0.8077, loss=0.4037/acc=0.8048, loss=0.3813/acc=0.8232, loss=0.4157/acc
=0.7983, loss=0.4598/acc=0.7634, loss=0.4151/acc=0.7919)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4176 (folds: loss=0.4067/acc=0.8171, loss=0.3996/acc=0.8033, loss=0.4245/acc=0.8013, loss=0.4117/acc=0.8020, loss=0.3871/acc=0.8195, loss=0.4215/acc
=0.7993, loss=0.4676/acc=0.7652, loss=0.4223/acc=0.7882)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4178 (folds: loss=0.4059/acc=0.8171, loss=0.4011/acc=0.8042, loss=0.4252/acc=0.8086, loss=0.4111/acc=0.8057, loss=0.3883/acc=0.8223, loss=0.4213/acc
=0.8002, loss=0.4669/acc=0.7634, loss=0.4225/acc=0.7901)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4024 (folds: loss=0.3871/acc=0.8290, loss=0.3834/acc=0.8088, loss=0.4083/acc=0.8086, loss=0.3959/acc=0.8076, loss=0.3772/acc=0.8195, loss=0.4097/acc
=0.8020, loss=0.4502/acc=0.7726, loss=0.4079/acc=0.7947)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4010 (folds: loss=0.3855/acc=0.8373, loss=0.3849/acc=0.8116, loss=0.4054/acc=0.8105, loss=0.3950/acc=0.8076, loss=0.3778/acc=0.8260, loss=0.4089/acc
=0.8057, loss=0.4461/acc=0.7744, loss=0.4048/acc=0.7947)
  best avg log loss this gen: 0.4001
Generation 9/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4006 (folds: loss=0.3835/acc=0.8392, loss=0.3834/acc=0.8125, loss=0.4061/acc=0.8114, loss=0.3949/acc=0.8039, loss=0.3783/acc=0.8204, loss=0.4080/acc
=0.8085, loss=0.4467/acc=0.7735, loss=0.4036/acc=0.7947)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4005 (folds: loss=0.3834/acc=0.8373, loss=0.3814/acc=0.8088, loss=0.4059/acc=0.8105, loss=0.3942/acc=0.8029, loss=0.3793/acc=0.8232, loss=0.4088/acc
=0.8076, loss=0.4467/acc=0.7799, loss=0.4045/acc=0.7956)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4141 (folds: loss=0.4013/acc=0.8180, loss=0.3932/acc=0.8107, loss=0.4223/acc=0.8077, loss=0.4073/acc=0.8085, loss=0.3849/acc=0.8232, loss=0.4198/acc
=0.7983, loss=0.4645/acc=0.7624, loss=0.4200/acc=0.7873)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4156 (folds: loss=0.4030/acc=0.8143, loss=0.3973/acc=0.8070, loss=0.4237/acc=0.8068, loss=0.4077/acc=0.8094, loss=0.3869/acc=0.8223, loss=0.4206/acc
=0.7947, loss=0.4646/acc=0.7680, loss=0.4210/acc=0.7882)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4030 (folds: loss=0.3866/acc=0.8300, loss=0.3854/acc=0.8042, loss=0.4086/acc=0.8142, loss=0.3945/acc=0.8122, loss=0.3784/acc=0.8223, loss=0.4114/acc
=0.8002, loss=0.4509/acc=0.7781, loss=0.4079/acc=0.7910)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4027 (folds: loss=0.3869/acc=0.8309, loss=0.3833/acc=0.8097, loss=0.4103/acc=0.8086, loss=0.3952/acc=0.8076, loss=0.3774/acc=0.8232, loss=0.4107/acc
=0.8002, loss=0.4502/acc=0.7772, loss=0.4076/acc=0.7956)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4141 (folds: loss=0.4005/acc=0.8162, loss=0.3939/acc=0.8116, loss=0.4224/acc=0.8077, loss=0.4078/acc=0.8066, loss=0.3850/acc=0.8232, loss=0.4198/acc
=0.7974, loss=0.4640/acc=0.7643, loss=0.4192/acc=0.7901)
  [extratrees] extratrees trees=300 depth=7 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4042 (folds: loss=0.3895/acc=0.8327, loss=0.3846/acc=0.8088, loss=0.4100/acc=0.8123, loss=0.3983/acc=0.8076, loss=0.3789/acc=0.8214, loss=0.4115/acc
=0.7983, loss=0.4525/acc=0.7744, loss=0.4086/acc=0.7974)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4057 (folds: loss=0.3914/acc=0.8254, loss=0.3857/acc=0.8116, loss=0.4125/acc=0.8105, loss=0.3996/acc=0.8103, loss=0.3779/acc=0.8177, loss=0.4132/acc
=0.7947, loss=0.4539/acc=0.7726, loss=0.4111/acc=0.7910)
  [extratrees] extratrees trees=200 depth=4 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4143 (folds: loss=0.4019/acc=0.8208, loss=0.3940/acc=0.8079, loss=0.4223/acc=0.8086, loss=0.4072/acc=0.8076, loss=0.3855/acc=0.8204, loss=0.4191/acc
=0.7974, loss=0.4645/acc=0.7634, loss=0.4199/acc=0.7864)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4106 (folds: loss=0.3978/acc=0.8235, loss=0.3914/acc=0.8116, loss=0.4175/acc=0.8105, loss=0.4045/acc=0.8066, loss=0.3818/acc=0.8204, loss=0.4164/acc
=0.8020, loss=0.4592/acc=0.7680, loss=0.4161/acc=0.7901)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4146 (folds: loss=0.4013/acc=0.8180, loss=0.3934/acc=0.8134, loss=0.4220/acc=0.8077, loss=0.4080/acc=0.8066, loss=0.3855/acc=0.8214, loss=0.4214/acc
=0.7974, loss=0.4649/acc=0.7615, loss=0.4205/acc=0.7882)
  [histgb] histgb trees=300 depth=8 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4422 (folds: loss=0.4269/acc=0.7987, loss=0.4363/acc=0.7978, loss=0.4483/acc=0.7985, loss=0.4292/acc=0.8048, loss=0.4330/acc=0.8057, loss=0.4516/acc=0.7818,
 loss=0.4743/acc=0.7680, loss=0.4384/acc=0.7799)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4018 (folds: loss=0.3868/acc=0.8318, loss=0.3832/acc=0.8097, loss=0.4079/acc=0.8105, loss=0.3952/acc=0.8048, loss=0.3776/acc=0.8204, loss=0.4088/acc
=0.7974, loss=0.4487/acc=0.7753, loss=0.4061/acc=0.7947)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4044 (folds: loss=0.3900/acc=0.8318, loss=0.3860/acc=0.8097, loss=0.4116/acc=0.8086, loss=0.3981/acc=0.8057, loss=0.3760/acc=0.8204, loss=0.4125/acc
=0.7983, loss=0.4519/acc=0.7762, loss=0.4094/acc=0.7993)
  best avg log loss this gen: 0.4005
Generation 10/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4004 (folds: loss=0.3834/acc=0.8401, loss=0.3813/acc=0.8088, loss=0.4064/acc=0.8059, loss=0.3935/acc=0.8029, loss=0.3793/acc=0.8195, loss=0.4076/acc
=0.8020, loss=0.4476/acc=0.7762, loss=0.4044/acc=0.7956)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4004 (folds: loss=0.3847/acc=0.8373, loss=0.3842/acc=0.8125, loss=0.4053/acc=0.8077, loss=0.3931/acc=0.8057, loss=0.3767/acc=0.8241, loss=0.4081/acc
=0.8048, loss=0.4472/acc=0.7753, loss=0.4036/acc=0.7947)
  [extratrees] extratrees trees=100 depth=6 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4049 (folds: loss=0.3903/acc=0.8290, loss=0.3857/acc=0.8125, loss=0.4130/acc=0.8068, loss=0.3981/acc=0.8103, loss=0.3772/acc=0.8177, loss=0.4128/acc
=0.7983, loss=0.4533/acc=0.7753, loss=0.4088/acc=0.7947)
  [gbstump] gbstump estimators=100 lr=0.3 min_leaf=50 -> avg_log_loss=0.4423 (folds: loss=0.4323/acc=0.7987, loss=0.4274/acc=0.7932, loss=0.4527/acc=0.8096, loss=0.4283/acc=0.8002, loss=0.4298/acc=0.8103, loss=0.4463/acc=0.7836, loss=0.4775/a
cc=0.7597, loss=0.4446/acc=0.7753)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4109 (folds: loss=0.3977/acc=0.8217, loss=0.3932/acc=0.8116, loss=0.4174/acc=0.8105, loss=0.4049/acc=0.8085, loss=0.3816/acc=0.8214, loss=0.4177/acc
=0.8002, loss=0.4592/acc=0.7707, loss=0.4156/acc=0.7919)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=50 -> avg_log_loss=0.4399 (folds: loss=0.4307/acc=0.7987, loss=0.4250/acc=0.7941, loss=0.4471/acc=0.8096, loss=0.4263/acc=0.8029, loss=0.4289/acc=0.8140, loss=0.4444/acc=0.7855, loss=0.4763/a
cc=0.7634, loss=0.4402/acc=0.7753)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4092 (folds: loss=0.3958/acc=0.8263, loss=0.3892/acc=0.8097, loss=0.4179/acc=0.8086, loss=0.4022/acc=0.8094, loss=0.3803/acc=0.8232, loss=0.4168/acc
=0.7965, loss=0.4575/acc=0.7689, loss=0.4139/acc=0.7901)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4093 (folds: loss=0.3965/acc=0.8244, loss=0.3890/acc=0.8125, loss=0.4175/acc=0.8114, loss=0.4024/acc=0.8094, loss=0.3798/acc=0.8241, loss=0.4159/acc
=0.7983, loss=0.4583/acc=0.7670, loss=0.4152/acc=0.7873)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4029 (folds: loss=0.3877/acc=0.8318, loss=0.3826/acc=0.8088, loss=0.4074/acc=0.8114, loss=0.3971/acc=0.8057, loss=0.3778/acc=0.8177, loss=0.4108/acc
=0.8011, loss=0.4511/acc=0.7772, loss=0.4084/acc=0.7919)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4033 (folds: loss=0.3882/acc=0.8318, loss=0.3839/acc=0.8107, loss=0.4097/acc=0.8142, loss=0.3971/acc=0.8066, loss=0.3754/acc=0.8186, loss=0.4136/acc
=0.7983, loss=0.4503/acc=0.7726, loss=0.4082/acc=0.8011)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4042 (folds: loss=0.3901/acc=0.8272, loss=0.3869/acc=0.8079, loss=0.4102/acc=0.8068, loss=0.3993/acc=0.8066, loss=0.3767/acc=0.8158, loss=0.4120/acc
=0.8011, loss=0.4514/acc=0.7716, loss=0.4072/acc=0.7947)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4175 (folds: loss=0.4066/acc=0.8143, loss=0.3993/acc=0.8015, loss=0.4242/acc=0.8031, loss=0.4126/acc=0.8029, loss=0.3867/acc=0.8204, loss=0.4210/acc
=0.7993, loss=0.4676/acc=0.7652, loss=0.4219/acc=0.7891)
  [histgb] histgb trees=300 depth=7 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4458 (folds: loss=0.4331/acc=0.7914, loss=0.4361/acc=0.7895, loss=0.4670/acc=0.7746, loss=0.4329/acc=0.7891, loss=0.4281/acc=0.8057, loss=0.4661/acc=0.7634,
 loss=0.4642/acc=0.7689, loss=0.4390/acc=0.7772)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4148 (folds: loss=0.4027/acc=0.8171, loss=0.3953/acc=0.8079, loss=0.4222/acc=0.8086, loss=0.4082/acc=0.8094, loss=0.3845/acc=0.8195, loss=0.4203/acc
=0.7974, loss=0.4650/acc=0.7624, loss=0.4201/acc=0.7873)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4052 (folds: loss=0.3913/acc=0.8235, loss=0.3854/acc=0.8061, loss=0.4128/acc=0.8114, loss=0.3977/acc=0.8085, loss=0.3778/acc=0.8140, loss=0.4140/acc
=0.7993, loss=0.4531/acc=0.7698, loss=0.4097/acc=0.7974)
  best avg log loss this gen: 0.4004
Generation 11/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4002 (folds: loss=0.3835/acc=0.8373, loss=0.3833/acc=0.8107, loss=0.4057/acc=0.8105, loss=0.3934/acc=0.8057, loss=0.3767/acc=0.8260, loss=0.4077/acc
=0.8085, loss=0.4462/acc=0.7735, loss=0.4054/acc=0.7965)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4004 (folds: loss=0.3828/acc=0.8373, loss=0.3825/acc=0.8079, loss=0.4055/acc=0.8096, loss=0.3939/acc=0.8011, loss=0.3786/acc=0.8214, loss=0.4076/acc
=0.8020, loss=0.4481/acc=0.7726, loss=0.4046/acc=0.7928)
  [histgb] histgb trees=300 depth=4 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4414 (folds: loss=0.4252/acc=0.7978, loss=0.4340/acc=0.7978, loss=0.4475/acc=0.8040, loss=0.4296/acc=0.8020, loss=0.4329/acc=0.8002, loss=0.4496/acc=0.7799,
 loss=0.4708/acc=0.7670, loss=0.4416/acc=0.7808)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4091 (folds: loss=0.3957/acc=0.8272, loss=0.3892/acc=0.8116, loss=0.4165/acc=0.8123, loss=0.4022/acc=0.8103, loss=0.3807/acc=0.8186, loss=0.4159/acc
=0.7965, loss=0.4583/acc=0.7670, loss=0.4147/acc=0.7864)
  [mlp] layers=[16, 16, 64] act=leaky_relu lr=0.0015 batch=128 epochs=15 -> avg_log_loss=0.4043 (folds: loss=0.3922/acc=0.8143, loss=0.3754/acc=0.8143, loss=0.4098/acc=0.8086, loss=0.4096/acc=0.8002, loss=0.3713/acc=0.8186, loss=0.4111/acc=0.
7901, loss=0.4547/acc=0.7781, loss=0.4102/acc=0.8020)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4044 (folds: loss=0.3893/acc=0.8300, loss=0.3868/acc=0.8116, loss=0.4099/acc=0.8059, loss=0.4000/acc=0.8085, loss=0.3762/acc=0.8168, loss=0.4129/acc
=0.7983, loss=0.4514/acc=0.7716, loss=0.4087/acc=0.7965)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4041 (folds: loss=0.3893/acc=0.8290, loss=0.3862/acc=0.8042, loss=0.4115/acc=0.8059, loss=0.3979/acc=0.8085, loss=0.3762/acc=0.8195, loss=0.4121/acc
=0.7947, loss=0.4511/acc=0.7735, loss=0.4083/acc=0.7965)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4105 (folds: loss=0.3984/acc=0.8226, loss=0.3925/acc=0.8116, loss=0.4179/acc=0.8086, loss=0.4048/acc=0.8057, loss=0.3801/acc=0.8241, loss=0.4170/acc
=0.8002, loss=0.4583/acc=0.7652, loss=0.4151/acc=0.7910)
  [extratrees] extratrees trees=300 depth=7 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4019 (folds: loss=0.3850/acc=0.8346, loss=0.3823/acc=0.8107, loss=0.4088/acc=0.8077, loss=0.3946/acc=0.8057, loss=0.3792/acc=0.8223, loss=0.4100/acc
=0.8066, loss=0.4494/acc=0.7772, loss=0.4057/acc=0.7937)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4056 (folds: loss=0.3933/acc=0.8263, loss=0.3899/acc=0.8088, loss=0.4113/acc=0.8068, loss=0.4007/acc=0.8029, loss=0.3756/acc=0.8214, loss=0.4130/acc
=0.8020, loss=0.4524/acc=0.7698, loss=0.4088/acc=0.7947)
  [extratrees] extratrees trees=400 depth=6 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4051 (folds: loss=0.3903/acc=0.8263, loss=0.3852/acc=0.8107, loss=0.4126/acc=0.8077, loss=0.3982/acc=0.8112, loss=0.3775/acc=0.8177, loss=0.4136/acc
=0.7993, loss=0.4532/acc=0.7753, loss=0.4102/acc=0.7937)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4085 (folds: loss=0.3957/acc=0.8235, loss=0.3888/acc=0.8134, loss=0.4159/acc=0.8114, loss=0.4013/acc=0.8076, loss=0.3788/acc=0.8214, loss=0.4151/acc
=0.7974, loss=0.4575/acc=0.7716, loss=0.4146/acc=0.7891)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4108 (folds: loss=0.3987/acc=0.8217, loss=0.3923/acc=0.8088, loss=0.4176/acc=0.8096, loss=0.4046/acc=0.8076, loss=0.3805/acc=0.8186, loss=0.4166/acc
=0.8048, loss=0.4603/acc=0.7670, loss=0.4155/acc=0.7910)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4043 (folds: loss=0.3899/acc=0.8281, loss=0.3855/acc=0.8107, loss=0.4115/acc=0.8086, loss=0.3990/acc=0.8076, loss=0.3770/acc=0.8204, loss=0.4123/acc
=0.7993, loss=0.4509/acc=0.7762, loss=0.4086/acc=0.7974)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4178 (folds: loss=0.4061/acc=0.8134, loss=0.4002/acc=0.8033, loss=0.4249/acc=0.8068, loss=0.4124/acc=0.7993, loss=0.3879/acc=0.8204, loss=0.4213/acc
=0.8002, loss=0.4671/acc=0.7643, loss=0.4226/acc=0.7864)
  best avg log loss this gen: 0.4002
Generation 12/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4003 (folds: loss=0.3836/acc=0.8373, loss=0.3826/acc=0.8125, loss=0.4064/acc=0.8114, loss=0.3943/acc=0.8085, loss=0.3767/acc=0.8269, loss=0.4073/acc
=0.8039, loss=0.4471/acc=0.7753, loss=0.4046/acc=0.7965)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4006 (folds: loss=0.3825/acc=0.8364, loss=0.3840/acc=0.8051, loss=0.4057/acc=0.8050, loss=0.3931/acc=0.8020, loss=0.3786/acc=0.8204, loss=0.4078/acc
=0.8076, loss=0.4479/acc=0.7762, loss=0.4052/acc=0.7919)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4025 (folds: loss=0.3868/acc=0.8336, loss=0.3830/acc=0.8097, loss=0.4083/acc=0.8114, loss=0.3955/acc=0.8085, loss=0.3777/acc=0.8223, loss=0.4112/acc
=0.8029, loss=0.4502/acc=0.7753, loss=0.4070/acc=0.7993)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4090 (folds: loss=0.3954/acc=0.8290, loss=0.3890/acc=0.8088, loss=0.4170/acc=0.8096, loss=0.4023/acc=0.8085, loss=0.3799/acc=0.8186, loss=0.4162/acc
=0.8020, loss=0.4582/acc=0.7689, loss=0.4142/acc=0.7910)
  [mlp] layers=[16, 64] act=relu lr=0.0049 batch=64 epochs=10 -> avg_log_loss=0.4001 (folds: loss=0.3877/acc=0.8244, loss=0.3680/acc=0.8290, loss=0.4047/acc=0.7994, loss=0.4044/acc=0.7993, loss=0.3680/acc=0.8241, loss=0.4182/acc=0.7993, loss=
0.4525/acc=0.7753, loss=0.3970/acc=0.8057)
  [extratrees] extratrees trees=400 depth=6 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4038 (folds: loss=0.3883/acc=0.8336, loss=0.3834/acc=0.8134, loss=0.4113/acc=0.8132, loss=0.3958/acc=0.8103, loss=0.3789/acc=0.8204, loss=0.4112/acc
=0.8020, loss=0.4524/acc=0.7799, loss=0.4092/acc=0.7956)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4181 (folds: loss=0.4070/acc=0.8125, loss=0.3999/acc=0.8033, loss=0.4254/acc=0.8068, loss=0.4140/acc=0.7993, loss=0.3879/acc=0.8204, loss=0.4205/acc
=0.8011, loss=0.4673/acc=0.7643, loss=0.4231/acc=0.7827)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4078 (folds: loss=0.3946/acc=0.8244, loss=0.3886/acc=0.8079, loss=0.4142/acc=0.8086, loss=0.4025/acc=0.8103, loss=0.3791/acc=0.8177, loss=0.4147/acc
=0.7965, loss=0.4567/acc=0.7680, loss=0.4123/acc=0.7919)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4013 (folds: loss=0.3851/acc=0.8364, loss=0.3830/acc=0.8107, loss=0.4077/acc=0.8114, loss=0.3949/acc=0.8112, loss=0.3772/acc=0.8232, loss=0.4092/acc
=0.8057, loss=0.4468/acc=0.7726, loss=0.4061/acc=0.7937)
  [rf] rf trees=200 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4379 (folds: loss=0.4233/acc=0.8097, loss=0.4226/acc=0.7895, loss=0.4417/acc=0.8077, loss=0.4271/acc=0.8039, loss=0.4320/acc=0.8122, loss=0.4406/acc=0.7864, loss=0.
4762/acc=0.7670, loss=0.4398/acc=0.7808)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4085 (folds: loss=0.3952/acc=0.8235, loss=0.3876/acc=0.8134, loss=0.4154/acc=0.8132, loss=0.4009/acc=0.8066, loss=0.3810/acc=0.8204, loss=0.4151/acc
=0.7983, loss=0.4585/acc=0.7689, loss=0.4140/acc=0.7901)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4145 (folds: loss=0.4006/acc=0.8199, loss=0.3939/acc=0.8107, loss=0.4228/acc=0.8077, loss=0.4077/acc=0.8076, loss=0.3860/acc=0.8214, loss=0.4195/acc
=0.7983, loss=0.4646/acc=0.7624, loss=0.4206/acc=0.7873)
  [rf] rf trees=300 depth=6 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4293 (folds: loss=0.4147/acc=0.8061, loss=0.4136/acc=0.7904, loss=0.4313/acc=0.8059, loss=0.4161/acc=0.8029, loss=0.4212/acc=0.8112, loss=0.4297/acc=0.7910, loss=0.
4724/acc=0.7744, loss=0.4353/acc=0.7818)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4005 (folds: loss=0.3832/acc=0.8355, loss=0.3827/acc=0.8143, loss=0.4061/acc=0.8059, loss=0.3943/acc=0.8020, loss=0.3776/acc=0.8232, loss=0.4084/acc
=0.8039, loss=0.4474/acc=0.7744, loss=0.4044/acc=0.7956)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4091 (folds: loss=0.3965/acc=0.8254, loss=0.3880/acc=0.8079, loss=0.4166/acc=0.8114, loss=0.4024/acc=0.8103, loss=0.3803/acc=0.8241, loss=0.4166/acc
=0.7993, loss=0.4583/acc=0.7689, loss=0.4139/acc=0.7910)
  best avg log loss this gen: 0.4001
Generation 13/15
  [mlp] layers=[16, 64] act=relu lr=0.0049 batch=64 epochs=10 -> avg_log_loss=0.4018 (folds: loss=0.3790/acc=0.8309, loss=0.3745/acc=0.8134, loss=0.4052/acc=0.8096, loss=0.4139/acc=0.7910, loss=0.3699/acc=0.8195, loss=0.4195/acc=0.8020, loss=
0.4505/acc=0.7818, loss=0.4023/acc=0.7983)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4003 (folds: loss=0.3832/acc=0.8373, loss=0.3831/acc=0.8125, loss=0.4054/acc=0.8086, loss=0.3936/acc=0.8029, loss=0.3781/acc=0.8241, loss=0.4079/acc
=0.8029, loss=0.4468/acc=0.7744, loss=0.4045/acc=0.7891)
  [mlp] layers=[256, 64] act=relu lr=0.0045 batch=256 epochs=25 -> avg_log_loss=0.4636 (folds: loss=0.4234/acc=0.8143, loss=0.4285/acc=0.8254, loss=0.4644/acc=0.8031, loss=0.4721/acc=0.8011, loss=0.4471/acc=0.8011, loss=0.4716/acc=0.7836, los
s=0.5194/acc=0.7689, loss=0.4822/acc=0.7827)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4054 (folds: loss=0.3913/acc=0.8244, loss=0.3854/acc=0.8116, loss=0.4121/acc=0.8031, loss=0.4000/acc=0.8122, loss=0.3777/acc=0.8195, loss=0.4128/acc
=0.8011, loss=0.4539/acc=0.7726, loss=0.4101/acc=0.7919)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4172 (folds: loss=0.4057/acc=0.8143, loss=0.3981/acc=0.8033, loss=0.4242/acc=0.8086, loss=0.4125/acc=0.8057, loss=0.3875/acc=0.8186, loss=0.4201/acc
=0.8011, loss=0.4675/acc=0.7634, loss=0.4222/acc=0.7864)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4090 (folds: loss=0.3962/acc=0.8217, loss=0.3891/acc=0.8143, loss=0.4165/acc=0.8105, loss=0.4020/acc=0.8094, loss=0.3804/acc=0.8168, loss=0.4154/acc
=0.7974, loss=0.4579/acc=0.7661, loss=0.4148/acc=0.7919)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4102 (folds: loss=0.3975/acc=0.8208, loss=0.3909/acc=0.8134, loss=0.4177/acc=0.8086, loss=0.4043/acc=0.8066, loss=0.3805/acc=0.8195, loss=0.4164/acc
=0.8002, loss=0.4593/acc=0.7643, loss=0.4147/acc=0.7919)
  [mlp] layers=[256] act=gelu lr=0.0013 batch=256 epochs=20 -> avg_log_loss=0.4046 (folds: loss=0.3742/acc=0.8254, loss=0.3749/acc=0.8235, loss=0.4033/acc=0.8077, loss=0.4094/acc=0.7937, loss=0.3845/acc=0.8112, loss=0.4176/acc=0.8002, loss=0.
4589/acc=0.7772, loss=0.4139/acc=0.7910)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4042 (folds: loss=0.3889/acc=0.8336, loss=0.3848/acc=0.8079, loss=0.4102/acc=0.8142, loss=0.3979/acc=0.8122, loss=0.3781/acc=0.8223, loss=0.4121/acc
=0.7993, loss=0.4534/acc=0.7772, loss=0.4085/acc=0.7974)
  [mlp] layers=[32, 64] act=relu lr=0.0050 batch=256 epochs=10 -> avg_log_loss=0.4058 (folds: loss=0.3870/acc=0.8217, loss=0.3762/acc=0.8226, loss=0.4079/acc=0.8160, loss=0.4070/acc=0.7919, loss=0.3741/acc=0.8103, loss=0.4232/acc=0.7965, loss
=0.4630/acc=0.7707, loss=0.4081/acc=0.7965)
  [mlp] layers=[512, 16, 512, 256, 512, 16] act=relu lr=0.0050 batch=512 epochs=25 -> avg_log_loss=0.4592 (folds: loss=0.4190/acc=0.8079, loss=0.4287/acc=0.8254, loss=0.4486/acc=0.7958, loss=0.4614/acc=0.7901, loss=0.4555/acc=0.7993, loss=0.4
605/acc=0.7956, loss=0.5110/acc=0.7735, loss=0.4887/acc=0.7836)
  [extratrees] extratrees trees=400 depth=6 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4078 (folds: loss=0.3950/acc=0.8244, loss=0.3893/acc=0.8070, loss=0.4140/acc=0.8096, loss=0.4021/acc=0.8103, loss=0.3789/acc=0.8186, loss=0.4148/acc
=0.7983, loss=0.4554/acc=0.7698, loss=0.4126/acc=0.7919)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4107 (folds: loss=0.3983/acc=0.8254, loss=0.3921/acc=0.8097, loss=0.4179/acc=0.8105, loss=0.4050/acc=0.8039, loss=0.3813/acc=0.8195, loss=0.4155/acc
=0.8011, loss=0.4591/acc=0.7643, loss=0.4166/acc=0.7882)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4009 (folds: loss=0.3848/acc=0.8382, loss=0.3837/acc=0.8116, loss=0.4066/acc=0.8059, loss=0.3934/acc=0.8057, loss=0.3771/acc=0.8214, loss=0.4087/acc
=0.8057, loss=0.4474/acc=0.7753, loss=0.4051/acc=0.7947)
  [mlp] layers=[16, 16] act=relu lr=0.0048 batch=512 epochs=25 -> avg_log_loss=0.4035 (folds: loss=0.3786/acc=0.8189, loss=0.3716/acc=0.8180, loss=0.4048/acc=0.8132, loss=0.4110/acc=0.7882, loss=0.3771/acc=0.8186, loss=0.4199/acc=0.8011, loss
=0.4590/acc=0.7818, loss=0.4060/acc=0.7974)
  best avg log loss this gen: 0.4003
Generation 14/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4003 (folds: loss=0.3841/acc=0.8401, loss=0.3828/acc=0.8116, loss=0.4061/acc=0.8105, loss=0.3936/acc=0.8020, loss=0.3765/acc=0.8250, loss=0.4080/acc
=0.8057, loss=0.4458/acc=0.7753, loss=0.4057/acc=0.7947)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4007 (folds: loss=0.3852/acc=0.8336, loss=0.3829/acc=0.8088, loss=0.4066/acc=0.8077, loss=0.3941/acc=0.8029, loss=0.3765/acc=0.8223, loss=0.4087/acc
=0.8020, loss=0.4467/acc=0.7781, loss=0.4048/acc=0.7937)
  [mlp] layers=[64, 64] act=relu lr=0.0050 batch=512 epochs=15 -> avg_log_loss=0.4071 (folds: loss=0.3781/acc=0.8217, loss=0.3786/acc=0.8208, loss=0.4110/acc=0.8031, loss=0.4184/acc=0.7873, loss=0.3767/acc=0.8149, loss=0.4248/acc=0.7956, loss
=0.4519/acc=0.7735, loss=0.4173/acc=0.8066)
  [mlp] layers=[16, 16] act=relu lr=0.0043 batch=128 epochs=20 -> avg_log_loss=0.4018 (folds: loss=0.3755/acc=0.8272, loss=0.3780/acc=0.8235, loss=0.4091/acc=0.8096, loss=0.4090/acc=0.7937, loss=0.3779/acc=0.8168, loss=0.4183/acc=0.7947, loss
=0.4405/acc=0.7808, loss=0.4062/acc=0.8002)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4004 (folds: loss=0.3841/acc=0.8373, loss=0.3834/acc=0.8097, loss=0.4062/acc=0.8077, loss=0.3935/acc=0.8057, loss=0.3780/acc=0.8214, loss=0.4074/acc
=0.8085, loss=0.4467/acc=0.7689, loss=0.4040/acc=0.7974)
  [gbstump] gbstump estimators=300 lr=0.2 min_leaf=50 -> avg_log_loss=0.4474 (folds: loss=0.4394/acc=0.7941, loss=0.4353/acc=0.7941, loss=0.4536/acc=0.7994, loss=0.4312/acc=0.7956, loss=0.4376/acc=0.8057, loss=0.4566/acc=0.7827, loss=0.4785/a
cc=0.7643, loss=0.4471/acc=0.7772)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4141 (folds: loss=0.4006/acc=0.8226, loss=0.3936/acc=0.8107, loss=0.4218/acc=0.8086, loss=0.4069/acc=0.8048, loss=0.3854/acc=0.8250, loss=0.4195/acc
=0.7965, loss=0.4655/acc=0.7624, loss=0.4200/acc=0.7873)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4088 (folds: loss=0.3948/acc=0.8254, loss=0.3873/acc=0.8125, loss=0.4170/acc=0.8151, loss=0.4011/acc=0.8112, loss=0.3806/acc=0.8195, loss=0.4150/acc
=0.7956, loss=0.4587/acc=0.7661, loss=0.4154/acc=0.7901)
  [gbstump] gbstump estimators=150 lr=0.3 min_leaf=50 -> avg_log_loss=0.4467 (folds: loss=0.4366/acc=0.7969, loss=0.4325/acc=0.7932, loss=0.4562/acc=0.8013, loss=0.4322/acc=0.7956, loss=0.4352/acc=0.8094, loss=0.4540/acc=0.7808, loss=0.4799/a
cc=0.7597, loss=0.4469/acc=0.7762)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4151 (folds: loss=0.4024/acc=0.8180, loss=0.3960/acc=0.8079, loss=0.4234/acc=0.8077, loss=0.4081/acc=0.8085, loss=0.3853/acc=0.8186, loss=0.4201/acc
=0.7956, loss=0.4651/acc=0.7606, loss=0.4200/acc=0.7882)
  [mlp] layers=[64, 32, 256, 256, 64, 128] act=relu lr=0.0042 batch=512 epochs=25 -> avg_log_loss=0.4184 (folds: loss=0.3873/acc=0.8153, loss=0.3853/acc=0.8272, loss=0.4233/acc=0.8031, loss=0.4207/acc=0.7956, loss=0.3911/acc=0.8029, loss=0.44
72/acc=0.7974, loss=0.4742/acc=0.7634, loss=0.4178/acc=0.7983)
  [extratrees] extratrees trees=100 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4020 (folds: loss=0.3862/acc=0.8346, loss=0.3842/acc=0.8079, loss=0.4080/acc=0.8068, loss=0.3950/acc=0.8094, loss=0.3767/acc=0.8223, loss=0.4100/acc
=0.8002, loss=0.4503/acc=0.7753, loss=0.4055/acc=0.7956)
  [histgb] histgb trees=200 depth=4 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4411 (folds: loss=0.4261/acc=0.7978, loss=0.4312/acc=0.7923, loss=0.4492/acc=0.8004, loss=0.4266/acc=0.7947, loss=0.4302/acc=0.7993, loss=0.4474/acc=0.7827,
 loss=0.4747/acc=0.7670, loss=0.4435/acc=0.7790)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4179 (folds: loss=0.4072/acc=0.8153, loss=0.3992/acc=0.8042, loss=0.4255/acc=0.8077, loss=0.4124/acc=0.8020, loss=0.3884/acc=0.8195, loss=0.4210/acc
=0.8002, loss=0.4679/acc=0.7652, loss=0.4217/acc=0.7882)
  [histgb] histgb trees=400 depth=7 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4370 (folds: loss=0.4232/acc=0.8033, loss=0.4275/acc=0.7941, loss=0.4429/acc=0.7976, loss=0.4263/acc=0.8002, loss=0.4242/acc=0.8057, loss=0.4492/acc=0.7864,
 loss=0.4647/acc=0.7698, loss=0.4377/acc=0.7808)
  best avg log loss this gen: 0.4003
Generation 15/15
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4003 (folds: loss=0.3840/acc=0.8373, loss=0.3831/acc=0.8143, loss=0.4062/acc=0.8105, loss=0.3934/acc=0.8020, loss=0.3764/acc=0.8232, loss=0.4077/acc
=0.8057, loss=0.4466/acc=0.7698, loss=0.4052/acc=0.7993)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4002 (folds: loss=0.3832/acc=0.8392, loss=0.3836/acc=0.8097, loss=0.4059/acc=0.8068, loss=0.3928/acc=0.8066, loss=0.3778/acc=0.8223, loss=0.4077/acc
=0.8066, loss=0.4458/acc=0.7744, loss=0.4046/acc=0.7937)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4175 (folds: loss=0.4060/acc=0.8143, loss=0.3981/acc=0.8079, loss=0.4253/acc=0.8086, loss=0.4136/acc=0.8029, loss=0.3872/acc=0.8186, loss=0.4225/acc
=0.7928, loss=0.4664/acc=0.7643, loss=0.4210/acc=0.7864)
  [extratrees] extratrees trees=200 depth=4 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4154 (folds: loss=0.4024/acc=0.8180, loss=0.3958/acc=0.8079, loss=0.4230/acc=0.8068, loss=0.4092/acc=0.8066, loss=0.3867/acc=0.8195, loss=0.4210/acc
=0.7956, loss=0.4649/acc=0.7643, loss=0.4205/acc=0.7873)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4141 (folds: loss=0.4010/acc=0.8199, loss=0.3937/acc=0.8079, loss=0.4221/acc=0.8068, loss=0.4072/acc=0.8066, loss=0.3857/acc=0.8214, loss=0.4196/acc
=0.7956, loss=0.4644/acc=0.7606, loss=0.4189/acc=0.7891)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4144 (folds: loss=0.4009/acc=0.8199, loss=0.3935/acc=0.8088, loss=0.4224/acc=0.8086, loss=0.4076/acc=0.8076, loss=0.3859/acc=0.8223, loss=0.4200/acc
=0.7993, loss=0.4652/acc=0.7624, loss=0.4194/acc=0.7882)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4062 (folds: loss=0.3923/acc=0.8226, loss=0.3879/acc=0.8107, loss=0.4119/acc=0.8105, loss=0.4000/acc=0.8076, loss=0.3777/acc=0.8149, loss=0.4134/acc
=0.7983, loss=0.4542/acc=0.7707, loss=0.4124/acc=0.7919)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4175 (folds: loss=0.4064/acc=0.8143, loss=0.4000/acc=0.8042, loss=0.4247/acc=0.8068, loss=0.4126/acc=0.8002, loss=0.3870/acc=0.8177, loss=0.4206/acc
=0.8029, loss=0.4669/acc=0.7624, loss=0.4221/acc=0.7882)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4049 (folds: loss=0.3913/acc=0.8254, loss=0.3871/acc=0.8097, loss=0.4105/acc=0.8059, loss=0.3997/acc=0.8076, loss=0.3762/acc=0.8186, loss=0.4131/acc
=0.7983, loss=0.4525/acc=0.7689, loss=0.4091/acc=0.8020)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4020 (folds: loss=0.3853/acc=0.8382, loss=0.3813/acc=0.8097, loss=0.4084/acc=0.8086, loss=0.3944/acc=0.8048, loss=0.3785/acc=0.8232, loss=0.4101/acc
=0.8020, loss=0.4508/acc=0.7744, loss=0.4072/acc=0.7965)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4143 (folds: loss=0.4020/acc=0.8226, loss=0.3938/acc=0.8107, loss=0.4224/acc=0.8077, loss=0.4070/acc=0.8057, loss=0.3856/acc=0.8223, loss=0.4195/acc
=0.7965, loss=0.4643/acc=0.7615, loss=0.4196/acc=0.7901)
  [mlp] layers=[512, 16] act=relu lr=0.0036 batch=128 epochs=30 -> avg_log_loss=0.4768 (folds: loss=0.4406/acc=0.8116, loss=0.4453/acc=0.8281, loss=0.4707/acc=0.7930, loss=0.4990/acc=0.8029, loss=0.4633/acc=0.8039, loss=0.4796/acc=0.7910, los
s=0.5229/acc=0.7836, loss=0.4929/acc=0.7808)
  [rf] rf trees=100 depth=6 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4247 (folds: loss=0.4093/acc=0.8033, loss=0.4095/acc=0.7868, loss=0.4304/acc=0.7976, loss=0.4110/acc=0.8011, loss=0.4094/acc=0.8158, loss=0.4257/acc=0.7891, loss=0.
4692/acc=0.7735, loss=0.4326/acc=0.7882)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4036 (folds: loss=0.3883/acc=0.8281, loss=0.3854/acc=0.8107, loss=0.4096/acc=0.8050, loss=0.3983/acc=0.8048, loss=0.3765/acc=0.8204, loss=0.4120/acc
=0.8039, loss=0.4507/acc=0.7744, loss=0.4080/acc=0.7965)
  [extratrees] extratrees trees=100 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4022 (folds: loss=0.3871/acc=0.8327, loss=0.3831/acc=0.8097, loss=0.4072/acc=0.8086, loss=0.3953/acc=0.8103, loss=0.3768/acc=0.8214, loss=0.4102/acc
=0.8039, loss=0.4498/acc=0.7762, loss=0.4078/acc=0.7993)
  best avg log loss this gen: 0.4002
Retraining top genome #1 (extratrees) with val_log_loss=0.4002
Saved model to models/ga_nn_1_1768489230.pt and submission to submissions/ga_nn_1_1768489230.csv
Retraining top genome #2 (extratrees) with val_log_loss=0.4002
Saved model to models/ga_nn_2_1768489232.pt and submission to submissions/ga_nn_2_1768489232.csv
Retraining top genome #3 (extratrees) with val_log_loss=0.4002
Saved model to models/ga_nn_3_1768489234.pt and submission to submissions/ga_nn_3_1768489234.csv
Backed up previous summary to models/ga_search_summary_20260115-150034.json
Wrote GA summary to models/ga_search_summary.json
[day12] Day 2 GA run complete. Review models/ga_search_summary.json for fresh elites.
root@6a432f30b2e3:/workspace# kaggle competitions submit -c spaceship-titanic -f submissions/ga_nn_1_1768489230.csv -m ""
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.2k/56.2k [00:00<00:00, 96.5kB/s]
Successfully submitted to Spaceship Titanicroot@6a432f30b2e3:/workspace# kaggle competitions submit -c spaceship-titanic -f submissions/ga_nn_2_1768489232.csv -m ""
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.2k/56.2k [00:00<00:00, 97.9kB/s]
Successfully submitted to Spaceship Titanicroot@6a432f30b2e3:/workspace# kaggle competitions submit -c spaceship-titanic -f submissions/ga_nn_3_1768489234.csv -m ""
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.2k/56.2k [00:00<00:00, 98.4kB/s]
Successfully submitted to Spackaggle competitions submit -c spaceship-titanic -f models/day1_stack/stack_meta_submission.csv -m "Day1 stacker"stack_meta_submission.csv -m "Day1 stacker"
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 137k/137k [00:00<00:00, 180kB/s]
Successfully submitted to Spacvim m^C
root@6a432f30b2e3:/workspace# ^C
root@6a432f30b2e3:/workspace# ^C
root@6a432f30b2e3:/workspace# vim models/
day1_stack/                             ga_nn_1_1768344940.pt                   ga_nn_3_1768344941.pt                   ga_nn_6_1768344943.pt                   ga_search_summary.json
day1_top_configs.json                   ga_nn_1_1768489230.pt                   ga_nn_3_1768489234.pt                   ga_nn_7_1768344943.pt                   ga_search_summary_20260113-225544.json
ga_nn_10_1768344944.pt                  ga_nn_2_1768344941.pt                   ga_nn_4_1768344941.pt                   ga_nn_8_1768344944.pt                   ga_search_summary_20260115-150034.json
ga_nn_1_1768337411.pt                   ga_nn_2_1768489232.pt                   ga_nn_5_1768344943.pt                   ga_nn_9_1768344944.pt
root@6a432f30b2e3:/workspace# vim models/day1_stack/stack_
stack_meta_model.joblib    stack_meta_submission.csv  stack_summary.json         stack_test_features.csv    stack_train_features.csv
root@6a432f30b2e3:/workspace# vim models/day1_stack/stack_meta_submission.csv
root@6a432f30b2e3:/workspace# vim models/day1_stack/stack_meta_submission.csv
root@6a432f30b2e3:/workspace# python - <<'PY'
     import pandas as pd
     path = "models/day1_stack/stack_meta_submission.csv"
     df = pd.read_csv(path)[["PassengerId", "Transported"]]
     df.to_csv("submissions/stack_meta_submission_trimmed.csv", index=False)
     PY
>
>
bash: warning: here-document at line 44 delimited by end-of-file (wanted `PY')
  File "<stdin>", line 1
    import pandas as pd
IndentationError: unexpected indent
root@6a432f30b2e3:/workspace# python
Python 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>>
root@6a432f30b2e3:/workspace# vim trim.sh
root@6a432f30b2e3:/workspace# chmod trim.sh
chmod: missing operand after ‘trim.sh’
Try 'chmod --help' for more information.
root@6a432f30b2e3:/workspace# chmod +x trim.sh
root@6a432f30b2e3:/workspace# ./trim.sh
./trim.sh: line 16: warning: here-document at line 7 delimited by end-of-file (wanted `PY')
  File "<stdin>", line 1
    import pandas as pd
IndentationError: unexpected indent
root@6a432f30b2e3:/workspace# vim trim.sh
root@6a432f30b2e3:/workspace# ./trim.sh
Traceback (most recent call last):
  File "<stdin>", line 4, in <module>
  File "/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py", line 1880, in _make_engine
    self.handles = get_handle(
                   ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/pandas/io/common.py", line 873, in get_handle
    handle = open(
             ^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '$INPUT'
root@6a432f30b2e3:/workspace# vim trim.sh
root@6a432f30b2e3:/workspace# vim trim.sh
root@6a432f30b2e3:/workspace# ./trim.sh
Trimmed submission saved to submissions/stack_meta_submission_trimmed.csv
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.4k/56.4k [00:00<00:00, 98.8kB/s]
Successfully submitted to Spacpython scripts/run_day3_ablation.py --stack-top-n 12 --ga-generations 10 --ga-population 12n 12 --ga-generations 10 --ga-population 12
[day3] Baseline val_log_loss: 0.4002
[day3] $ python scripts/prepare_data.py --input data/train.csv --output data/train_prepared.csv --exclude-columns Transported
Wrote processed data to data/train_prepared.csv
[day3] $ python scripts/prepare_data.py --input data/test.csv --output data/test_prepared.csv
Wrote processed data to data/test_prepared.csv
[day3] $ python scripts/add_baseline_predictions.py --train data/train_prepared.csv --test data/test_prepared.csv --target-column Transported
[baseline] Training logistic regression on device cpu.
[baseline] Training gradient boosting stumps.
Wrote augmented training data to data/train_prepared.csv
Wrote augmented test data to data/test_prepared.csv
[day3] $ python scripts/select_top_submissions.py --summaries-dir models --pattern ga_search_summary*.json --top 12 --output models/day3_top_configs.json --include-missing
Wrote top 12 entries to models/day3_top_configs.json
[day3] $ python scripts/build_stack_meta.py --train data/train_prepared.csv --test data/test_prepared.csv --raw-test data/test.csv --top-configs models/day3_top_configs.json --folds 5 --seed 2026 --output-dir models/day3_stack
[stack] Loaded 12 configs from models/day3_top_configs.json
[stack] Generating OOF predictions for ga_rank_1_ts_1768489230_logloss_0_4002 (extratrees)
[stack] Generating OOF predictions for ga_rank_2_ts_1768489232_logloss_0_4002 (extratrees)
[stack] Generating OOF predictions for ga_rank_3_ts_1768489234_logloss_0_4002 (extratrees)
[stack] Generating OOF predictions for ga_rank_1_ts_1768337411_logloss_0_3960 (extratrees)
[stack] Generating OOF predictions for ga_rank_1_ts_1768344940_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_2_ts_1768344941_logloss_0_3887 (gbstump)
[stack] Generating OOF predictions for ga_rank_3_ts_1768344941_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_4_ts_1768344941_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_5_ts_1768344943_logloss_0_3887 (gbstump)
[stack] Generating OOF predictions for ga_rank_6_ts_1768344943_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_7_ts_1768344943_logloss_0_3889 (extratrees)
[stack] Generating OOF predictions for ga_rank_8_ts_1768344944_logloss_0_3889 (extratrees)
[stack] Saved train features to models/day3_stack/stack_train_features.csv
[stack] Saved test features to models/day3_stack/stack_test_features.csv
[stack] Saved meta-model to models/day3_stack/stack_meta_model.joblib
[stack] Saved submission preview to models/day3_stack/stack_meta_submission.csv
[day3] $ python scripts/train_ga_nn.py --train-prepared data/train_prepared.csv --test-prepared data/test_prepared.csv --raw-test data/test.csv --generations 10 --population 12 --top-k 2 --cv-folds 5 --resume-summary models/day3_top_configs.j
son --resume-top-n 12 --seed 1701 --no-kaggle
Using stratified 5-fold CV for GA fitness.
Using device: cpu
Loaded 12 genomes from models/day3_top_configs.json to seed the population.
Generation 1/10
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3936 (folds: loss=0.3724/acc=0.8212, loss=0.4022/acc=0.8074, loss=0.3881/acc=0.8177, loss=0.4050/acc=0.8032, loss=0.4002/acc=0.8130)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3977 (folds: loss=0.3778/acc=0.8223, loss=0.4087/acc=0.8062, loss=0.3884/acc=0.8166, loss=0.4092/acc=0.7940, loss=0.4043/acc=0.8061)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4033 (folds: loss=0.3838/acc=0.8200, loss=0.4158/acc=0.8056, loss=0.3912/acc=0.8183, loss=0.4156/acc=0.7911, loss=0.4102/acc=0.8055)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.3956 (folds: loss=0.3754/acc=0.8206, loss=0.4077/acc=0.8016, loss=0.3878/acc=0.8108, loss=0.4045/acc=0.7986, loss=0.4025/acc=0.8067)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.3943 (folds: loss=0.3726/acc=0.8166, loss=0.4024/acc=0.8102, loss=0.3885/acc=0.8137, loss=0.4077/acc=0.7992, loss=0.4002/acc=0.8078)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=50 -> avg_log_loss=0.4399 (folds: loss=0.4194/acc=0.8056, loss=0.4457/acc=0.7826, loss=0.4265/acc=0.8016, loss=0.4608/acc=0.7842, loss=0.4472/acc=0.7940)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4008 (folds: loss=0.3816/acc=0.8223, loss=0.4130/acc=0.8045, loss=0.3896/acc=0.8148, loss=0.4112/acc=0.7929, loss=0.4083/acc=0.8038)
  [extratrees] extratrees trees=300 depth=7 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.3952 (folds: loss=0.3744/acc=0.8229, loss=0.4062/acc=0.7987, loss=0.3885/acc=0.8143, loss=0.4054/acc=0.7963, loss=0.4016/acc=0.8072)
  [gbstump] gbstump estimators=150 lr=0.2 min_leaf=150 -> avg_log_loss=0.4422 (folds: loss=0.4231/acc=0.8033, loss=0.4493/acc=0.7838, loss=0.4276/acc=0.8005, loss=0.4601/acc=0.7814, loss=0.4506/acc=0.7952)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4100 (folds: loss=0.3915/acc=0.8183, loss=0.4233/acc=0.8039, loss=0.3957/acc=0.8166, loss=0.4224/acc=0.7888, loss=0.4169/acc=0.7980)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3931 (folds: loss=0.3714/acc=0.8206, loss=0.4026/acc=0.8039, loss=0.3864/acc=0.8177, loss=0.4055/acc=0.7986, loss=0.3997/acc=0.8078)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.3980 (folds: loss=0.3783/acc=0.8206, loss=0.4093/acc=0.8033, loss=0.3886/acc=0.8183, loss=0.4085/acc=0.7934, loss=0.4052/acc=0.8049)
  best avg log loss this gen: 0.3931
Generation 2/10
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3936 (folds: loss=0.3722/acc=0.8212, loss=0.4032/acc=0.8039, loss=0.3885/acc=0.8154, loss=0.4046/acc=0.8038, loss=0.3996/acc=0.8107)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3936 (folds: loss=0.3730/acc=0.8200, loss=0.4026/acc=0.8033, loss=0.3878/acc=0.8143, loss=0.4051/acc=0.8026, loss=0.3996/acc=0.8072)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4032 (folds: loss=0.3843/acc=0.8212, loss=0.4161/acc=0.8045, loss=0.3909/acc=0.8194, loss=0.4143/acc=0.7900, loss=0.4104/acc=0.8049)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.3966 (folds: loss=0.3761/acc=0.8194, loss=0.4065/acc=0.7993, loss=0.3884/acc=0.8143, loss=0.4087/acc=0.7969, loss=0.4034/acc=0.8038)
  [extratrees] extratrees trees=100 depth=6 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.3983 (folds: loss=0.3780/acc=0.8206, loss=0.4101/acc=0.8079, loss=0.3890/acc=0.8154, loss=0.4097/acc=0.7969, loss=0.4047/acc=0.8032)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4026 (folds: loss=0.3834/acc=0.8217, loss=0.4149/acc=0.8068, loss=0.3916/acc=0.8206, loss=0.4129/acc=0.7946, loss=0.4099/acc=0.8044)
  [gbstump] gbstump estimators=200 lr=0.2 min_leaf=150 -> avg_log_loss=0.4426 (folds: loss=0.4223/acc=0.7993, loss=0.4470/acc=0.7890, loss=0.4310/acc=0.7930, loss=0.4624/acc=0.7808, loss=0.4505/acc=0.7929)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4110 (folds: loss=0.3924/acc=0.8183, loss=0.4243/acc=0.8005, loss=0.3975/acc=0.8166, loss=0.4231/acc=0.7888, loss=0.4175/acc=0.8009)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4134 (folds: loss=0.3963/acc=0.8120, loss=0.4273/acc=0.7959, loss=0.3999/acc=0.8154, loss=0.4238/acc=0.7825, loss=0.4196/acc=0.7992)
  [histgb] histgb trees=200 depth=7 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4405 (folds: loss=0.4212/acc=0.8033, loss=0.4427/acc=0.7872, loss=0.4386/acc=0.7895, loss=0.4536/acc=0.7802, loss=0.4465/acc=0.7831)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.3939 (folds: loss=0.3736/acc=0.8229, loss=0.4039/acc=0.8016, loss=0.3871/acc=0.8102, loss=0.4045/acc=0.7998, loss=0.4006/acc=0.8101)
  [mlp] layers=[512, 16, 256, 64] act=relu lr=0.0047 batch=128 epochs=15 -> avg_log_loss=0.4318 (folds: loss=0.3954/acc=0.8212, loss=0.4618/acc=0.7970, loss=0.4179/acc=0.8033, loss=0.4410/acc=0.7929, loss=0.4432/acc=0.7946)
  best avg log loss this gen: 0.3936
Generation 3/10
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3933 (folds: loss=0.3725/acc=0.8212, loss=0.4024/acc=0.8056, loss=0.3873/acc=0.8183, loss=0.4048/acc=0.8009, loss=0.3995/acc=0.8096)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3930 (folds: loss=0.3718/acc=0.8194, loss=0.4020/acc=0.8033, loss=0.3864/acc=0.8148, loss=0.4048/acc=0.8044, loss=0.4000/acc=0.8084)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4040 (folds: loss=0.3862/acc=0.8200, loss=0.4172/acc=0.8028, loss=0.3918/acc=0.8217, loss=0.4146/acc=0.7911, loss=0.4105/acc=0.8044)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.3999 (folds: loss=0.3804/acc=0.8194, loss=0.4126/acc=0.8045, loss=0.3895/acc=0.8206, loss=0.4099/acc=0.7929, loss=0.4071/acc=0.8084)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4040 (folds: loss=0.3857/acc=0.8189, loss=0.4174/acc=0.8051, loss=0.3921/acc=0.8200, loss=0.4142/acc=0.7923, loss=0.4105/acc=0.8044)
  [extratrees] extratrees trees=300 depth=7 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.3956 (folds: loss=0.3756/acc=0.8229, loss=0.4066/acc=0.8022, loss=0.3873/acc=0.8183, loss=0.4062/acc=0.7940, loss=0.4023/acc=0.8090)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=50 -> avg_log_loss=0.4399 (folds: loss=0.4194/acc=0.8056, loss=0.4457/acc=0.7832, loss=0.4265/acc=0.8016, loss=0.4608/acc=0.7842, loss=0.4472/acc=0.7940)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4035 (folds: loss=0.3838/acc=0.8206, loss=0.4159/acc=0.8074, loss=0.3913/acc=0.8143, loss=0.4158/acc=0.7934, loss=0.4106/acc=0.8032)
  [gbstump] gbstump estimators=300 lr=0.2 min_leaf=150 -> avg_log_loss=0.4455 (folds: loss=0.4254/acc=0.8028, loss=0.4490/acc=0.7901, loss=0.4343/acc=0.7913, loss=0.4665/acc=0.7808, loss=0.4523/acc=0.7929)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4101 (folds: loss=0.3913/acc=0.8189, loss=0.4228/acc=0.8028, loss=0.3965/acc=0.8166, loss=0.4226/acc=0.7883, loss=0.4172/acc=0.7986)
  [gbstump] gbstump estimators=200 lr=0.2 min_leaf=150 -> avg_log_loss=0.4426 (folds: loss=0.4222/acc=0.7987, loss=0.4470/acc=0.7890, loss=0.4310/acc=0.7941, loss=0.4624/acc=0.7808, loss=0.4505/acc=0.7929)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4114 (folds: loss=0.3939/acc=0.8143, loss=0.4249/acc=0.8016, loss=0.3978/acc=0.8166, loss=0.4219/acc=0.7894, loss=0.4183/acc=0.7998)
  best avg log loss this gen: 0.3930
Generation 4/10
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3935 (folds: loss=0.3718/acc=0.8223, loss=0.4027/acc=0.8045, loss=0.3874/acc=0.8154, loss=0.4054/acc=0.7969, loss=0.4003/acc=0.8107)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3933 (folds: loss=0.3722/acc=0.8212, loss=0.4025/acc=0.8045, loss=0.3880/acc=0.8154, loss=0.4045/acc=0.8026, loss=0.3994/acc=0.8090)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4034 (folds: loss=0.3837/acc=0.8212, loss=0.4156/acc=0.8039, loss=0.3910/acc=0.8177, loss=0.4155/acc=0.7923, loss=0.4111/acc=0.8055)
  [extratrees] extratrees trees=100 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.3958 (folds: loss=0.3747/acc=0.8223, loss=0.4075/acc=0.7964, loss=0.3874/acc=0.8143, loss=0.4069/acc=0.7980, loss=0.4027/acc=0.8067)
  [gbstump] gbstump estimators=200 lr=0.1 min_leaf=200 -> avg_log_loss=0.4363 (folds: loss=0.4154/acc=0.7982, loss=0.4427/acc=0.7895, loss=0.4259/acc=0.8010, loss=0.4580/acc=0.7837, loss=0.4394/acc=0.7998)
  [gbstump] gbstump estimators=300 lr=0.1 min_leaf=200 -> avg_log_loss=0.4375 (folds: loss=0.4163/acc=0.8005, loss=0.4424/acc=0.7872, loss=0.4279/acc=0.8005, loss=0.4603/acc=0.7837, loss=0.4407/acc=0.7992)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.3953 (folds: loss=0.3746/acc=0.8235, loss=0.4062/acc=0.8033, loss=0.3876/acc=0.8171, loss=0.4057/acc=0.7980, loss=0.4024/acc=0.8072)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.3955 (folds: loss=0.3757/acc=0.8194, loss=0.4059/acc=0.8039, loss=0.3871/acc=0.8200, loss=0.4066/acc=0.7975, loss=0.4022/acc=0.8049)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.3995 (folds: loss=0.3801/acc=0.8235, loss=0.4116/acc=0.8022, loss=0.3888/acc=0.8183, loss=0.4106/acc=0.7946, loss=0.4064/acc=0.8067)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.3962 (folds: loss=0.3758/acc=0.8217, loss=0.4078/acc=0.8039, loss=0.3874/acc=0.8171, loss=0.4068/acc=0.7969, loss=0.4031/acc=0.8072)
  [rf] rf trees=300 depth=4 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4319 (folds: loss=0.4093/acc=0.8068, loss=0.4368/acc=0.7867, loss=0.4224/acc=0.8028, loss=0.4514/acc=0.7796, loss=0.4394/acc=0.7883)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.3951 (folds: loss=0.3746/acc=0.8194, loss=0.4062/acc=0.8045, loss=0.3872/acc=0.8183, loss=0.4060/acc=0.7998, loss=0.4016/acc=0.8096)
  best avg log loss this gen: 0.3933
Generation 5/10
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3931 (folds: loss=0.3723/acc=0.8194, loss=0.4024/acc=0.8045, loss=0.3862/acc=0.8148, loss=0.4057/acc=0.7998, loss=0.3990/acc=0.8078)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3933 (folds: loss=0.3720/acc=0.8240, loss=0.4018/acc=0.8051, loss=0.3877/acc=0.8154, loss=0.4056/acc=0.7998, loss=0.3995/acc=0.8107)
  [gbstump] gbstump estimators=100 lr=0.1 min_leaf=50 -> avg_log_loss=0.4354 (folds: loss=0.4141/acc=0.8062, loss=0.4431/acc=0.7861, loss=0.4214/acc=0.8033, loss=0.4543/acc=0.7842, loss=0.4441/acc=0.7940)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4034 (folds: loss=0.3846/acc=0.8200, loss=0.4156/acc=0.8051, loss=0.3921/acc=0.8171, loss=0.4141/acc=0.7911, loss=0.4104/acc=0.8026)
  [extratrees] extratrees trees=100 depth=6 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.3998 (folds: loss=0.3793/acc=0.8223, loss=0.4114/acc=0.8062, loss=0.3901/acc=0.8166, loss=0.4108/acc=0.7963, loss=0.4073/acc=0.8072)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4097 (folds: loss=0.3915/acc=0.8171, loss=0.4227/acc=0.8022, loss=0.3956/acc=0.8177, loss=0.4220/acc=0.7894, loss=0.4167/acc=0.8003)
  [mlp] layers=[128, 64, 256, 256, 512, 128] act=leaky_relu lr=0.0036 batch=256 epochs=10 -> avg_log_loss=0.4016 (folds: loss=0.3795/acc=0.8171, loss=0.4210/acc=0.8022, loss=0.3950/acc=0.8252, loss=0.4024/acc=0.7963, loss=0.4103/acc=0.7946)
  [extratrees] extratrees trees=200 depth=4 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4097 (folds: loss=0.3915/acc=0.8177, loss=0.4228/acc=0.8033, loss=0.3960/acc=0.8183, loss=0.4221/acc=0.7888, loss=0.4162/acc=0.8009)
  [extratrees] extratrees trees=400 depth=6 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.3992 (folds: loss=0.3798/acc=0.8217, loss=0.4110/acc=0.8039, loss=0.3894/acc=0.8171, loss=0.4096/acc=0.7911, loss=0.4064/acc=0.8072)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.3966 (folds: loss=0.3764/acc=0.8212, loss=0.4065/acc=0.7999, loss=0.3884/acc=0.8137, loss=0.4077/acc=0.7980, loss=0.4039/acc=0.8049)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.3962 (folds: loss=0.3751/acc=0.8200, loss=0.4066/acc=0.8045, loss=0.3878/acc=0.8183, loss=0.4084/acc=0.7952, loss=0.4033/acc=0.8026)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4027 (folds: loss=0.3835/acc=0.8212, loss=0.4157/acc=0.8074, loss=0.3908/acc=0.8166, loss=0.4135/acc=0.7923, loss=0.4104/acc=0.8067)
  best avg log loss this gen: 0.3931
Generation 6/10
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3931 (folds: loss=0.3719/acc=0.8183, loss=0.4030/acc=0.8022, loss=0.3859/acc=0.8154, loss=0.4050/acc=0.8015, loss=0.3996/acc=0.8072)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3931 (folds: loss=0.3724/acc=0.8235, loss=0.4017/acc=0.8051, loss=0.3876/acc=0.8166, loss=0.4051/acc=0.8032, loss=0.3987/acc=0.8096)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.3999 (folds: loss=0.3802/acc=0.8217, loss=0.4119/acc=0.8045, loss=0.3892/acc=0.8148, loss=0.4110/acc=0.7940, loss=0.4071/acc=0.8044)
  [rf] rf trees=400 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4270 (folds: loss=0.4036/acc=0.8074, loss=0.4301/acc=0.7844, loss=0.4222/acc=0.8016, loss=0.4454/acc=0.7785, loss=0.4338/acc=0.7923)
bd  [rf] rf trees=400 depth=8 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4350 (folds: loss=0.4131/acc=0.8102, loss=0.4352/acc=0.7930, loss=0.4296/acc=0.8010, loss=0.4469/acc=0.7802, loss=0.4500/acc=0.7877)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.3944 (folds: loss=0.3731/acc=0.8200, loss=0.4048/acc=0.8039, loss=0.3875/acc=0.8166, loss=0.4047/acc=0.8015, loss=0.4016/acc=0.8067)
  [histgb] histgb trees=400 depth=8 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4281 (folds: loss=0.4091/acc=0.8068, loss=0.4384/acc=0.7838, loss=0.4189/acc=0.7947, loss=0.4444/acc=0.7848, loss=0.4294/acc=0.7980)
  [histgb] histgb trees=200 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4369 (folds: loss=0.4181/acc=0.8005, loss=0.4390/acc=0.7838, loss=0.4318/acc=0.7913, loss=0.4520/acc=0.7842, loss=0.4436/acc=0.7980)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.3955 (folds: loss=0.3739/acc=0.8246, loss=0.4073/acc=0.8016, loss=0.3878/acc=0.8200, loss=0.4066/acc=0.7952, loss=0.4019/acc=0.8072)
  [extratrees] extratrees trees=100 depth=6 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.3997 (folds: loss=0.3805/acc=0.8235, loss=0.4115/acc=0.8051, loss=0.3894/acc=0.8183, loss=0.4096/acc=0.7894, loss=0.4076/acc=0.8067)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4026 (folds: loss=0.3834/acc=0.8235, loss=0.4156/acc=0.8016, loss=0.3908/acc=0.8194, loss=0.4134/acc=0.7940, loss=0.4097/acc=0.8026)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.3970 (folds: loss=0.3765/acc=0.8177, loss=0.4073/acc=0.8045, loss=0.3875/acc=0.8171, loss=0.4090/acc=0.7969, loss=0.4047/acc=0.8021)
  best avg log loss this gen: 0.3931
Generation 7/10
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3932 (folds: loss=0.3718/acc=0.8194, loss=0.4019/acc=0.8068, loss=0.3862/acc=0.8171, loss=0.4061/acc=0.8032, loss=0.3999/acc=0.8078)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3932 (folds: loss=0.3716/acc=0.8229, loss=0.4022/acc=0.8074, loss=0.3874/acc=0.8177, loss=0.4055/acc=0.8032, loss=0.3994/acc=0.8084)
  [extratrees] extratrees trees=400 depth=6 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.3994 (folds: loss=0.3794/acc=0.8206, loss=0.4112/acc=0.8051, loss=0.3895/acc=0.8171, loss=0.4101/acc=0.7969, loss=0.4069/acc=0.8049)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.3968 (folds: loss=0.3760/acc=0.8183, loss=0.4080/acc=0.8045, loss=0.3878/acc=0.8166, loss=0.4081/acc=0.7969, loss=0.4039/acc=0.8038)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4115 (folds: loss=0.3944/acc=0.8160, loss=0.4243/acc=0.7999, loss=0.3969/acc=0.8160, loss=0.4238/acc=0.7883, loss=0.4182/acc=0.7986)
  [extratrees] extratrees trees=200 depth=4 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4145 (folds: loss=0.3970/acc=0.8102, loss=0.4287/acc=0.7953, loss=0.4000/acc=0.8183, loss=0.4259/acc=0.7842, loss=0.4208/acc=0.7986)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=50 -> avg_log_loss=0.4399 (folds: loss=0.4194/acc=0.8056, loss=0.4457/acc=0.7832, loss=0.4265/acc=0.8016, loss=0.4607/acc=0.7854, loss=0.4472/acc=0.7940)
  [rf] rf trees=400 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4280 (folds: loss=0.4034/acc=0.8074, loss=0.4238/acc=0.7913, loss=0.4219/acc=0.8051, loss=0.4441/acc=0.7808, loss=0.4470/acc=0.7911)
  [extratrees] extratrees trees=100 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.3936 (folds: loss=0.3729/acc=0.8212, loss=0.4019/acc=0.8028, loss=0.3880/acc=0.8120, loss=0.4055/acc=0.7963, loss=0.3996/acc=0.8096)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4006 (folds: loss=0.3814/acc=0.8229, loss=0.4133/acc=0.8028, loss=0.3902/acc=0.8166, loss=0.4110/acc=0.7900, loss=0.4072/acc=0.8078)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3957 (folds: loss=0.3752/acc=0.8194, loss=0.4062/acc=0.8033, loss=0.3882/acc=0.8166, loss=0.4066/acc=0.7980, loss=0.4021/acc=0.8055)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4061 (folds: loss=0.3879/acc=0.8217, loss=0.4206/acc=0.7999, loss=0.3934/acc=0.8217, loss=0.4159/acc=0.7923, loss=0.4127/acc=0.8044)
  best avg log loss this gen: 0.3932
Generation 8/10
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3931 (folds: loss=0.3712/acc=0.8229, loss=0.4023/acc=0.8062, loss=0.3872/acc=0.8171, loss=0.4054/acc=0.7986, loss=0.3993/acc=0.8101)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3935 (folds: loss=0.3719/acc=0.8212, loss=0.4025/acc=0.8033, loss=0.3879/acc=0.8171, loss=0.4054/acc=0.7980, loss=0.3998/acc=0.8101)
  [rf] rf trees=200 depth=6 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4189 (folds: loss=0.3935/acc=0.8114, loss=0.4193/acc=0.7941, loss=0.4126/acc=0.8010, loss=0.4362/acc=0.7773, loss=0.4330/acc=0.7934)
  [mlp] layers=[32, 16, 256, 32, 64, 64] act=gelu lr=0.0043 batch=128 epochs=10 -> avg_log_loss=0.3963 (folds: loss=0.3866/acc=0.8183, loss=0.4158/acc=0.7924, loss=0.3787/acc=0.8292, loss=0.4011/acc=0.7917, loss=0.3992/acc=0.8084)
  [rf] rf trees=200 depth=8 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4293 (folds: loss=0.4035/acc=0.8079, loss=0.4274/acc=0.7878, loss=0.4282/acc=0.8022, loss=0.4448/acc=0.7865, loss=0.4427/acc=0.7888)
  [extratrees] extratrees trees=400 depth=4 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4141 (folds: loss=0.3967/acc=0.8102, loss=0.4281/acc=0.7959, loss=0.4003/acc=0.8166, loss=0.4245/acc=0.7831, loss=0.4208/acc=0.7998)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.3950 (folds: loss=0.3737/acc=0.8194, loss=0.4043/acc=0.8079, loss=0.3883/acc=0.8171, loss=0.4077/acc=0.8015, loss=0.4012/acc=0.8049)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3954 (folds: loss=0.3747/acc=0.8246, loss=0.4046/acc=0.8039, loss=0.3889/acc=0.8114, loss=0.4060/acc=0.7980, loss=0.4027/acc=0.8055)
  [mlp] layers=[32] act=relu lr=0.0043 batch=256 epochs=15 -> avg_log_loss=0.3925 (folds: loss=0.3765/acc=0.8177, loss=0.4166/acc=0.7884, loss=0.3791/acc=0.8286, loss=0.3956/acc=0.7986, loss=0.3947/acc=0.8113)
  [extratrees] extratrees trees=100 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.3944 (folds: loss=0.3731/acc=0.8189, loss=0.4026/acc=0.8091, loss=0.3884/acc=0.8194, loss=0.4070/acc=0.8038, loss=0.4006/acc=0.8096)
  [rf] rf trees=100 depth=8 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4249 (folds: loss=0.4005/acc=0.8091, loss=0.4233/acc=0.7930, loss=0.4205/acc=0.8045, loss=0.4425/acc=0.7802, loss=0.4375/acc=0.7917)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.3984 (folds: loss=0.3785/acc=0.8177, loss=0.4106/acc=0.8045, loss=0.3879/acc=0.8171, loss=0.4093/acc=0.7923, loss=0.4058/acc=0.8044)
  best avg log loss this gen: 0.3925
Generation 9/10
  [mlp] layers=[32] act=relu lr=0.0043 batch=256 epochs=15 -> avg_log_loss=0.3918 (folds: loss=0.3800/acc=0.8137, loss=0.4141/acc=0.7970, loss=0.3764/acc=0.8281, loss=0.3962/acc=0.7929, loss=0.3925/acc=0.8038)
  [extratrees] extratrees trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3936 (folds: loss=0.3726/acc=0.8200, loss=0.4026/acc=0.8074, loss=0.3870/acc=0.8137, loss=0.4058/acc=0.8032, loss=0.3998/acc=0.8090)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.3976 (folds: loss=0.3765/acc=0.8200, loss=0.4095/acc=0.8033, loss=0.3895/acc=0.8177, loss=0.4091/acc=0.7957, loss=0.4035/acc=0.8015)
  [gbstump] gbstump estimators=200 lr=0.2 min_leaf=200 -> avg_log_loss=0.4391 (folds: loss=0.4191/acc=0.7999, loss=0.4420/acc=0.7924, loss=0.4305/acc=0.7970, loss=0.4622/acc=0.7819, loss=0.4419/acc=0.7952)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.3982 (folds: loss=0.3774/acc=0.8177, loss=0.4090/acc=0.8028, loss=0.3886/acc=0.8166, loss=0.4103/acc=0.7952, loss=0.4056/acc=0.8032)
  [mlp] layers=[16] act=relu lr=0.0042 batch=128 epochs=25 -> avg_log_loss=0.3921 (folds: loss=0.3774/acc=0.8246, loss=0.4098/acc=0.7959, loss=0.3806/acc=0.8131, loss=0.3945/acc=0.7946, loss=0.3983/acc=0.8044)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4047 (folds: loss=0.3864/acc=0.8194, loss=0.4182/acc=0.8022, loss=0.3928/acc=0.8200, loss=0.4149/acc=0.7929, loss=0.4112/acc=0.8044)
  [extratrees] extratrees trees=400 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4034 (folds: loss=0.3839/acc=0.8223, loss=0.4167/acc=0.8051, loss=0.3913/acc=0.8183, loss=0.4145/acc=0.7911, loss=0.4107/acc=0.8015)
  [extratrees] extratrees trees=100 depth=8 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.3935 (folds: loss=0.3729/acc=0.8229, loss=0.4038/acc=0.8039, loss=0.3872/acc=0.8131, loss=0.4046/acc=0.7986, loss=0.3992/acc=0.8119)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.3993 (folds: loss=0.3789/acc=0.8223, loss=0.4099/acc=0.8028, loss=0.3898/acc=0.8166, loss=0.4109/acc=0.7946, loss=0.4068/acc=0.8044)
  [mlp] layers=[16] act=relu lr=0.0042 batch=256 epochs=30 -> avg_log_loss=0.3918 (folds: loss=0.3801/acc=0.8125, loss=0.4078/acc=0.7959, loss=0.3828/acc=0.8189, loss=0.3964/acc=0.8015, loss=0.3920/acc=0.7969)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.3986 (folds: loss=0.3790/acc=0.8223, loss=0.4108/acc=0.8033, loss=0.3896/acc=0.8154, loss=0.4084/acc=0.7934, loss=0.4052/acc=0.8049)
  best avg log loss this gen: 0.3918
Generation 10/10
  [mlp] layers=[16] act=relu lr=0.0042 batch=256 epochs=30 -> avg_log_loss=0.3914 (folds: loss=0.3780/acc=0.8125, loss=0.4125/acc=0.7987, loss=0.3779/acc=0.8217, loss=0.3959/acc=0.7911, loss=0.3928/acc=0.8032)
  [mlp] layers=[32] act=relu lr=0.0043 batch=256 epochs=15 -> avg_log_loss=0.3917 (folds: loss=0.3741/acc=0.8246, loss=0.4163/acc=0.7982, loss=0.3790/acc=0.8189, loss=0.3932/acc=0.7992, loss=0.3958/acc=0.8044)
  [mlp] layers=[128] act=relu lr=0.0050 batch=512 epochs=20 -> avg_log_loss=0.3967 (folds: loss=0.3808/acc=0.8189, loss=0.4198/acc=0.7959, loss=0.3847/acc=0.8114, loss=0.3993/acc=0.7906, loss=0.3989/acc=0.8055)
  [mlp] layers=[16, 256, 128, 128, 128] act=relu lr=0.0041 batch=64 epochs=15 -> avg_log_loss=0.3932 (folds: loss=0.3835/acc=0.8189, loss=0.4123/acc=0.7993, loss=0.3819/acc=0.8258, loss=0.3987/acc=0.7929, loss=0.3896/acc=0.7975)
  [mlp] layers=[32] act=relu lr=0.0046 batch=256 epochs=30 -> avg_log_loss=0.3907 (folds: loss=0.3742/acc=0.8189, loss=0.4153/acc=0.7941, loss=0.3795/acc=0.8229, loss=0.3917/acc=0.8015, loss=0.3926/acc=0.8078)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.3964 (folds: loss=0.3757/acc=0.8194, loss=0.4057/acc=0.8045, loss=0.3890/acc=0.8148, loss=0.4084/acc=0.7980, loss=0.4030/acc=0.8044)
  [rf] rf trees=100 depth=6 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4304 (folds: loss=0.4095/acc=0.8079, loss=0.4295/acc=0.7913, loss=0.4283/acc=0.7982, loss=0.4437/acc=0.7860, loss=0.4411/acc=0.7917)
  [mlp] layers=[32] act=relu lr=0.0050 batch=256 epochs=20 -> avg_log_loss=0.3917 (folds: loss=0.3740/acc=0.8189, loss=0.4156/acc=0.7930, loss=0.3801/acc=0.8240, loss=0.3921/acc=0.7986, loss=0.3966/acc=0.8072)
  [extratrees] extratrees trees=400 depth=6 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4002 (folds: loss=0.3805/acc=0.8246, loss=0.4130/acc=0.8016, loss=0.3900/acc=0.8200, loss=0.4104/acc=0.7917, loss=0.4073/acc=0.8067)
  [mlp] layers=[64] act=relu lr=0.0037 batch=256 epochs=15 -> avg_log_loss=0.3934 (folds: loss=0.3755/acc=0.8217, loss=0.4161/acc=0.7913, loss=0.3789/acc=0.8200, loss=0.4041/acc=0.7929, loss=0.3923/acc=0.8113)
  [mlp] layers=[512] act=relu lr=0.0040 batch=128 epochs=30 -> avg_log_loss=0.4151 (folds: loss=0.3952/acc=0.8200, loss=0.4393/acc=0.7964, loss=0.3955/acc=0.8114, loss=0.4207/acc=0.7842, loss=0.4250/acc=0.7946)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.3959 (folds: loss=0.3748/acc=0.8171, loss=0.4064/acc=0.8028, loss=0.3887/acc=0.8148, loss=0.4076/acc=0.7969, loss=0.4022/acc=0.8026)
  best avg log loss this gen: 0.3907
Retraining top genome #1 (mlp) with val_log_loss=0.3907
Saved model to models/ga_nn_1_1768502994.pt and submission to submissions/ga_nn_1_1768502994.csv
Retraining top genome #2 (mlp) with val_log_loss=0.3907
Saved model to models/ga_nn_2_1768502994.pt and submission to submissions/ga_nn_2_1768502994.csv
Backed up previous summary to models/ga_search_summary_20260115-184954.json
Wrote GA summary to models/ga_search_summary.json
[day3] New GA best val_log_loss: 0.3907
[day3] Improvement over baseline: 0.0095
[day3] Improvement threshold satisfied. You may submit the new GA model.
root@6a432f30b2e3:/workspace# kaggle competitions submit -c spaceship-titanic -f submissions/ga_nn_1_1768502994.csv -m "Day3 GA mlp val_log_loss 0.3907"
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.4k/56.4k [00:00<00:00, 98.9kB/s]
Successfully submitted to Spackaggle competitions submit -c spaceship-titanic -f models/day1_stack/stack_meta_submission.csv -m "Day1 stacker"^C
root@6a432f30b2e3:/workspace# ^C
root@6a432f30b2e3:/workspace# vim trim.sh^C
root@6a432f30b2e3:/workspace# python scripts/build_stack_meta.py \
    --train data/train_prepared.csv \
    --test data/test_prepared.csv \
    --raw-test data/test.csv \
    --top-configs models/day3_top_configs.json \
    --folds 5 \
    --seed 2026 \
    --baseline-columns LR_pred,GB_pred \
    --allowed-types rf,extratrees,gbstump,histgb \
    --output-dir models/day3_stack
[stack] Loaded 12 configs from models/day3_top_configs.json
[stack] Generating OOF predictions for ga_rank_1_ts_1768489230_logloss_0_4002 (extratrees)
[stack] Generating OOF predictions for ga_rank_2_ts_1768489232_logloss_0_4002 (extratrees)
[stack] Generating OOF predictions for ga_rank_3_ts_1768489234_logloss_0_4002 (extratrees)
[stack] Generating OOF predictions for ga_rank_1_ts_1768337411_logloss_0_3960 (extratrees)
[stack] Generating OOF predictions for ga_rank_1_ts_1768344940_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_2_ts_1768344941_logloss_0_3887 (gbstump)
[stack] Generating OOF predictions for ga_rank_3_ts_1768344941_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_4_ts_1768344941_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_5_ts_1768344943_logloss_0_3887 (gbstump)
[stack] Generating OOF predictions for ga_rank_6_ts_1768344943_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_7_ts_1768344943_logloss_0_3889 (extratrees)
[stack] Generating OOF predictions for ga_rank_8_ts_1768344944_logloss_0_3889 (extratrees)
[stack] Saved train features to models/day3_stack/stack_train_features.csv
[stack] Saved test features to models/day3_stack/stack_test_features.csv
[stack] Saved meta-model to models/day3_stack/stack_meta_model.joblib
[stack] Saved submission preview to models/day3_stack/stack_meta_submission.csv
root@6a432f30b2e3:/workspace# vim models/day3_stack/stack_meta_submission.csv
root@6a432f30b2e3:/workspace# vim trim.sh
root@6a432f30b2e3:/workspace# ./trim.sh
Trimmed submission saved to submissions/day3_stack_meta_submission_trimmed.csv
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.4k/56.4k [00:00<00:00, 97.6kB/s]
Successfully submitted to Spacpython scripts/ensemble_submissions.py \e# python scripts/ensemble_submissions.py \
    submissions/ga_nn_1_1768502994.csv \
    submissions/stack_meta_submission_trimmed.csv \
    --output submissions/ga_stack_blend.csv
Wrote ensemble submission to submissions/ga_stack_blend.csv
root@6a432f30b2e3:/workspace# kaggle competitions submit -c spaceship-titanic -f submissions/ga_stack_blend.csv -m "Day3 GA + Day1 stack blend"
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.2k/56.2k [00:00<00:00, 102kB/s]
root@6a432f30b2e3:/workspace# python scripts/run_day4_ensembles.py \ace# python scripts/run_day4_ensembles.py \
    --day2 submissions/ga_nn_1_1768489230.csv submissions/ga_nn_2_1768489232.csv submissions/ga_nn_3_1768489234.csv \
    --historical submissions/ga_nn_1_1768184686.csv submissions/ga_nn_2_1768184947.csv submissions/ga_nn_1_1768502994.csv \
    --stacker submissions/stack_meta_submission_trimmed.csv \
    --output-dir models/day4_ensembles \
    --keep-prob
Traceback (most recent call last):
  File "/workspace/scripts/run_day4_ensembles.py", line 302, in <module>
    main()
  File "/workspace/scripts/run_day4_ensembles.py", line 193, in main
    hist_group = aggregate_group("historical", args.historical)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/scripts/run_day4_ensembles.py", line 96, in aggregate_group
    ids, probs = load_submission_probs(Path(file_path))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/scripts/run_day4_ensembles.py", line 70, in load_submission_probs
    df = pd.read_csv(path)
         ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py", line 1880, in _make_engine
    self.handles = get_handle(
                   ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/pandas/io/common.py", line 873, in get_handle
    handle = open(
             ^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'submissions/ga_nn_1_1768184686.csv'
root@6a432f30b2e3:/workspace# ls submissions/
day3_stack_meta_submission_trimmed.csv  ga_nn_1_1768344940.csv  ga_nn_2_1768344941.csv  ga_nn_3_1768344941.csv  ga_nn_5_1768344943.csv  ga_nn_8_1768344944.csv  stack_meta_submission_trimmed.csv
ga_nn_10_1768344944.csv                 ga_nn_1_1768489230.csv  ga_nn_2_1768489232.csv  ga_nn_3_1768489234.csv  ga_nn_6_1768344943.csv  ga_nn_9_1768344944.csv
ga_nn_1_1768337411.csv                  ga_nn_1_1768502994.csv  ga_nn_2_1768502994.csv  ga_nn_4_1768344941.csv  ga_nn_7_1768344943.csv  ga_stack_blend.csv
root@6a432f30b2e3:/workspace# python scripts/run_day4_ensembles.py \
    --day2 \
      submissions/ga_nn_1_1768502994.csv \
      submissions/ga_nn_2_1768502994.csv \
      submissions/ga_nn_1_1768489230.csv \
      submissions/ga_nn_2_1768489232.csv \
      submissions/ga_nn_3_1768489234.csv \
    --historical \
      submissions/ga_nn_1_1768344940.csv \
      submissions/ga_nn_2_1768344941.csv \
      submissions/ga_nn_3_1768344941.csv \
      submissions/ga_nn_4_1768344941.csv \
      submissions/ga_nn_5_1768344943.csv \
      submissions/ga_nn_6_1768344943.csv \
      submissions/ga_nn_7_1768344943.csv \
      submissions/ga_nn_8_1768344944.csv \
      submissions/ga_nn_9_1768344944.csv \
      submissions/ga_nn_10_1768344944.csv \
      submissions/ga_stack_blend.csv \
    --stacker submissions/day3_stack_meta_submission_trimmed.csv \
    --output-dir models/day4_ensembles \
    --keep-prob
[day4] wrote ensemble to models/day4_ensembles/ensemble_day2_stacker.csv
[day4] wrote ensemble to models/day4_ensembles/ensemble_historical_stacker.csv
[day4] wrote ensemble to models/day4_ensembles/ensemble_weighted_hybrid.csv
[day4] wrote ensemble to models/day4_ensembles/ensemble_lowcorr_historical_stacker.csv
[day4] wrote summary to models/day4_ensembles/day4_summary.json
root@6a432f30b2e3:/workspace# kaggle competitions submit -c spaceship-titanic \
       -f models/day4_ensembles/ensemble_weighted_hybrid.csv \
       -m "Day4 weighted hybrid"
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 81.8k/81.8k [00:00<00:00, 125kB/s]
Successfully submitted to Spackaggle competitions submit -c spaceship-titanic \
       -f models/day4_ensembles/ensemble_lowcorr_historical_stacker.csv \
       -m "Day4 low-corr blend"
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80.8k/80.8k [00:00<00:00, 127kB/s]
Successfully submitted to Spaceship Titanicroot@6a432f30b2e3:/workspace# vim models/day4_ensembles/ensemble_weighted_hybrid.csv
root@6a432f30b2e3:/workspace# vim trim.sh
root@6a432f30b2e3:/workspace# ./trim.sh
Trimmed submission saved to models/day4_ensembles/ensemble_weighted_hybrid_trimmed.csv
root@6a432f30b2e3:/workspace# kaggle competitions submit -c spaceship-titanic        -f models/day4_ensembles/ensemble_weighted_hybrid_trimmed.csv        -m "Day4 weighted hybrid"
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.3k/56.3k [00:00<00:00, 98.7kB/s]
Successfully submitted to Spaceship Titanicroot@6a432f30b2e3:/workspace# vim trim.sh
root@6a432f30b2e3:/workspace# ./trim.sh
Trimmed submission saved to models/day4_ensembles/ensemble_lowcorr_historical_stacker_trimmed.csv
root@6a432f30b2e3:/workspace# kaggle competitions submit -c spaceship-titanic        -f models/day4_ensembles/ensemble_lowcorr_historical_stacker_trimmed.csv        -m "Day4 low-corr blend"
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.3k/56.3k [00:00<00:00, 98.5kB/s]
Successfully submitted to Spac python scripts/run_day5_push.py \rkspace#  python scripts/run_day5_push.py \
    --prepare-data \
    --stacker-submission submissions/day3_stack_meta_submission_trimmed.csv \
    --stack-top-n 15 \
    --ga-generations 20 \
    --ga-population 18 \
    --ga-cv-folds 8 \
    --ga-top-k 5 \
    --submit-ga  # omit this flag if you want to review before submitting
[day5] $ python scripts/prepare_data.py --input data/train.csv --output data/train_prepared.csv --exclude-columns Transported
Wrote processed data to data/train_prepared.csv
[day5] $ python scripts/prepare_data.py --input data/test.csv --output data/test_prepared.csv
Wrote processed data to data/test_prepared.csv
[day5] $ python scripts/add_baseline_predictions.py --train data/train_prepared.csv --test data/test_prepared.csv --target-column Transported
[baseline] Training logistic regression on device cpu.
[baseline] Training gradient boosting stumps.
Wrote augmented training data to data/train_prepared.csv
Wrote augmented test data to data/test_prepared.csv
[day5] $ python scripts/select_top_submissions.py --summaries-dir models --pattern ga_search_summary*.json --top 15 --output models/day5_top_configs.json --include-missing
Wrote top 15 entries to models/day5_top_configs.json
[day5] $ python scripts/build_stack_meta.py --train data/train_prepared.csv --test data/test_prepared.csv --raw-test data/test.csv --top-configs models/day5_top_configs.json --folds 5 --seed 2027 --allowed-types histgb,rf,extratrees --output-
dir models/day5_stack
[stack] Loaded 11 configs from models/day5_top_configs.json
[stack] Generating OOF predictions for ga_rank_1_ts_1768337411_logloss_0_3960 (extratrees)
[stack] Generating OOF predictions for ga_rank_1_ts_1768344940_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_3_ts_1768344941_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_4_ts_1768344941_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_6_ts_1768344943_logloss_0_3887 (extratrees)
[stack] Generating OOF predictions for ga_rank_7_ts_1768344943_logloss_0_3889 (extratrees)
[stack] Generating OOF predictions for ga_rank_8_ts_1768344944_logloss_0_3889 (extratrees)
[stack] Generating OOF predictions for ga_rank_9_ts_1768344944_logloss_0_3889 (extratrees)
[stack] Generating OOF predictions for ga_rank_10_ts_1768344944_logloss_0_3889 (rf)
[stack] Generating OOF predictions for ga_rank_1_ts_1768489230_logloss_0_4002 (extratrees)
[stack] Generating OOF predictions for ga_rank_2_ts_1768489232_logloss_0_4002 (extratrees)
[stack] Saved train features to models/day5_stack/stack_train_features.csv
[stack] Saved test features to models/day5_stack/stack_test_features.csv
[stack] Saved meta-model to models/day5_stack/stack_meta_model.joblib
[stack] Saved submission preview to models/day5_stack/stack_meta_submission.csv
[day5] $ python scripts/train_ga_nn.py --train-prepared data/train_prepared.csv --test-prepared data/test_prepared.csv --raw-test data/test.csv --generations 20 --population 18 --top-k 5 --cv-folds 8 --resume-summary models/ga_search_summary.
json --resume-top-n 15 --seed 2045
Using stratified 8-fold CV for GA fitness.
Using device: cpu
Loaded 2 genomes from models/ga_search_summary.json to seed the population.
Generation 1/20
  [mlp] layers=[32] act=relu lr=0.0046 batch=256 epochs=30 -> avg_log_loss=0.3918 (folds: loss=0.3793/acc=0.8097, loss=0.4128/acc=0.7960, loss=0.4449/acc=0.7764, loss=0.3871/acc=0.8094, loss=0.3791/acc=0.8241, loss=0.3840/acc=0.8168, loss=0.3
510/acc=0.8315, loss=0.3963/acc=0.7983)
  [mlp] layers=[128] act=relu lr=0.0041 batch=512 epochs=15 -> avg_log_loss=0.3944 (folds: loss=0.3907/acc=0.8070, loss=0.4146/acc=0.7996, loss=0.4493/acc=0.7737, loss=0.3920/acc=0.8066, loss=0.3832/acc=0.8223, loss=0.3810/acc=0.8103, loss=0.
3507/acc=0.8269, loss=0.3937/acc=0.7993)
  [gbstump] gbstump estimators=300 lr=0.2 min_leaf=50 -> avg_log_loss=0.4485 (folds: loss=0.4361/acc=0.7822, loss=0.4531/acc=0.7812, loss=0.4874/acc=0.7645, loss=0.4547/acc=0.7855, loss=0.4236/acc=0.8112, loss=0.4463/acc=0.7882, loss=0.4248/a
cc=0.8149, loss=0.4617/acc=0.7827)
  [gbstump] gbstump estimators=300 lr=0.1 min_leaf=100 -> avg_log_loss=0.4411 (folds: loss=0.4313/acc=0.7840, loss=0.4476/acc=0.7831, loss=0.4782/acc=0.7755, loss=0.4457/acc=0.7928, loss=0.4217/acc=0.8112, loss=0.4416/acc=0.7956, loss=0.4183/
acc=0.8232, loss=0.4442/acc=0.8002)
  [mlp] layers=[128, 32, 32] act=relu lr=0.0020 batch=64 epochs=10 -> avg_log_loss=0.4160 (folds: loss=0.3995/acc=0.8015, loss=0.4472/acc=0.7858, loss=0.4795/acc=0.7737, loss=0.4054/acc=0.8011, loss=0.4147/acc=0.8085, loss=0.3961/acc=0.8158,
loss=0.3671/acc=0.8361, loss=0.4183/acc=0.8011)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4102 (folds: loss=0.4025/acc=0.7969, loss=0.4295/acc=0.7960, loss=0.4497/acc=0.7902, loss=0.4119/acc=0.7983, loss=0.3902/acc=0.8149, loss=0.4154/acc
=0.8112, loss=0.3767/acc=0.8278, loss=0.4056/acc=0.8076)
  [rf] rf trees=400 depth=6 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4309 (folds: loss=0.4306/acc=0.7932, loss=0.4438/acc=0.7739, loss=0.4819/acc=0.7663, loss=0.4294/acc=0.8011, loss=0.4137/acc=0.8066, loss=0.4175/acc=0.8002, loss=0.
4025/acc=0.8204, loss=0.4279/acc=0.8122)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.3943 (folds: loss=0.3846/acc=0.7969, loss=0.4166/acc=0.7960, loss=0.4377/acc=0.7764, loss=0.3971/acc=0.8085, loss=0.3764/acc=0.8140, loss=0.3929/acc
=0.8158, loss=0.3636/acc=0.8306, loss=0.3856/acc=0.8223)
  [mlp] layers=[16] act=relu lr=0.0030 batch=128 epochs=30 -> avg_log_loss=0.3895 (folds: loss=0.3782/acc=0.7987, loss=0.4085/acc=0.8015, loss=0.4415/acc=0.7728, loss=0.3881/acc=0.8168, loss=0.3711/acc=0.8232, loss=0.3819/acc=0.8214, loss=0.3
539/acc=0.8232, loss=0.3928/acc=0.8076)
  [rf] rf trees=200 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4389 (folds: loss=0.4425/acc=0.7941, loss=0.4493/acc=0.7785, loss=0.4982/acc=0.7654, loss=0.4388/acc=0.8039, loss=0.4161/acc=0.8168, loss=0.4251/acc=0.8011, loss=0.
4076/acc=0.8122, loss=0.4339/acc=0.8076)
  [gbstump] gbstump estimators=300 lr=0.1 min_leaf=200 -> avg_log_loss=0.4415 (folds: loss=0.4310/acc=0.7849, loss=0.4493/acc=0.7812, loss=0.4780/acc=0.7728, loss=0.4452/acc=0.7910, loss=0.4243/acc=0.8066, loss=0.4408/acc=0.7974, loss=0.4184/
acc=0.8223, loss=0.4448/acc=0.8039)
  [mlp] layers=[32, 64, 512, 256] act=relu lr=0.0005 batch=256 epochs=30 -> avg_log_loss=0.3949 (folds: loss=0.3830/acc=0.8015, loss=0.4137/acc=0.7960, loss=0.4367/acc=0.7746, loss=0.3986/acc=0.8094, loss=0.3828/acc=0.8085, loss=0.3861/acc=0.
8140, loss=0.3621/acc=0.8223, loss=0.3960/acc=0.8122)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3956 (folds: loss=0.3849/acc=0.7914, loss=0.4176/acc=0.8006, loss=0.4382/acc=0.7856, loss=0.3991/acc=0.8112, loss=0.3777/acc=0.8158, loss=0.3948/acc
=0.8140, loss=0.3655/acc=0.8324, loss=0.3865/acc=0.8223)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4048 (folds: loss=0.3953/acc=0.7969, loss=0.4260/acc=0.7960, loss=0.4442/acc=0.7893, loss=0.4050/acc=0.8057, loss=0.3887/acc=0.8177, loss=0.4087/acc
=0.8103, loss=0.3705/acc=0.8315, loss=0.4003/acc=0.8122)
  [extratrees] extratrees trees=200 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3957 (folds: loss=0.3853/acc=0.8006, loss=0.4172/acc=0.7996, loss=0.4396/acc=0.7801, loss=0.3980/acc=0.8076, loss=0.3772/acc=0.8085, loss=0.3949/acc
=0.8158, loss=0.3653/acc=0.8315, loss=0.3885/acc=0.8186)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.3997 (folds: loss=0.3901/acc=0.7960, loss=0.4206/acc=0.8006, loss=0.4451/acc=0.7810, loss=0.4011/acc=0.8149, loss=0.3784/acc=0.8204, loss=0.4006/acc
=0.8122, loss=0.3694/acc=0.8333, loss=0.3926/acc=0.8186)
  [mlp] layers=[64, 16, 16, 32, 256, 128] act=leaky_relu lr=0.0050 batch=256 epochs=30 -> avg_log_loss=0.5725 (folds: loss=0.5703/acc=0.7831, loss=0.5977/acc=0.7739, loss=0.7012/acc=0.7636, loss=0.5676/acc=0.7919, loss=0.5472/acc=0.7956, loss
=0.5184/acc=0.7947, loss=0.5144/acc=0.8131, loss=0.5633/acc=0.7891)
  [mlp] layers=[512, 128, 16] act=gelu lr=0.0020 batch=64 epochs=15 -> avg_log_loss=0.4639 (folds: loss=0.4555/acc=0.7858, loss=0.4796/acc=0.7858, loss=0.5328/acc=0.7783, loss=0.4732/acc=0.7928, loss=0.4713/acc=0.7901, loss=0.4271/acc=0.8103,
 loss=0.4174/acc=0.8048, loss=0.4546/acc=0.7983)
  best avg log loss this gen: 0.3895
Generation 2/20
  [mlp] layers=[16] act=relu lr=0.0030 batch=128 epochs=30 -> avg_log_loss=0.3911 (folds: loss=0.3726/acc=0.8079, loss=0.4138/acc=0.8024, loss=0.4488/acc=0.7691, loss=0.3925/acc=0.8131, loss=0.3763/acc=0.8204, loss=0.3825/acc=0.8168, loss=0.3
497/acc=0.8333, loss=0.3928/acc=0.8094)
  [mlp] layers=[32] act=relu lr=0.0046 batch=256 epochs=30 -> avg_log_loss=0.3929 (folds: loss=0.3854/acc=0.8015, loss=0.4158/acc=0.8024, loss=0.4501/acc=0.7636, loss=0.3867/acc=0.8103, loss=0.3791/acc=0.8195, loss=0.3821/acc=0.8149, loss=0.3
483/acc=0.8306, loss=0.3954/acc=0.8020)
  [mlp] layers=[512] act=relu lr=0.0031 batch=512 epochs=15 -> avg_log_loss=0.4024 (folds: loss=0.4011/acc=0.8015, loss=0.4124/acc=0.7987, loss=0.4596/acc=0.7663, loss=0.3978/acc=0.8039, loss=0.3901/acc=0.8223, loss=0.3906/acc=0.8168, loss=0.
3541/acc=0.8324, loss=0.4132/acc=0.7956)
  [histgb] histgb trees=400 depth=5 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4362 (folds: loss=0.4279/acc=0.7969, loss=0.4457/acc=0.7803, loss=0.4743/acc=0.7626, loss=0.4392/acc=0.7993, loss=0.4241/acc=0.8112, loss=0.4325/acc=0.8011,
 loss=0.4105/acc=0.8131, loss=0.4355/acc=0.7937)
  [gbstump] gbstump estimators=150 lr=0.2 min_leaf=100 -> avg_log_loss=0.4422 (folds: loss=0.4331/acc=0.7868, loss=0.4502/acc=0.7840, loss=0.4789/acc=0.7737, loss=0.4454/acc=0.7956, loss=0.4241/acc=0.8149, loss=0.4435/acc=0.7901, loss=0.4179/
acc=0.8223, loss=0.4444/acc=0.7993)
  [mlp] layers=[64, 512, 32, 256, 64] act=relu lr=0.0036 batch=128 epochs=10 -> avg_log_loss=0.3986 (folds: loss=0.3824/acc=0.8079, loss=0.4325/acc=0.7987, loss=0.4428/acc=0.7783, loss=0.3922/acc=0.8131, loss=0.3848/acc=0.8103, loss=0.3774/ac
c=0.8204, loss=0.3744/acc=0.8287, loss=0.4027/acc=0.8029)
  [rf] rf trees=200 depth=8 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4383 (folds: loss=0.4386/acc=0.7914, loss=0.4506/acc=0.7831, loss=0.4977/acc=0.7654, loss=0.4347/acc=0.8029, loss=0.4165/acc=0.8112, loss=0.4239/acc=0.7974, loss=0.
4095/acc=0.8168, loss=0.4353/acc=0.8039)
  [mlp] layers=[512] act=relu lr=0.0042 batch=512 epochs=10 -> avg_log_loss=0.4024 (folds: loss=0.3919/acc=0.8079, loss=0.4120/acc=0.8024, loss=0.4632/acc=0.7755, loss=0.4019/acc=0.7993, loss=0.3931/acc=0.8186, loss=0.3926/acc=0.8094, loss=0.
3673/acc=0.8297, loss=0.3975/acc=0.8131)
  [mlp] layers=[512] act=relu lr=0.0047 batch=64 epochs=25 -> avg_log_loss=0.4154 (folds: loss=0.4069/acc=0.7932, loss=0.4258/acc=0.7978, loss=0.4821/acc=0.7691, loss=0.4141/acc=0.7974, loss=0.4049/acc=0.8094, loss=0.3978/acc=0.8131, loss=0.3
748/acc=0.8112, loss=0.4163/acc=0.8076)
  [mlp] layers=[512] act=relu lr=0.0050 batch=64 epochs=30 -> avg_log_loss=0.4209 (folds: loss=0.4147/acc=0.8006, loss=0.4386/acc=0.7978, loss=0.4786/acc=0.7663, loss=0.4211/acc=0.7965, loss=0.4139/acc=0.8103, loss=0.4035/acc=0.8103, loss=0.3
757/acc=0.8112, loss=0.4213/acc=0.8057)
  [histgb] histgb trees=300 depth=4 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4428 (folds: loss=0.4291/acc=0.8006, loss=0.4467/acc=0.7693, loss=0.4837/acc=0.7654, loss=0.4507/acc=0.7983, loss=0.4250/acc=0.8094, loss=0.4339/acc=0.7983,
 loss=0.4201/acc=0.7965, loss=0.4534/acc=0.7762)
  [mlp] layers=[32] act=relu lr=0.0030 batch=128 epochs=10 -> avg_log_loss=0.3946 (folds: loss=0.3889/acc=0.7923, loss=0.4100/acc=0.7987, loss=0.4405/acc=0.7691, loss=0.3924/acc=0.8186, loss=0.3822/acc=0.8131, loss=0.3885/acc=0.8029, loss=0.3
588/acc=0.8214, loss=0.3953/acc=0.8002)
  [mlp] layers=[512] act=relu lr=0.0041 batch=512 epochs=25 -> avg_log_loss=0.4034 (folds: loss=0.4030/acc=0.7996, loss=0.4192/acc=0.7987, loss=0.4577/acc=0.7709, loss=0.3999/acc=0.8122, loss=0.3887/acc=0.8149, loss=0.3918/acc=0.8131, loss=0.
3632/acc=0.8214, loss=0.4038/acc=0.8076)
  [mlp] layers=[32, 64, 32, 16] act=relu lr=0.0050 batch=256 epochs=30 -> avg_log_loss=0.3942 (folds: loss=0.3878/acc=0.7978, loss=0.4150/acc=0.8015, loss=0.4449/acc=0.7746, loss=0.3973/acc=0.8057, loss=0.3783/acc=0.8131, loss=0.3835/acc=0.81
68, loss=0.3523/acc=0.8297, loss=0.3942/acc=0.8039)
  [gbstump] gbstump estimators=200 lr=0.3 min_leaf=150 -> avg_log_loss=0.4456 (folds: loss=0.4355/acc=0.7914, loss=0.4529/acc=0.7776, loss=0.4802/acc=0.7700, loss=0.4496/acc=0.7891, loss=0.4267/acc=0.8122, loss=0.4452/acc=0.7947, loss=0.4248/
acc=0.8131, loss=0.4503/acc=0.8002)
  [mlp] layers=[16] act=relu lr=0.0029 batch=512 epochs=20 -> avg_log_loss=0.3931 (folds: loss=0.3800/acc=0.7950, loss=0.4146/acc=0.7987, loss=0.4418/acc=0.7718, loss=0.3891/acc=0.8140, loss=0.3826/acc=0.8122, loss=0.3849/acc=0.8140, loss=0.3
598/acc=0.8269, loss=0.3924/acc=0.8048)
  [mlp] layers=[128] act=relu lr=0.0035 batch=512 epochs=20 -> avg_log_loss=0.3961 (folds: loss=0.3807/acc=0.8116, loss=0.4105/acc=0.7996, loss=0.4503/acc=0.7663, loss=0.3970/acc=0.8057, loss=0.3851/acc=0.8131, loss=0.3886/acc=0.8094, loss=0.
3559/acc=0.8297, loss=0.4010/acc=0.7983)
  [mlp] layers=[32] act=relu lr=0.0048 batch=64 epochs=25 -> avg_log_loss=0.3898 (folds: loss=0.3823/acc=0.8051, loss=0.4128/acc=0.7978, loss=0.4420/acc=0.7737, loss=0.3869/acc=0.8149, loss=0.3815/acc=0.8195, loss=0.3797/acc=0.8177, loss=0.34
36/acc=0.8416, loss=0.3893/acc=0.8131)
  best avg log loss this gen: 0.3898
Generation 3/20
  [mlp] layers=[32] act=relu lr=0.0048 batch=64 epochs=25 -> avg_log_loss=0.3917 (folds: loss=0.3846/acc=0.7960, loss=0.4122/acc=0.8051, loss=0.4487/acc=0.7746, loss=0.3913/acc=0.8039, loss=0.3765/acc=0.8241, loss=0.3765/acc=0.8186, loss=0.35
59/acc=0.8260, loss=0.3882/acc=0.8048)
  [mlp] layers=[16] act=relu lr=0.0030 batch=128 epochs=30 -> avg_log_loss=0.3908 (folds: loss=0.3826/acc=0.8042, loss=0.4071/acc=0.8088, loss=0.4428/acc=0.7709, loss=0.3885/acc=0.8085, loss=0.3786/acc=0.8195, loss=0.3829/acc=0.8057, loss=0.3
509/acc=0.8343, loss=0.3929/acc=0.8039)
  [mlp] layers=[128] act=relu lr=0.0026 batch=128 epochs=20 -> avg_log_loss=0.3968 (folds: loss=0.3894/acc=0.8070, loss=0.4080/acc=0.8024, loss=0.4535/acc=0.7682, loss=0.3997/acc=0.8103, loss=0.3870/acc=0.8186, loss=0.3862/acc=0.8158, loss=0.
3554/acc=0.8204, loss=0.3951/acc=0.8048)
  [mlp] layers=[64] act=relu lr=0.0050 batch=256 epochs=30 -> avg_log_loss=0.3928 (folds: loss=0.3792/acc=0.8153, loss=0.4144/acc=0.7941, loss=0.4496/acc=0.7672, loss=0.3928/acc=0.8066, loss=0.3794/acc=0.8232, loss=0.3841/acc=0.8195, loss=0.3
481/acc=0.8250, loss=0.3947/acc=0.8094)
  [mlp] layers=[64] act=relu lr=0.0033 batch=128 epochs=15 -> avg_log_loss=0.3941 (folds: loss=0.3834/acc=0.8061, loss=0.4160/acc=0.8051, loss=0.4548/acc=0.7691, loss=0.3928/acc=0.8057, loss=0.3833/acc=0.8168, loss=0.3812/acc=0.8177, loss=0.3
473/acc=0.8241, loss=0.3942/acc=0.8076)
  [mlp] layers=[64] act=relu lr=0.0031 batch=512 epochs=25 -> avg_log_loss=0.3954 (folds: loss=0.3818/acc=0.8125, loss=0.4098/acc=0.8033, loss=0.4533/acc=0.7728, loss=0.3966/acc=0.8158, loss=0.3806/acc=0.8131, loss=0.3862/acc=0.8195, loss=0.3
572/acc=0.8278, loss=0.3978/acc=0.8039)
  [gbstump] gbstump estimators=150 lr=0.3 min_leaf=200 -> avg_log_loss=0.4447 (folds: loss=0.4357/acc=0.7849, loss=0.4523/acc=0.7739, loss=0.4805/acc=0.7709, loss=0.4491/acc=0.7873, loss=0.4236/acc=0.8112, loss=0.4432/acc=0.7910, loss=0.4247/
acc=0.8177, loss=0.4485/acc=0.8057)
  [mlp] layers=[64] act=relu lr=0.0031 batch=256 epochs=20 -> avg_log_loss=0.3943 (folds: loss=0.3849/acc=0.8024, loss=0.4079/acc=0.8079, loss=0.4488/acc=0.7728, loss=0.3904/acc=0.8112, loss=0.3787/acc=0.8214, loss=0.3933/acc=0.8085, loss=0.3
554/acc=0.8260, loss=0.3952/acc=0.7993)
  [mlp] layers=[32] act=relu lr=0.0028 batch=256 epochs=25 -> avg_log_loss=0.3936 (folds: loss=0.3828/acc=0.7904, loss=0.4118/acc=0.7978, loss=0.4508/acc=0.7746, loss=0.3901/acc=0.8112, loss=0.3786/acc=0.8315, loss=0.3864/acc=0.8140, loss=0.3
568/acc=0.8158, loss=0.3912/acc=0.8076)
  [mlp] layers=[128] act=relu lr=0.0033 batch=64 epochs=10 -> avg_log_loss=0.3942 (folds: loss=0.3804/acc=0.8006, loss=0.4098/acc=0.7904, loss=0.4502/acc=0.7718, loss=0.3941/acc=0.8131, loss=0.3818/acc=0.8177, loss=0.3908/acc=0.8085, loss=0.3
519/acc=0.8324, loss=0.3946/acc=0.7919)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=50 -> avg_log_loss=0.4435 (folds: loss=0.4344/acc=0.7840, loss=0.4483/acc=0.7868, loss=0.4841/acc=0.7746, loss=0.4481/acc=0.7901, loss=0.4245/acc=0.8076, loss=0.4420/acc=0.7956, loss=0.4166/a
cc=0.8204, loss=0.4501/acc=0.7928)
  [mlp] layers=[256] act=relu lr=0.0050 batch=512 epochs=10 -> avg_log_loss=0.4005 (folds: loss=0.3842/acc=0.8134, loss=0.4101/acc=0.7969, loss=0.4519/acc=0.7654, loss=0.3924/acc=0.8048, loss=0.3884/acc=0.8112, loss=0.3965/acc=0.8103, loss=0.
3794/acc=0.8122, loss=0.4008/acc=0.8002)
  [extratrees] extratrees trees=400 depth=8 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.3947 (folds: loss=0.3845/acc=0.7978, loss=0.4167/acc=0.8024, loss=0.4394/acc=0.7810, loss=0.3981/acc=0.8048, loss=0.3758/acc=0.8168, loss=0.3917/acc
=0.8140, loss=0.3654/acc=0.8297, loss=0.3861/acc=0.8241)
  [mlp] layers=[16] act=relu lr=0.0024 batch=512 epochs=20 -> avg_log_loss=0.3937 (folds: loss=0.3831/acc=0.7941, loss=0.4115/acc=0.8042, loss=0.4430/acc=0.7764, loss=0.3957/acc=0.8066, loss=0.3784/acc=0.8186, loss=0.3887/acc=0.8158, loss=0.3
574/acc=0.8370, loss=0.3917/acc=0.8094)
  [mlp] layers=[16] act=relu lr=0.0046 batch=256 epochs=25 -> avg_log_loss=0.3937 (folds: loss=0.3803/acc=0.8061, loss=0.4130/acc=0.8042, loss=0.4435/acc=0.7709, loss=0.3934/acc=0.8131, loss=0.3798/acc=0.8158, loss=0.3859/acc=0.8214, loss=0.3
577/acc=0.8232, loss=0.3956/acc=0.8020)
  [mlp] layers=[16] act=relu lr=0.0031 batch=64 epochs=25 -> avg_log_loss=0.3903 (folds: loss=0.3790/acc=0.7987, loss=0.4152/acc=0.7923, loss=0.4407/acc=0.7737, loss=0.3833/acc=0.8158, loss=0.3746/acc=0.8223, loss=0.3775/acc=0.8232, loss=0.35
53/acc=0.8269, loss=0.3969/acc=0.7983)
  [mlp] layers=[32] act=relu lr=0.0027 batch=64 epochs=25 -> avg_log_loss=0.3939 (folds: loss=0.3799/acc=0.8070, loss=0.4223/acc=0.8061, loss=0.4491/acc=0.7718, loss=0.3970/acc=0.8112, loss=0.3763/acc=0.8177, loss=0.3778/acc=0.8158, loss=0.35
71/acc=0.8204, loss=0.3915/acc=0.8048)
  [mlp] layers=[32] act=relu lr=0.0028 batch=512 epochs=15 -> avg_log_loss=0.3957 (folds: loss=0.3854/acc=0.7932, loss=0.4165/acc=0.8042, loss=0.4485/acc=0.7617, loss=0.3894/acc=0.8122, loss=0.3830/acc=0.8223, loss=0.3905/acc=0.8158, loss=0.3
585/acc=0.8287, loss=0.3936/acc=0.8112)
  best avg log loss this gen: 0.3903
Generation 4/20
  [mlp] layers=[16] act=relu lr=0.0031 batch=64 epochs=25 -> avg_log_loss=0.3896 (folds: loss=0.3785/acc=0.8097, loss=0.4124/acc=0.8033, loss=0.4454/acc=0.7672, loss=0.3877/acc=0.8131, loss=0.3725/acc=0.8186, loss=0.3834/acc=0.8122, loss=0.34
92/acc=0.8287, loss=0.3881/acc=0.8094)
  [mlp] layers=[16] act=relu lr=0.0030 batch=128 epochs=30 -> avg_log_loss=0.3907 (folds: loss=0.3759/acc=0.7960, loss=0.4085/acc=0.8006, loss=0.4418/acc=0.7682, loss=0.3875/acc=0.8103, loss=0.3756/acc=0.8149, loss=0.3789/acc=0.8149, loss=0.3
588/acc=0.8260, loss=0.3982/acc=0.8103)
  [mlp] layers=[256] act=relu lr=0.0047 batch=256 epochs=20 -> avg_log_loss=0.4033 (folds: loss=0.3935/acc=0.8051, loss=0.4177/acc=0.7932, loss=0.4663/acc=0.7636, loss=0.4037/acc=0.8039, loss=0.3875/acc=0.8177, loss=0.3864/acc=0.8140, loss=0.
3639/acc=0.8315, loss=0.4074/acc=0.8085)
  [mlp] layers=[64, 128] act=leaky_relu lr=0.0030 batch=64 epochs=25 -> avg_log_loss=0.4016 (folds: loss=0.3900/acc=0.8024, loss=0.4174/acc=0.7941, loss=0.4685/acc=0.7783, loss=0.4066/acc=0.7993, loss=0.3923/acc=0.8140, loss=0.3805/acc=0.8149
, loss=0.3604/acc=0.8306, loss=0.3971/acc=0.7983)
  [mlp] layers=[128] act=relu lr=0.0026 batch=256 epochs=20 -> avg_log_loss=0.3970 (folds: loss=0.3842/acc=0.8116, loss=0.4157/acc=0.8079, loss=0.4550/acc=0.7608, loss=0.3941/acc=0.8011, loss=0.3848/acc=0.8232, loss=0.3886/acc=0.8103, loss=0.
3553/acc=0.8214, loss=0.3984/acc=0.8002)
  [mlp] layers=[16, 256, 16, 16, 32, 32] act=leaky_relu lr=0.0048 batch=64 epochs=10 -> avg_log_loss=0.3927 (folds: loss=0.3794/acc=0.8070, loss=0.4110/acc=0.7969, loss=0.4405/acc=0.7847, loss=0.3870/acc=0.7965, loss=0.3773/acc=0.8232, loss=0
.3866/acc=0.8131, loss=0.3604/acc=0.8306, loss=0.3996/acc=0.8048)
  [mlp] layers=[256] act=leaky_relu lr=0.0050 batch=128 epochs=30 -> avg_log_loss=0.4020 (folds: loss=0.3894/acc=0.8070, loss=0.4167/acc=0.8006, loss=0.4604/acc=0.7718, loss=0.4052/acc=0.8066, loss=0.3919/acc=0.8195, loss=0.3895/acc=0.8131, l
oss=0.3575/acc=0.8223, loss=0.4052/acc=0.8057)
  [mlp] layers=[128] act=relu lr=0.0031 batch=128 epochs=30 -> avg_log_loss=0.3992 (folds: loss=0.3869/acc=0.8079, loss=0.4132/acc=0.7969, loss=0.4558/acc=0.7709, loss=0.4030/acc=0.8039, loss=0.3892/acc=0.8168, loss=0.3887/acc=0.8140, loss=0.
3561/acc=0.8195, loss=0.4012/acc=0.8039)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.3962 (folds: loss=0.3868/acc=0.7932, loss=0.4184/acc=0.7969, loss=0.4389/acc=0.7820, loss=0.3985/acc=0.8103, loss=0.3787/acc=0.8140, loss=0.3959/acc
=0.8149, loss=0.3653/acc=0.8333, loss=0.3871/acc=0.8204)
  [mlp] layers=[64] act=relu lr=0.0045 batch=128 epochs=15 -> avg_log_loss=0.3929 (folds: loss=0.3829/acc=0.8042, loss=0.4135/acc=0.8015, loss=0.4491/acc=0.7746, loss=0.3916/acc=0.8177, loss=0.3814/acc=0.8177, loss=0.3768/acc=0.8168, loss=0.3
500/acc=0.8315, loss=0.3975/acc=0.7983)
  [mlp] layers=[128] act=relu lr=0.0034 batch=256 epochs=20 -> avg_log_loss=0.4011 (folds: loss=0.3932/acc=0.8070, loss=0.4143/acc=0.8006, loss=0.4577/acc=0.7645, loss=0.4010/acc=0.8029, loss=0.3930/acc=0.8140, loss=0.3892/acc=0.8158, loss=0.
3576/acc=0.8140, loss=0.4025/acc=0.8039)
  [mlp] layers=[128, 512, 32, 256] act=leaky_relu lr=0.0045 batch=64 epochs=30 -> avg_log_loss=0.4245 (folds: loss=0.3987/acc=0.8079, loss=0.4430/acc=0.7960, loss=0.5128/acc=0.7682, loss=0.4180/acc=0.8066, loss=0.4050/acc=0.8140, loss=0.4079/
acc=0.8057, loss=0.3933/acc=0.8370, loss=0.4170/acc=0.7983)
  [mlp] layers=[64] act=relu lr=0.0042 batch=128 epochs=25 -> avg_log_loss=0.3931 (folds: loss=0.3830/acc=0.8061, loss=0.4070/acc=0.7932, loss=0.4503/acc=0.7691, loss=0.3963/acc=0.8029, loss=0.3810/acc=0.8250, loss=0.3833/acc=0.8140, loss=0.3
558/acc=0.8269, loss=0.3878/acc=0.8085)
  [histgb] histgb trees=200 depth=6 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4383 (folds: loss=0.4267/acc=0.7969, loss=0.4455/acc=0.7665, loss=0.4939/acc=0.7571, loss=0.4385/acc=0.7845, loss=0.4243/acc=0.7983, loss=0.4303/acc=0.7993,
 loss=0.4081/acc=0.8112, loss=0.4394/acc=0.7974)
  [gbstump] gbstump estimators=300 lr=0.1 min_leaf=50 -> avg_log_loss=0.4440 (folds: loss=0.4320/acc=0.7822, loss=0.4478/acc=0.7785, loss=0.4831/acc=0.7755, loss=0.4510/acc=0.7891, loss=0.4238/acc=0.8103, loss=0.4417/acc=0.7947, loss=0.4188/a
cc=0.8214, loss=0.4541/acc=0.7864)
  [mlp] layers=[512] act=relu lr=0.0031 batch=128 epochs=10 -> avg_log_loss=0.4037 (folds: loss=0.3922/acc=0.8051, loss=0.4210/acc=0.7987, loss=0.4761/acc=0.7737, loss=0.3997/acc=0.7983, loss=0.4002/acc=0.8057, loss=0.3824/acc=0.8122, loss=0.
3553/acc=0.8278, loss=0.4026/acc=0.8094)
  [mlp] layers=[64] act=relu lr=0.0027 batch=64 epochs=30 -> avg_log_loss=0.3938 (folds: loss=0.3828/acc=0.8116, loss=0.4091/acc=0.8015, loss=0.4474/acc=0.7691, loss=0.3922/acc=0.8085, loss=0.3833/acc=0.8186, loss=0.3847/acc=0.8186, loss=0.35
53/acc=0.8214, loss=0.3957/acc=0.7983)
  [mlp] layers=[16] act=relu lr=0.0028 batch=128 epochs=25 -> avg_log_loss=0.3928 (folds: loss=0.3779/acc=0.8079, loss=0.4167/acc=0.7868, loss=0.4475/acc=0.7672, loss=0.3883/acc=0.8122, loss=0.3808/acc=0.8177, loss=0.3858/acc=0.8094, loss=0.3
561/acc=0.8379, loss=0.3892/acc=0.8103)
  best avg log loss this gen: 0.3896
Generation 5/20
  [mlp] layers=[16] act=relu lr=0.0031 batch=64 epochs=25 -> avg_log_loss=0.3923 (folds: loss=0.3816/acc=0.8033, loss=0.4104/acc=0.7996, loss=0.4354/acc=0.7810, loss=0.3936/acc=0.8057, loss=0.3780/acc=0.8204, loss=0.3876/acc=0.8140, loss=0.36
02/acc=0.8306, loss=0.3920/acc=0.8011)
  [mlp] layers=[16] act=relu lr=0.0030 batch=128 epochs=30 -> avg_log_loss=0.3898 (folds: loss=0.3787/acc=0.7996, loss=0.4117/acc=0.8006, loss=0.4417/acc=0.7691, loss=0.3878/acc=0.8195, loss=0.3732/acc=0.8260, loss=0.3795/acc=0.8195, loss=0.3
537/acc=0.8297, loss=0.3919/acc=0.8131)
  [mlp] layers=[64] act=relu lr=0.0030 batch=128 epochs=30 -> avg_log_loss=0.3944 (folds: loss=0.3856/acc=0.8143, loss=0.4124/acc=0.7960, loss=0.4477/acc=0.7718, loss=0.3970/acc=0.7993, loss=0.3837/acc=0.8232, loss=0.3864/acc=0.8177, loss=0.3
506/acc=0.8260, loss=0.3922/acc=0.8103)
  [mlp] layers=[64] act=relu lr=0.0024 batch=128 epochs=10 -> avg_log_loss=0.3960 (folds: loss=0.3866/acc=0.8097, loss=0.4072/acc=0.8033, loss=0.4497/acc=0.7801, loss=0.4013/acc=0.8048, loss=0.3804/acc=0.8158, loss=0.3909/acc=0.8223, loss=0.3
530/acc=0.8398, loss=0.3986/acc=0.7965)
  [mlp] layers=[64] act=relu lr=0.0029 batch=256 epochs=15 -> avg_log_loss=0.3928 (folds: loss=0.3851/acc=0.8061, loss=0.4084/acc=0.8015, loss=0.4474/acc=0.7709, loss=0.3894/acc=0.8112, loss=0.3793/acc=0.8241, loss=0.3853/acc=0.8149, loss=0.3
515/acc=0.8287, loss=0.3960/acc=0.8094)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4102 (folds: loss=0.4022/acc=0.7978, loss=0.4296/acc=0.7960, loss=0.4494/acc=0.7912, loss=0.4123/acc=0.7983, loss=0.3907/acc=0.8177, loss=0.4145/acc
=0.8112, loss=0.3777/acc=0.8260, loss=0.4054/acc=0.8085)
  [gbstump] gbstump estimators=300 lr=0.1 min_leaf=150 -> avg_log_loss=0.4411 (folds: loss=0.4299/acc=0.7849, loss=0.4478/acc=0.7840, loss=0.4779/acc=0.7746, loss=0.4451/acc=0.7919, loss=0.4243/acc=0.8076, loss=0.4409/acc=0.7947, loss=0.4176/
acc=0.8214, loss=0.4452/acc=0.8039)
  [extratrees] extratrees trees=100 depth=5 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4037 (folds: loss=0.3956/acc=0.7914, loss=0.4236/acc=0.7996, loss=0.4442/acc=0.7893, loss=0.4061/acc=0.8076, loss=0.3862/acc=0.8168, loss=0.4053/acc
=0.8112, loss=0.3703/acc=0.8333, loss=0.3983/acc=0.8177)
  [mlp] layers=[16] act=relu lr=0.0030 batch=512 epochs=15 -> avg_log_loss=0.3984 (folds: loss=0.3831/acc=0.8006, loss=0.4163/acc=0.7904, loss=0.4446/acc=0.7709, loss=0.4017/acc=0.8039, loss=0.3819/acc=0.8168, loss=0.3902/acc=0.8094, loss=0.3
675/acc=0.8214, loss=0.4016/acc=0.8122)
  [mlp] layers=[16, 256, 16, 16, 32, 32] act=leaky_relu lr=0.0047 batch=64 epochs=30 -> avg_log_loss=0.3891 (folds: loss=0.3790/acc=0.7987, loss=0.4126/acc=0.8042, loss=0.4384/acc=0.7746, loss=0.3889/acc=0.8057, loss=0.3727/acc=0.8122, loss=0
.3845/acc=0.8066, loss=0.3485/acc=0.8407, loss=0.3886/acc=0.8085)
  [mlp] layers=[16, 512, 16, 16, 32, 32] act=leaky_relu lr=0.0050 batch=256 epochs=15 -> avg_log_loss=0.3953 (folds: loss=0.3859/acc=0.8061, loss=0.4091/acc=0.8061, loss=0.4486/acc=0.7728, loss=0.3935/acc=0.8029, loss=0.3779/acc=0.8177, loss=
0.3912/acc=0.8076, loss=0.3560/acc=0.8260, loss=0.3998/acc=0.8048)
  [mlp] layers=[128] act=relu lr=0.0033 batch=256 epochs=10 -> avg_log_loss=0.3969 (folds: loss=0.3776/acc=0.8051, loss=0.4190/acc=0.7987, loss=0.4481/acc=0.7728, loss=0.3974/acc=0.8020, loss=0.3880/acc=0.8122, loss=0.3854/acc=0.8085, loss=0.
3579/acc=0.8306, loss=0.4021/acc=0.8085)
  [mlp] layers=[512] act=relu lr=0.0034 batch=64 epochs=20 -> avg_log_loss=0.4121 (folds: loss=0.4108/acc=0.7941, loss=0.4221/acc=0.7987, loss=0.4731/acc=0.7682, loss=0.4136/acc=0.7993, loss=0.4002/acc=0.8204, loss=0.3920/acc=0.8112, loss=0.3
694/acc=0.8140, loss=0.4158/acc=0.8002)
  [mlp] layers=[256] act=relu lr=0.0035 batch=512 epochs=25 -> avg_log_loss=0.3994 (folds: loss=0.3833/acc=0.8116, loss=0.4140/acc=0.7969, loss=0.4530/acc=0.7682, loss=0.4011/acc=0.8048, loss=0.3916/acc=0.8223, loss=0.3864/acc=0.8140, loss=0.
3605/acc=0.8177, loss=0.4050/acc=0.8029)
  [mlp] layers=[64] act=relu lr=0.0025 batch=128 epochs=30 -> avg_log_loss=0.3951 (folds: loss=0.3830/acc=0.8107, loss=0.4128/acc=0.8033, loss=0.4573/acc=0.7691, loss=0.3925/acc=0.8066, loss=0.3844/acc=0.8186, loss=0.3805/acc=0.8214, loss=0.3
509/acc=0.8287, loss=0.4000/acc=0.8039)
  [mlp] layers=[256] act=relu lr=0.0026 batch=64 epochs=15 -> avg_log_loss=0.4013 (folds: loss=0.3985/acc=0.8006, loss=0.4181/acc=0.7923, loss=0.4597/acc=0.7682, loss=0.4056/acc=0.8057, loss=0.3872/acc=0.8140, loss=0.3808/acc=0.8158, loss=0.3
647/acc=0.8269, loss=0.3959/acc=0.8149)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.3994 (folds: loss=0.3895/acc=0.7950, loss=0.4204/acc=0.8015, loss=0.4410/acc=0.7829, loss=0.4017/acc=0.8122, loss=0.3813/acc=0.8140, loss=0.4012/acc
=0.8149, loss=0.3665/acc=0.8315, loss=0.3936/acc=0.8177)
  [mlp] layers=[32] act=relu lr=0.0029 batch=512 epochs=30 -> avg_log_loss=0.3925 (folds: loss=0.3822/acc=0.8051, loss=0.4134/acc=0.8024, loss=0.4471/acc=0.7718, loss=0.3919/acc=0.8112, loss=0.3799/acc=0.8140, loss=0.3850/acc=0.8168, loss=0.3
480/acc=0.8260, loss=0.3927/acc=0.8066)
  best avg log loss this gen: 0.3891
Generation 6/20
  [mlp] layers=[16, 256, 16, 16, 32, 32] act=leaky_relu lr=0.0047 batch=64 epochs=30 -> avg_log_loss=0.3894 (folds: loss=0.3768/acc=0.8024, loss=0.4050/acc=0.8070, loss=0.4404/acc=0.7764, loss=0.3901/acc=0.8057, loss=0.3804/acc=0.8057, loss=0
.3864/acc=0.8204, loss=0.3491/acc=0.8416, loss=0.3871/acc=0.8140)
  [mlp] layers=[16] act=relu lr=0.0030 batch=128 epochs=30 -> avg_log_loss=0.3904 (folds: loss=0.3773/acc=0.8051, loss=0.4142/acc=0.7950, loss=0.4479/acc=0.7636, loss=0.3931/acc=0.8186, loss=0.3753/acc=0.8223, loss=0.3839/acc=0.8177, loss=0.3
446/acc=0.8315, loss=0.3870/acc=0.8076)
  [mlp] layers=[128] act=relu lr=0.0026 batch=64 epochs=10 -> avg_log_loss=0.3960 (folds: loss=0.3954/acc=0.7969, loss=0.4101/acc=0.8033, loss=0.4454/acc=0.7700, loss=0.4008/acc=0.8066, loss=0.3834/acc=0.8315, loss=0.3880/acc=0.8103, loss=0.3
526/acc=0.8269, loss=0.3925/acc=0.8057)
  [mlp] layers=[16, 256, 32, 16, 32, 32] act=leaky_relu lr=0.0045 batch=64 epochs=15 -> avg_log_loss=0.3907 (folds: loss=0.3852/acc=0.7886, loss=0.4073/acc=0.8042, loss=0.4424/acc=0.7801, loss=0.4002/acc=0.8048, loss=0.3752/acc=0.8250, loss=0
.3822/acc=0.8149, loss=0.3489/acc=0.8398, loss=0.3842/acc=0.8232)
  [mlp] layers=[128] act=relu lr=0.0033 batch=64 epochs=15 -> avg_log_loss=0.3970 (folds: loss=0.3888/acc=0.8006, loss=0.4126/acc=0.7978, loss=0.4507/acc=0.7626, loss=0.3959/acc=0.8002, loss=0.3866/acc=0.8158, loss=0.3882/acc=0.8085, loss=0.3
560/acc=0.8278, loss=0.3972/acc=0.8048)
  [mlp] layers=[32] act=relu lr=0.0029 batch=512 epochs=30 -> avg_log_loss=0.3933 (folds: loss=0.3779/acc=0.8070, loss=0.4107/acc=0.8024, loss=0.4479/acc=0.7700, loss=0.3947/acc=0.8131, loss=0.3801/acc=0.8168, loss=0.3850/acc=0.8076, loss=0.3
563/acc=0.8250, loss=0.3938/acc=0.8066)
  [extratrees] extratrees trees=200 depth=7 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.3972 (folds: loss=0.3867/acc=0.7932, loss=0.4185/acc=0.7987, loss=0.4421/acc=0.7820, loss=0.3986/acc=0.8140, loss=0.3774/acc=0.8158, loss=0.3974/acc
=0.8158, loss=0.3666/acc=0.8361, loss=0.3906/acc=0.8186)
  [mlp] layers=[32, 256, 16, 16, 32, 32] act=leaky_relu lr=0.0046 batch=256 epochs=10 -> avg_log_loss=0.3965 (folds: loss=0.3859/acc=0.8024, loss=0.4043/acc=0.8042, loss=0.4482/acc=0.7856, loss=0.3994/acc=0.8011, loss=0.3808/acc=0.8195, loss=
0.3913/acc=0.8131, loss=0.3699/acc=0.8278, loss=0.3925/acc=0.8076)
  [extratrees] extratrees trees=400 depth=7 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.3962 (folds: loss=0.3876/acc=0.7978, loss=0.4174/acc=0.7960, loss=0.4396/acc=0.7801, loss=0.3989/acc=0.8140, loss=0.3784/acc=0.8140, loss=0.3953/acc
=0.8149, loss=0.3652/acc=0.8315, loss=0.3867/acc=0.8214)
  [mlp] layers=[64] act=relu lr=0.0024 batch=256 epochs=30 -> avg_log_loss=0.3963 (folds: loss=0.3934/acc=0.8033, loss=0.4104/acc=0.8051, loss=0.4499/acc=0.7682, loss=0.3992/acc=0.8112, loss=0.3846/acc=0.8131, loss=0.3828/acc=0.8140, loss=0.3
517/acc=0.8241, loss=0.3983/acc=0.8029)
  [mlp] layers=[128] act=relu lr=0.0030 batch=128 epochs=10 -> avg_log_loss=0.3965 (folds: loss=0.3979/acc=0.7886, loss=0.4133/acc=0.8042, loss=0.4479/acc=0.7617, loss=0.3944/acc=0.8103, loss=0.3838/acc=0.8168, loss=0.3851/acc=0.8057, loss=0.
3553/acc=0.8122, loss=0.3941/acc=0.8048)
  [mlp] layers=[16, 256, 16, 16, 32, 64] act=leaky_relu lr=0.0050 batch=512 epochs=30 -> avg_log_loss=0.3953 (folds: loss=0.3880/acc=0.8125, loss=0.4167/acc=0.7996, loss=0.4505/acc=0.7746, loss=0.4007/acc=0.8039, loss=0.3814/acc=0.8122, loss=
0.3850/acc=0.8214, loss=0.3530/acc=0.8352, loss=0.3870/acc=0.8122)
  [mlp] layers=[256] act=relu lr=0.0026 batch=64 epochs=20 -> avg_log_loss=0.4021 (folds: loss=0.4006/acc=0.8033, loss=0.4174/acc=0.7932, loss=0.4606/acc=0.7700, loss=0.3987/acc=0.8057, loss=0.3889/acc=0.8158, loss=0.3843/acc=0.8076, loss=0.3
644/acc=0.8195, loss=0.4017/acc=0.8011)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4032 (folds: loss=0.3947/acc=0.7941, loss=0.4233/acc=0.8006, loss=0.4456/acc=0.7912, loss=0.4059/acc=0.8094, loss=0.3837/acc=0.8177, loss=0.4054/acc
=0.8103, loss=0.3711/acc=0.8287, loss=0.3960/acc=0.8168)
  [mlp] layers=[32] act=relu lr=0.0031 batch=128 epochs=20 -> avg_log_loss=0.3921 (folds: loss=0.3849/acc=0.8024, loss=0.4106/acc=0.8042, loss=0.4464/acc=0.7700, loss=0.3885/acc=0.8076, loss=0.3756/acc=0.8214, loss=0.3827/acc=0.8103, loss=0.3
578/acc=0.8112, loss=0.3899/acc=0.8039)
  [mlp] layers=[16] act=relu lr=0.0028 batch=64 epochs=15 -> avg_log_loss=0.3932 (folds: loss=0.3839/acc=0.8015, loss=0.4161/acc=0.8006, loss=0.4412/acc=0.7691, loss=0.3908/acc=0.8039, loss=0.3775/acc=0.8177, loss=0.3850/acc=0.8204, loss=0.34
84/acc=0.8481, loss=0.4030/acc=0.8085)
  [gbstump] gbstump estimators=150 lr=0.1 min_leaf=50 -> avg_log_loss=0.4403 (folds: loss=0.4301/acc=0.7840, loss=0.4442/acc=0.7904, loss=0.4805/acc=0.7728, loss=0.4454/acc=0.7873, loss=0.4213/acc=0.8057, loss=0.4385/acc=0.8020, loss=0.4147/a
cc=0.8241, loss=0.4479/acc=0.7937)
  [mlp] layers=[64] act=relu lr=0.0035 batch=256 epochs=20 -> avg_log_loss=0.3940 (folds: loss=0.3847/acc=0.8079, loss=0.4104/acc=0.7960, loss=0.4474/acc=0.7590, loss=0.3917/acc=0.8103, loss=0.3779/acc=0.8158, loss=0.3846/acc=0.8112, loss=0.3
604/acc=0.8131, loss=0.3947/acc=0.8076)
  best avg log loss this gen: 0.3894
Generation 7/20
  [mlp] layers=[16, 256, 16, 16, 32, 32] act=leaky_relu lr=0.0047 batch=64 epochs=30 -> avg_log_loss=0.3883 (folds: loss=0.3763/acc=0.7996, loss=0.4093/acc=0.8015, loss=0.4314/acc=0.7912, loss=0.3862/acc=0.8039, loss=0.3778/acc=0.8131, loss=0
.3794/acc=0.8177, loss=0.3497/acc=0.8379, loss=0.3962/acc=0.8094)
  [mlp] layers=[16] act=relu lr=0.0030 batch=128 epochs=30 -> avg_log_loss=0.3914 (folds: loss=0.3808/acc=0.8079, loss=0.4124/acc=0.8015, loss=0.4410/acc=0.7682, loss=0.3914/acc=0.8085, loss=0.3799/acc=0.8168, loss=0.3760/acc=0.8260, loss=0.3
562/acc=0.8297, loss=0.3935/acc=0.8112)
  [mlp] layers=[128] act=relu lr=0.0031 batch=64 epochs=20 -> avg_log_loss=0.3952 (folds: loss=0.3813/acc=0.8116, loss=0.4107/acc=0.8015, loss=0.4508/acc=0.7700, loss=0.3984/acc=0.8094, loss=0.3789/acc=0.8241, loss=0.3912/acc=0.8085, loss=0.3
559/acc=0.8223, loss=0.3943/acc=0.7983)
  [mlp] layers=[16, 256, 16, 16, 32, 128] act=leaky_relu lr=0.0044 batch=64 epochs=30 -> avg_log_loss=0.3883 (folds: loss=0.3739/acc=0.8024, loss=0.4054/acc=0.8097, loss=0.4343/acc=0.7746, loss=0.3909/acc=0.7993, loss=0.3778/acc=0.8131, loss=
0.3857/acc=0.8177, loss=0.3501/acc=0.8333, loss=0.3879/acc=0.8029)
  [gbstump] gbstump estimators=150 lr=0.3 min_leaf=100 -> avg_log_loss=0.4439 (folds: loss=0.4362/acc=0.7895, loss=0.4497/acc=0.7822, loss=0.4799/acc=0.7691, loss=0.4496/acc=0.7836, loss=0.4226/acc=0.8112, loss=0.4450/acc=0.7919, loss=0.4215/
acc=0.8131, loss=0.4467/acc=0.8029)
  [rf] rf trees=300 depth=6 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4244 (folds: loss=0.4175/acc=0.7950, loss=0.4411/acc=0.7730, loss=0.4759/acc=0.7718, loss=0.4239/acc=0.8002, loss=0.4045/acc=0.8094, loss=0.4137/acc=0.7993, loss=0.
3973/acc=0.8177, loss=0.4215/acc=0.8066)
  [mlp] layers=[128] act=relu lr=0.0029 batch=256 epochs=10 -> avg_log_loss=0.3959 (folds: loss=0.3891/acc=0.7987, loss=0.4152/acc=0.7914, loss=0.4476/acc=0.7663, loss=0.3936/acc=0.8085, loss=0.3855/acc=0.8214, loss=0.3834/acc=0.8177, loss=0.
3555/acc=0.8278, loss=0.3972/acc=0.7956)
  [histgb] histgb trees=400 depth=7 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4322 (folds: loss=0.4223/acc=0.7950, loss=0.4422/acc=0.7721, loss=0.4705/acc=0.7691, loss=0.4356/acc=0.7965, loss=0.4196/acc=0.8122, loss=0.4239/acc=0.7983,
 loss=0.4051/acc=0.8122, loss=0.4387/acc=0.7993)
  [mlp] layers=[16] act=relu lr=0.0032 batch=512 epochs=15 -> avg_log_loss=0.3945 (folds: loss=0.3868/acc=0.7932, loss=0.4126/acc=0.8042, loss=0.4462/acc=0.7829, loss=0.3924/acc=0.8048, loss=0.3770/acc=0.8186, loss=0.3857/acc=0.8149, loss=0.3
596/acc=0.8260, loss=0.3955/acc=0.8131)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4051 (folds: loss=0.3979/acc=0.8006, loss=0.4241/acc=0.7969, loss=0.4453/acc=0.7921, loss=0.4061/acc=0.8057, loss=0.3875/acc=0.8177, loss=0.4092/acc
=0.8131, loss=0.3703/acc=0.8333, loss=0.4007/acc=0.8094)
  [mlp] layers=[16, 256, 32, 16, 32, 32] act=leaky_relu lr=0.0043 batch=64 epochs=10 -> avg_log_loss=0.3930 (folds: loss=0.3778/acc=0.7996, loss=0.4092/acc=0.7960, loss=0.4410/acc=0.7746, loss=0.3944/acc=0.7974, loss=0.3868/acc=0.8195, loss=0
.3913/acc=0.8158, loss=0.3580/acc=0.8315, loss=0.3851/acc=0.8140)
  [mlp] layers=[512] act=relu lr=0.0033 batch=64 epochs=15 -> avg_log_loss=0.4056 (folds: loss=0.3957/acc=0.7987, loss=0.4158/acc=0.8015, loss=0.4582/acc=0.7746, loss=0.4084/acc=0.7974, loss=0.4026/acc=0.8039, loss=0.3934/acc=0.8122, loss=0.3
623/acc=0.8287, loss=0.4084/acc=0.8011)
  [rf] rf trees=300 depth=4 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4257 (folds: loss=0.4178/acc=0.7978, loss=0.4433/acc=0.7693, loss=0.4785/acc=0.7626, loss=0.4260/acc=0.7956, loss=0.4016/acc=0.8085, loss=0.4174/acc=0.8002, loss=0.
3974/acc=0.8140, loss=0.4239/acc=0.8085)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=200 -> avg_log_loss=0.4409 (folds: loss=0.4318/acc=0.7803, loss=0.4480/acc=0.7868, loss=0.4788/acc=0.7728, loss=0.4430/acc=0.7910, loss=0.4251/acc=0.8076, loss=0.4404/acc=0.7965, loss=0.4148/
acc=0.8232, loss=0.4455/acc=0.8057)
  [mlp] layers=[64] act=relu lr=0.0030 batch=256 epochs=15 -> avg_log_loss=0.3951 (folds: loss=0.3869/acc=0.8079, loss=0.4081/acc=0.7996, loss=0.4517/acc=0.7682, loss=0.3899/acc=0.8158, loss=0.3838/acc=0.8131, loss=0.3864/acc=0.8122, loss=0.3
570/acc=0.8287, loss=0.3973/acc=0.7956)
  [extratrees] extratrees trees=300 depth=6 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.3992 (folds: loss=0.3898/acc=0.7941, loss=0.4212/acc=0.7996, loss=0.4428/acc=0.7810, loss=0.4017/acc=0.8094, loss=0.3802/acc=0.8168, loss=0.3991/acc
=0.8140, loss=0.3685/acc=0.8287, loss=0.3902/acc=0.8186)
  [mlp] layers=[16, 256, 16, 16, 64, 32] act=leaky_relu lr=0.0050 batch=128 epochs=20 -> avg_log_loss=0.3894 (folds: loss=0.3801/acc=0.8033, loss=0.4017/acc=0.8079, loss=0.4426/acc=0.7718, loss=0.3859/acc=0.8029, loss=0.3787/acc=0.8195, loss=
0.3862/acc=0.8177, loss=0.3516/acc=0.8435, loss=0.3883/acc=0.8122)
  [mlp] layers=[16, 256, 16, 256, 32, 32] act=leaky_relu lr=0.0050 batch=64 epochs=20 -> avg_log_loss=0.3914 (folds: loss=0.3868/acc=0.8024, loss=0.4094/acc=0.7996, loss=0.4315/acc=0.7893, loss=0.3910/acc=0.7947, loss=0.3711/acc=0.8131, loss=
0.3942/acc=0.8103, loss=0.3528/acc=0.8370, loss=0.3939/acc=0.8011)
  best avg log loss this gen: 0.3883
Generation 8/20
  [mlp] layers=[16, 256, 16, 16, 32, 128] act=leaky_relu lr=0.0044 batch=64 epochs=30 -> avg_log_loss=0.3890 (folds: loss=0.3756/acc=0.7978, loss=0.4070/acc=0.8033, loss=0.4408/acc=0.7718, loss=0.3920/acc=0.8103, loss=0.3769/acc=0.8204, loss=
0.3860/acc=0.8094, loss=0.3508/acc=0.8333, loss=0.3833/acc=0.8140)
  [mlp] layers=[16, 256, 16, 16, 32, 32] act=leaky_relu lr=0.0047 batch=64 epochs=30 -> avg_log_loss=0.3895 (folds: loss=0.3747/acc=0.8088, loss=0.4044/acc=0.8088, loss=0.4382/acc=0.7691, loss=0.3882/acc=0.8066, loss=0.3823/acc=0.8140, loss=0
.3882/acc=0.8149, loss=0.3478/acc=0.8306, loss=0.3922/acc=0.8057)
  [mlp] layers=[128, 256, 16, 256, 32, 32] act=leaky_relu lr=0.0045 batch=128 epochs=15 -> avg_log_loss=0.3914 (folds: loss=0.3773/acc=0.8006, loss=0.4139/acc=0.8042, loss=0.4423/acc=0.7810, loss=0.3929/acc=0.8094, loss=0.3799/acc=0.8122, los
s=0.3751/acc=0.8177, loss=0.3566/acc=0.8370, loss=0.3932/acc=0.8094)
  [mlp] layers=[16, 64, 16, 16, 64, 32] act=leaky_relu lr=0.0041 batch=256 epochs=20 -> avg_log_loss=0.3926 (folds: loss=0.3795/acc=0.7978, loss=0.4042/acc=0.8051, loss=0.4459/acc=0.7700, loss=0.4005/acc=0.8057, loss=0.3840/acc=0.8214, loss=0
.3821/acc=0.8168, loss=0.3567/acc=0.8195, loss=0.3883/acc=0.8158)
  [mlp] layers=[16, 256, 16, 16, 256, 32] act=leaky_relu lr=0.0047 batch=128 epochs=15 -> avg_log_loss=0.3901 (folds: loss=0.3751/acc=0.8116, loss=0.4168/acc=0.8006, loss=0.4405/acc=0.7737, loss=0.3905/acc=0.8076, loss=0.3850/acc=0.8186, loss
=0.3841/acc=0.8195, loss=0.3421/acc=0.8398, loss=0.3872/acc=0.8048)
  [gbstump] gbstump estimators=150 lr=0.1 min_leaf=50 -> avg_log_loss=0.4403 (folds: loss=0.4300/acc=0.7840, loss=0.4442/acc=0.7895, loss=0.4805/acc=0.7728, loss=0.4454/acc=0.7873, loss=0.4213/acc=0.8057, loss=0.4385/acc=0.8020, loss=0.4147/a
cc=0.8241, loss=0.4479/acc=0.7937)
  [mlp] layers=[16, 256, 16, 256, 32, 32] act=leaky_relu lr=0.0050 batch=512 epochs=15 -> avg_log_loss=0.3978 (folds: loss=0.3861/acc=0.7941, loss=0.4215/acc=0.7978, loss=0.4332/acc=0.7718, loss=0.3943/acc=0.8057, loss=0.3819/acc=0.8158, loss
=0.3979/acc=0.8149, loss=0.3676/acc=0.8306, loss=0.3997/acc=0.8149)
  [mlp] layers=[16, 256, 16, 256, 256, 32] act=leaky_relu lr=0.0049 batch=64 epochs=25 -> avg_log_loss=0.3882 (folds: loss=0.3735/acc=0.8079, loss=0.4073/acc=0.7996, loss=0.4317/acc=0.7847, loss=0.3889/acc=0.8149, loss=0.3776/acc=0.8112, loss
=0.3863/acc=0.8195, loss=0.3521/acc=0.8361, loss=0.3885/acc=0.8002)
  [mlp] layers=[16, 256, 256, 16, 32, 32] act=leaky_relu lr=0.0050 batch=256 epochs=25 -> avg_log_loss=0.3906 (folds: loss=0.3786/acc=0.7950, loss=0.4014/acc=0.8051, loss=0.4425/acc=0.7737, loss=0.3943/acc=0.8029, loss=0.3793/acc=0.8158, loss
=0.3819/acc=0.8204, loss=0.3616/acc=0.8361, loss=0.3855/acc=0.8103)
  [mlp] layers=[16, 256, 32, 16, 32, 32] act=leaky_relu lr=0.0046 batch=128 epochs=20 -> avg_log_loss=0.3892 (folds: loss=0.3788/acc=0.8015, loss=0.4075/acc=0.8116, loss=0.4370/acc=0.7737, loss=0.3898/acc=0.8066, loss=0.3752/acc=0.8214, loss=
0.3870/acc=0.8112, loss=0.3526/acc=0.8389, loss=0.3859/acc=0.8195)
  [mlp] layers=[16, 256, 64, 16, 32, 128] act=leaky_relu lr=0.0039 batch=64 epochs=10 -> avg_log_loss=0.3953 (folds: loss=0.3865/acc=0.7895, loss=0.4106/acc=0.8024, loss=0.4353/acc=0.7718, loss=0.3937/acc=0.7983, loss=0.3741/acc=0.8195, loss=
0.3931/acc=0.8094, loss=0.3644/acc=0.8195, loss=0.4046/acc=0.8039)
  [mlp] layers=[16, 16, 16, 256, 32, 32] act=leaky_relu lr=0.0050 batch=64 epochs=20 -> avg_log_loss=0.3915 (folds: loss=0.3804/acc=0.7960, loss=0.4090/acc=0.7996, loss=0.4411/acc=0.7691, loss=0.3928/acc=0.8085, loss=0.3745/acc=0.8122, loss=0
.3893/acc=0.8158, loss=0.3558/acc=0.8352, loss=0.3888/acc=0.8029)
  [histgb] histgb trees=200 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4265 (folds: loss=0.4169/acc=0.7960, loss=0.4380/acc=0.7665, loss=0.4668/acc=0.7590, loss=0.4294/acc=0.7910, loss=0.4158/acc=0.8048, loss=0.4213/acc=0.7974,
 loss=0.3979/acc=0.8140, loss=0.4261/acc=0.7983)
  [mlp] layers=[64, 16, 256] act=gelu lr=0.0050 batch=64 epochs=25 -> avg_log_loss=0.4022 (folds: loss=0.3819/acc=0.8061, loss=0.4244/acc=0.7996, loss=0.4634/acc=0.7709, loss=0.4058/acc=0.8066, loss=0.3901/acc=0.8260, loss=0.3809/acc=0.8131,
loss=0.3678/acc=0.8195, loss=0.4036/acc=0.8112)
  [mlp] layers=[16, 64, 16, 16, 32, 128] act=leaky_relu lr=0.0049 batch=128 epochs=20 -> avg_log_loss=0.3924 (folds: loss=0.3809/acc=0.8033, loss=0.4084/acc=0.8033, loss=0.4337/acc=0.7847, loss=0.3924/acc=0.8002, loss=0.3871/acc=0.8094, loss=
0.3892/acc=0.8149, loss=0.3568/acc=0.8398, loss=0.3906/acc=0.8158)
  [mlp] layers=[16, 256, 64, 16, 32, 128] act=leaky_relu lr=0.0050 batch=128 epochs=20 -> avg_log_loss=0.3895 (folds: loss=0.3800/acc=0.7960, loss=0.4048/acc=0.8051, loss=0.4374/acc=0.7718, loss=0.3963/acc=0.7974, loss=0.3775/acc=0.8140, loss
=0.3875/acc=0.8131, loss=0.3490/acc=0.8343, loss=0.3838/acc=0.8029)
  [mlp] layers=[16, 512, 16, 256, 32, 32] act=leaky_relu lr=0.0050 batch=128 epochs=10 -> avg_log_loss=0.3947 (folds: loss=0.3789/acc=0.8033, loss=0.4108/acc=0.7987, loss=0.4425/acc=0.7663, loss=0.3960/acc=0.8140, loss=0.3862/acc=0.8103, loss
=0.3902/acc=0.8158, loss=0.3660/acc=0.8278, loss=0.3869/acc=0.8112)
  [gbstump] gbstump estimators=200 lr=0.3 min_leaf=200 -> avg_log_loss=0.4457 (folds: loss=0.4345/acc=0.7895, loss=0.4562/acc=0.7767, loss=0.4796/acc=0.7663, loss=0.4497/acc=0.7882, loss=0.4259/acc=0.8131, loss=0.4431/acc=0.7956, loss=0.4258/
acc=0.8149, loss=0.4509/acc=0.8011)
  best avg log loss this gen: 0.3882
Generation 9/20
  [mlp] layers=[16, 256, 16, 256, 256, 32] act=leaky_relu lr=0.0049 batch=64 epochs=25 -> avg_log_loss=0.3886 (folds: loss=0.3763/acc=0.8015, loss=0.4066/acc=0.8033, loss=0.4380/acc=0.7792, loss=0.3898/acc=0.8066, loss=0.3764/acc=0.8177, loss
=0.3795/acc=0.8195, loss=0.3540/acc=0.8398, loss=0.3885/acc=0.8177)
  [mlp] layers=[16, 256, 16, 16, 32, 128] act=leaky_relu lr=0.0044 batch=64 epochs=30 -> avg_log_loss=0.3890 (folds: loss=0.3738/acc=0.8079, loss=0.4070/acc=0.8051, loss=0.4431/acc=0.7691, loss=0.3896/acc=0.8094, loss=0.3754/acc=0.8186, loss=
0.3782/acc=0.8158, loss=0.3535/acc=0.8306, loss=0.3918/acc=0.8177)
  [mlp] layers=[16, 256, 32, 16, 512, 32] act=leaky_relu lr=0.0043 batch=128 epochs=10 -> avg_log_loss=0.3941 (folds: loss=0.3838/acc=0.7950, loss=0.4126/acc=0.7886, loss=0.4371/acc=0.7801, loss=0.3937/acc=0.7974, loss=0.3805/acc=0.8186, loss
=0.3868/acc=0.8232, loss=0.3633/acc=0.8315, loss=0.3946/acc=0.8122)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4047 (folds: loss=0.3966/acc=0.7978, loss=0.4252/acc=0.8015, loss=0.4439/acc=0.7866, loss=0.4055/acc=0.8029, loss=0.3868/acc=0.8186, loss=0.4090/acc
=0.8131, loss=0.3700/acc=0.8306, loss=0.4009/acc=0.8131)
  [mlp] layers=[16, 256, 32, 16, 64, 32] act=leaky_relu lr=0.0050 batch=512 epochs=30 -> avg_log_loss=0.3988 (folds: loss=0.3795/acc=0.8024, loss=0.4280/acc=0.7978, loss=0.4565/acc=0.7764, loss=0.3978/acc=0.8168, loss=0.3831/acc=0.8094, loss=
0.3897/acc=0.8076, loss=0.3603/acc=0.8315, loss=0.3958/acc=0.8085)
  [mlp] layers=[256, 256, 16, 16, 32, 128] act=leaky_relu lr=0.0050 batch=512 epochs=25 -> avg_log_loss=0.4313 (folds: loss=0.4294/acc=0.8051, loss=0.4363/acc=0.7960, loss=0.5119/acc=0.7728, loss=0.4328/acc=0.8002, loss=0.4269/acc=0.8039, los
s=0.3959/acc=0.8186, loss=0.3789/acc=0.8250, loss=0.4385/acc=0.8066)
  [mlp] layers=[16, 256, 32, 16, 32, 16] act=leaky_relu lr=0.0038 batch=128 epochs=10 -> avg_log_loss=0.3937 (folds: loss=0.3860/acc=0.7950, loss=0.4062/acc=0.7932, loss=0.4457/acc=0.7691, loss=0.3949/acc=0.7993, loss=0.3819/acc=0.8122, loss=
0.3862/acc=0.8177, loss=0.3563/acc=0.8462, loss=0.3926/acc=0.8131)
  [mlp] layers=[16, 256, 16, 16, 256, 128] act=leaky_relu lr=0.0043 batch=64 epochs=20 -> avg_log_loss=0.3920 (folds: loss=0.3810/acc=0.8097, loss=0.4089/acc=0.8061, loss=0.4361/acc=0.7746, loss=0.3915/acc=0.8011, loss=0.3833/acc=0.8186, loss
=0.3865/acc=0.8168, loss=0.3634/acc=0.8250, loss=0.3854/acc=0.8149)
  [mlp] layers=[16, 256, 16, 16, 32, 512] act=leaky_relu lr=0.0038 batch=256 epochs=15 -> avg_log_loss=0.3951 (folds: loss=0.3880/acc=0.7996, loss=0.4194/acc=0.8006, loss=0.4465/acc=0.7774, loss=0.3915/acc=0.8039, loss=0.3800/acc=0.8241, loss
=0.3874/acc=0.8039, loss=0.3577/acc=0.8370, loss=0.3900/acc=0.7993)
  [mlp] layers=[16, 256, 16, 256, 256, 32] act=leaky_relu lr=0.0048 batch=512 epochs=10 -> avg_log_loss=0.4010 (folds: loss=0.3918/acc=0.7914, loss=0.4110/acc=0.8042, loss=0.4444/acc=0.7654, loss=0.4025/acc=0.7937, loss=0.3981/acc=0.8066, los
s=0.3956/acc=0.8085, loss=0.3680/acc=0.8204, loss=0.3963/acc=0.8066)
  [mlp] layers=[16, 256, 16, 256, 128, 32] act=leaky_relu lr=0.0050 batch=64 epochs=30 -> avg_log_loss=0.3877 (folds: loss=0.3781/acc=0.8070, loss=0.4066/acc=0.8042, loss=0.4332/acc=0.7856, loss=0.3827/acc=0.8029, loss=0.3778/acc=0.8232, loss
=0.3840/acc=0.8149, loss=0.3441/acc=0.8389, loss=0.3952/acc=0.7993)
  [mlp] layers=[16, 256, 16, 16, 32, 32] act=leaky_relu lr=0.0050 batch=256 epochs=25 -> avg_log_loss=0.3927 (folds: loss=0.3786/acc=0.8024, loss=0.4116/acc=0.8015, loss=0.4422/acc=0.7801, loss=0.3877/acc=0.8066, loss=0.3909/acc=0.8168, loss=
0.3837/acc=0.8057, loss=0.3497/acc=0.8407, loss=0.3969/acc=0.8094)
  [mlp] layers=[16, 256, 16, 16, 32, 64] act=leaky_relu lr=0.0041 batch=128 epochs=25 -> avg_log_loss=0.3904 (folds: loss=0.3715/acc=0.8006, loss=0.4039/acc=0.8107, loss=0.4379/acc=0.7626, loss=0.4013/acc=0.8085, loss=0.3773/acc=0.8195, loss=
0.3824/acc=0.8149, loss=0.3532/acc=0.8315, loss=0.3954/acc=0.7965)



  [mlp] layers=[16, 256, 32, 16, 512, 32] act=leaky_relu lr=0.0044 batch=128 epochs=30 -> avg_log_loss=0.3914 (folds: loss=0.3734/acc=0.8107, loss=0.4112/acc=0.7978, loss=0.4527/acc=0.7755, loss=0.3907/acc=0.8158, loss=0.3721/acc=0.8177, loss
=0.3755/acc=0.8158, loss=0.3547/acc=0.8407, loss=0.4009/acc=0.8011)
  [mlp] layers=[16, 256, 32, 16, 128, 32] act=leaky_relu lr=0.0050 batch=128 epochs=10 -> avg_log_loss=0.3920 (folds: loss=0.3826/acc=0.7886, loss=0.4120/acc=0.8033, loss=0.4444/acc=0.7764, loss=0.3992/acc=0.8085, loss=0.3732/acc=0.8204, loss
=0.3863/acc=0.8223, loss=0.3503/acc=0.8398, loss=0.3883/acc=0.8057)
  [mlp] layers=[16, 256, 16, 16, 64, 32] act=leaky_relu lr=0.0050 batch=64 epochs=30 -> avg_log_loss=0.3881 (folds: loss=0.3734/acc=0.8061, loss=0.4073/acc=0.7923, loss=0.4423/acc=0.7829, loss=0.3862/acc=0.8112, loss=0.3704/acc=0.8260, loss=0
.3883/acc=0.8158, loss=0.3560/acc=0.8416, loss=0.3808/acc=0.8039)
  [mlp] layers=[16, 256, 16, 16, 32, 32] act=leaky_relu lr=0.0039 batch=256 epochs=20 -> avg_log_loss=0.3893 (folds: loss=0.3788/acc=0.8079, loss=0.4014/acc=0.8107, loss=0.4373/acc=0.7746, loss=0.3940/acc=0.7993, loss=0.3751/acc=0.8278, loss=
0.3839/acc=0.8122, loss=0.3480/acc=0.8481, loss=0.3954/acc=0.8020)
  [mlp] layers=[16, 256, 16, 32, 32, 128] act=leaky_relu lr=0.0048 batch=64 epochs=30 -> avg_log_loss=0.3890 (folds: loss=0.3770/acc=0.8024, loss=0.4043/acc=0.8006, loss=0.4364/acc=0.7672, loss=0.3884/acc=0.8094, loss=0.3810/acc=0.8168, loss=
0.3836/acc=0.8158, loss=0.3571/acc=0.8370, loss=0.3844/acc=0.8066)
  best avg log loss this gen: 0.3877
Generation 10/20
  [mlp] layers=[16, 256, 16, 256, 128, 32] act=leaky_relu lr=0.0050 batch=64 epochs=30 -> avg_log_loss=0.3879 (folds: loss=0.3785/acc=0.7987, loss=0.4045/acc=0.8125, loss=0.4386/acc=0.7764, loss=0.3883/acc=0.8085, loss=0.3738/acc=0.8214, loss
=0.3770/acc=0.8186, loss=0.3508/acc=0.8177, loss=0.3919/acc=0.8140)
  [mlp] layers=[16, 256, 16, 16, 64, 32] act=leaky_relu lr=0.0050 batch=64 epochs=30 -> avg_log_loss=0.3889 (folds: loss=0.3839/acc=0.7978, loss=0.4043/acc=0.8042, loss=0.4334/acc=0.7783, loss=0.3931/acc=0.8002, loss=0.3763/acc=0.8214, loss=0
.3826/acc=0.8122, loss=0.3520/acc=0.8315, loss=0.3855/acc=0.8140)
  [mlp] layers=[16, 256, 16, 32, 32, 64] act=leaky_relu lr=0.0050 batch=64 epochs=20 -> avg_log_loss=0.3909 (folds: loss=0.3761/acc=0.8042, loss=0.4051/acc=0.8061, loss=0.4345/acc=0.7810, loss=0.3901/acc=0.8020, loss=0.3880/acc=0.8122, loss=0
.3893/acc=0.8186, loss=0.3512/acc=0.8370, loss=0.3929/acc=0.8002)
  [mlp] layers=[16, 256, 512, 32, 32, 128] act=leaky_relu lr=0.0043 batch=512 epochs=25 -> avg_log_loss=0.3948 (folds: loss=0.3808/acc=0.8015, loss=0.4173/acc=0.8033, loss=0.4429/acc=0.7792, loss=0.3912/acc=0.8029, loss=0.3795/acc=0.8122, los
s=0.4045/acc=0.8057, loss=0.3543/acc=0.8425, loss=0.3878/acc=0.8131)
  [mlp] layers=[16, 256, 16, 16, 64, 16] act=leaky_relu lr=0.0043 batch=256 epochs=20 -> avg_log_loss=0.3923 (folds: loss=0.3754/acc=0.8125, loss=0.4097/acc=0.8042, loss=0.4414/acc=0.7663, loss=0.3905/acc=0.8076, loss=0.3860/acc=0.8140, loss=
0.3884/acc=0.8112, loss=0.3606/acc=0.8278, loss=0.3867/acc=0.8048)
  [mlp] layers=[16, 256, 16, 512, 64, 32] act=leaky_relu lr=0.0048 batch=256 epochs=15 -> avg_log_loss=0.3929 (folds: loss=0.3885/acc=0.7960, loss=0.4214/acc=0.7941, loss=0.4340/acc=0.7801, loss=0.3944/acc=0.8131, loss=0.3763/acc=0.8140, loss
=0.3788/acc=0.8214, loss=0.3624/acc=0.8306, loss=0.3876/acc=0.8029)
  [rf] rf trees=300 depth=7 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4358 (folds: loss=0.4362/acc=0.7987, loss=0.4514/acc=0.7794, loss=0.4917/acc=0.7663, loss=0.4355/acc=0.8011, loss=0.4125/acc=0.8112, loss=0.4228/acc=0.8020, loss=0.
4026/acc=0.8168, loss=0.4338/acc=0.8122)
  [mlp] layers=[64, 256, 16, 256, 128, 32] act=leaky_relu lr=0.0050 batch=128 epochs=20 -> avg_log_loss=0.3997 (folds: loss=0.3819/acc=0.8051, loss=0.4175/acc=0.7969, loss=0.4598/acc=0.7820, loss=0.4111/acc=0.8029, loss=0.3912/acc=0.8195, los
s=0.3799/acc=0.8149, loss=0.3635/acc=0.8260, loss=0.3929/acc=0.7947)
  [gbstump] gbstump estimators=150 lr=0.2 min_leaf=50 -> avg_log_loss=0.4452 (folds: loss=0.4337/acc=0.7822, loss=0.4509/acc=0.7767, loss=0.4848/acc=0.7728, loss=0.4504/acc=0.7845, loss=0.4249/acc=0.8140, loss=0.4438/acc=0.7901, loss=0.4183/a
cc=0.8232, loss=0.4547/acc=0.7864)
  [mlp] layers=[16, 256, 16, 32, 32, 128] act=leaky_relu lr=0.0042 batch=64 epochs=30 -> avg_log_loss=0.3896 (folds: loss=0.3779/acc=0.7950, loss=0.4028/acc=0.8006, loss=0.4367/acc=0.7764, loss=0.3945/acc=0.8020, loss=0.3826/acc=0.8103, loss=
0.3851/acc=0.8076, loss=0.3497/acc=0.8398, loss=0.3875/acc=0.7965)
  [mlp] layers=[16, 256, 64, 256, 256, 32] act=leaky_relu lr=0.0039 batch=64 epochs=15 -> avg_log_loss=0.3913 (folds: loss=0.3778/acc=0.8024, loss=0.4031/acc=0.8088, loss=0.4343/acc=0.7746, loss=0.3899/acc=0.8158, loss=0.3768/acc=0.8195, loss
=0.3957/acc=0.8260, loss=0.3600/acc=0.8333, loss=0.3926/acc=0.8002)
  [rf] rf trees=300 depth=4 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4209 (folds: loss=0.4094/acc=0.7923, loss=0.4390/acc=0.7757, loss=0.4727/acc=0.7672, loss=0.4206/acc=0.7919, loss=0.3979/acc=0.8085, loss=0.4159/acc=0.8011, loss=0.
3937/acc=0.8177, loss=0.4182/acc=0.8112)
  [rf] rf trees=300 depth=8 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4352 (folds: loss=0.4376/acc=0.7932, loss=0.4486/acc=0.7785, loss=0.4885/acc=0.7636, loss=0.4339/acc=0.8002, loss=0.4163/acc=0.8076, loss=0.4213/acc=0.7974, loss=0.
4061/acc=0.8140, loss=0.4294/acc=0.8002)
  [extratrees] extratrees trees=200 depth=6 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4001 (folds: loss=0.3915/acc=0.7950, loss=0.4206/acc=0.7978, loss=0.4426/acc=0.7856, loss=0.4012/acc=0.8112, loss=0.3828/acc=0.8168, loss=0.4022/acc
=0.8131, loss=0.3669/acc=0.8297, loss=0.3933/acc=0.8214)
  [extratrees] extratrees trees=100 depth=8 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.3952 (folds: loss=0.3839/acc=0.8006, loss=0.4179/acc=0.7960, loss=0.4401/acc=0.7764, loss=0.3976/acc=0.8094, loss=0.3775/acc=0.8131, loss=0.3935/acc
=0.8168, loss=0.3640/acc=0.8324, loss=0.3872/acc=0.8214)
  [gbstump] gbstump estimators=150 lr=0.1 min_leaf=100 -> avg_log_loss=0.4387 (folds: loss=0.4298/acc=0.7840, loss=0.4443/acc=0.7895, loss=0.4762/acc=0.7755, loss=0.4420/acc=0.7845, loss=0.4212/acc=0.8048, loss=0.4386/acc=0.8011, loss=0.4142/
acc=0.8232, loss=0.4433/acc=0.8057)
  [mlp] layers=[16, 32, 16, 256, 256, 32] act=leaky_relu lr=0.0050 batch=64 epochs=20 -> avg_log_loss=0.3882 (folds: loss=0.3763/acc=0.8042, loss=0.4044/acc=0.8051, loss=0.4415/acc=0.7709, loss=0.3836/acc=0.8085, loss=0.3768/acc=0.8177, loss=
0.3827/acc=0.8177, loss=0.3482/acc=0.8260, loss=0.3919/acc=0.8039)
  [mlp] layers=[16, 256, 16, 16, 32, 128] act=leaky_relu lr=0.0044 batch=128 epochs=10 -> avg_log_loss=0.4000 (folds: loss=0.3852/acc=0.8015, loss=0.4124/acc=0.8015, loss=0.4385/acc=0.7856, loss=0.4016/acc=0.7974, loss=0.3979/acc=0.8094, loss
=0.3939/acc=0.8122, loss=0.3720/acc=0.8241, loss=0.3988/acc=0.7937)
  best avg log loss this gen: 0.3879
Generation 11/20
  [mlp] layers=[16, 256, 16, 256, 128, 32] act=leaky_relu lr=0.0050 batch=64 epochs=30 -> avg_log_loss=0.3895 (folds: loss=0.3750/acc=0.8125, loss=0.4031/acc=0.8042, loss=0.4424/acc=0.7783, loss=0.3879/acc=0.8048, loss=0.3793/acc=0.8131, loss
=0.3861/acc=0.8158, loss=0.3543/acc=0.8361, loss=0.3874/acc=0.8066)
  [mlp] layers=[16, 32, 16, 256, 256, 32] act=leaky_relu lr=0.0050 batch=64 epochs=20 -> avg_log_loss=0.3922 (folds: loss=0.3798/acc=0.8061, loss=0.4051/acc=0.7987, loss=0.4432/acc=0.7866, loss=0.3923/acc=0.8002, loss=0.3825/acc=0.8269, loss=
0.3827/acc=0.8112, loss=0.3620/acc=0.8333, loss=0.3901/acc=0.8094)
  [mlp] layers=[16, 512, 16, 16, 64, 32] act=leaky_relu lr=0.0050 batch=256 epochs=10 -> avg_log_loss=0.3939 (folds: loss=0.3772/acc=0.8143, loss=0.4149/acc=0.7914, loss=0.4446/acc=0.7737, loss=0.3961/acc=0.8048, loss=0.3837/acc=0.8122, loss=
0.3853/acc=0.8168, loss=0.3588/acc=0.8333, loss=0.3904/acc=0.8094)
  [mlp] layers=[16, 32, 16, 256, 256, 32] act=leaky_relu lr=0.0050 batch=64 epochs=20 -> avg_log_loss=0.3916 (folds: loss=0.3760/acc=0.8088, loss=0.4052/acc=0.8033, loss=0.4421/acc=0.7810, loss=0.3895/acc=0.8103, loss=0.3733/acc=0.8195, loss=
0.3950/acc=0.8103, loss=0.3608/acc=0.8361, loss=0.3909/acc=0.8066)
  [mlp] layers=[16, 256, 32, 256, 128, 32] act=leaky_relu lr=0.0043 batch=256 epochs=30 -> avg_log_loss=0.3933 (folds: loss=0.3941/acc=0.8070, loss=0.4117/acc=0.8024, loss=0.4454/acc=0.7764, loss=0.3850/acc=0.8158, loss=0.3807/acc=0.8214, los
s=0.3975/acc=0.8112, loss=0.3468/acc=0.8398, loss=0.3849/acc=0.8177)
  [mlp] layers=[16, 32, 16, 32, 256, 32] act=leaky_relu lr=0.0043 batch=128 epochs=30 -> avg_log_loss=0.3905 (folds: loss=0.3824/acc=0.8006, loss=0.4066/acc=0.8125, loss=0.4380/acc=0.7718, loss=0.3941/acc=0.8039, loss=0.3762/acc=0.8140, loss=
0.3828/acc=0.8131, loss=0.3564/acc=0.8407, loss=0.3877/acc=0.8076)
  [mlp] layers=[16, 256, 16, 16, 32, 32] act=leaky_relu lr=0.0050 batch=128 epochs=20 -> avg_log_loss=0.3906 (folds: loss=0.3710/acc=0.7978, loss=0.4110/acc=0.8006, loss=0.4387/acc=0.7774, loss=0.3966/acc=0.8066, loss=0.3914/acc=0.8103, loss=
0.3785/acc=0.8122, loss=0.3483/acc=0.8379, loss=0.3894/acc=0.8094)
  [histgb] histgb trees=100 depth=8 max_feat=0.50 sample_ratio=0.60 -> avg_log_loss=0.4262 (folds: loss=0.4169/acc=0.7987, loss=0.4375/acc=0.7702, loss=0.4638/acc=0.7571, loss=0.4292/acc=0.8011, loss=0.4140/acc=0.8131, loss=0.4205/acc=0.7947,
 loss=0.3996/acc=0.8158, loss=0.4280/acc=0.8029)
  [histgb] histgb trees=400 depth=6 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4370 (folds: loss=0.4273/acc=0.8042, loss=0.4452/acc=0.7767, loss=0.4778/acc=0.7682, loss=0.4400/acc=0.7937, loss=0.4257/acc=0.8131, loss=0.4328/acc=0.8011,
 loss=0.4105/acc=0.8158, loss=0.4365/acc=0.7993)
  [mlp] layers=[16, 32, 16, 256, 256, 16] act=leaky_relu lr=0.0050 batch=256 epochs=20 -> avg_log_loss=0.3922 (folds: loss=0.3769/acc=0.7969, loss=0.4093/acc=0.8051, loss=0.4451/acc=0.7663, loss=0.3900/acc=0.7993, loss=0.3919/acc=0.8149, loss
=0.3854/acc=0.8140, loss=0.3473/acc=0.8398, loss=0.3919/acc=0.8112)
  [mlp] layers=[128, 32] act=leaky_relu lr=0.0041 batch=64 epochs=15 -> avg_log_loss=0.3962 (folds: loss=0.3819/acc=0.8143, loss=0.4158/acc=0.7987, loss=0.4589/acc=0.7718, loss=0.3945/acc=0.8076, loss=0.3816/acc=0.8131, loss=0.3840/acc=0.8204
, loss=0.3585/acc=0.8315, loss=0.3943/acc=0.8020)
  [mlp] layers=[16, 128, 16, 256, 128, 32] act=leaky_relu lr=0.0050 batch=64 epochs=10 -> avg_log_loss=0.3939 (folds: loss=0.3825/acc=0.7978, loss=0.4126/acc=0.7969, loss=0.4383/acc=0.7774, loss=0.3911/acc=0.8020, loss=0.3741/acc=0.8140, loss
=0.3981/acc=0.8057, loss=0.3687/acc=0.8287, loss=0.3855/acc=0.8131)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=150 -> avg_log_loss=0.4409 (folds: loss=0.4316/acc=0.7831, loss=0.4480/acc=0.7895, loss=0.4781/acc=0.7728, loss=0.4432/acc=0.7947, loss=0.4251/acc=0.8076, loss=0.4407/acc=0.7956, loss=0.4149/
acc=0.8223, loss=0.4457/acc=0.8076)
  [mlp] layers=[16, 256, 64, 16, 64, 32] act=leaky_relu lr=0.0050 batch=64 epochs=30 -> avg_log_loss=0.3912 (folds: loss=0.3770/acc=0.8006, loss=0.4002/acc=0.8042, loss=0.4440/acc=0.7774, loss=0.3990/acc=0.7983, loss=0.3815/acc=0.8158, loss=0
.3849/acc=0.8204, loss=0.3498/acc=0.8352, loss=0.3929/acc=0.8029)
  [mlp] layers=[16, 256, 16, 256, 128, 32] act=leaky_relu lr=0.0040 batch=128 epochs=15 -> avg_log_loss=0.3929 (folds: loss=0.3800/acc=0.8042, loss=0.4106/acc=0.7978, loss=0.4431/acc=0.7718, loss=0.3978/acc=0.7974, loss=0.3828/acc=0.8085, los
s=0.3933/acc=0.8186, loss=0.3538/acc=0.8361, loss=0.3817/acc=0.8122)
  [mlp] layers=[16, 512, 16, 256, 256, 32] act=leaky_relu lr=0.0050 batch=128 epochs=15 -> avg_log_loss=0.3909 (folds: loss=0.3766/acc=0.8006, loss=0.4039/acc=0.8061, loss=0.4393/acc=0.7746, loss=0.3882/acc=0.8131, loss=0.3822/acc=0.8103, los
s=0.3878/acc=0.8140, loss=0.3617/acc=0.8324, loss=0.3874/acc=0.8029)
  [mlp] layers=[16, 32, 16, 256, 256, 16] act=leaky_relu lr=0.0047 batch=128 epochs=20 -> avg_log_loss=0.3928 (folds: loss=0.3821/acc=0.8042, loss=0.4054/acc=0.8097, loss=0.4404/acc=0.7783, loss=0.3927/acc=0.8039, loss=0.3872/acc=0.8149, loss
=0.3828/acc=0.8149, loss=0.3644/acc=0.8287, loss=0.3878/acc=0.8140)
  [mlp] layers=[16, 256, 128, 16, 64, 32] act=leaky_relu lr=0.0050 batch=128 epochs=30 -> avg_log_loss=0.3914 (folds: loss=0.3813/acc=0.8070, loss=0.4099/acc=0.8051, loss=0.4525/acc=0.7672, loss=0.3922/acc=0.8002, loss=0.3763/acc=0.8186, loss
=0.3813/acc=0.8131, loss=0.3525/acc=0.8260, loss=0.3848/acc=0.8122)
  best avg log loss this gen: 0.3895
Generation 12/20
  [mlp] layers=[16, 256, 16, 256, 128, 32] act=leaky_relu lr=0.0050 batch=64 epochs=30 -> avg_log_loss=0.3880 (folds: loss=0.3754/acc=0.8079, loss=0.4107/acc=0.8116, loss=0.4371/acc=0.7801, loss=0.3899/acc=0.8131, loss=0.3653/acc=0.8149, loss
=0.3782/acc=0.8186, loss=0.3528/acc=0.8416, loss=0.3944/acc=0.8103)
  [mlp] layers=[16, 32, 16, 32, 256, 32] act=leaky_relu lr=0.0043 batch=128 epochs=30 -> avg_log_loss=0.3884 (folds: loss=0.3841/acc=0.7950, loss=0.4064/acc=0.8006, loss=0.4396/acc=0.7700, loss=0.3879/acc=0.8158, loss=0.3716/acc=0.8149, loss=
0.3796/acc=0.8186, loss=0.3507/acc=0.8324, loss=0.3873/acc=0.8076)
  [histgb] histgb trees=400 depth=4 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4306 (folds: loss=0.4227/acc=0.7978, loss=0.4399/acc=0.7684, loss=0.4708/acc=0.7599, loss=0.4327/acc=0.7891, loss=0.4200/acc=0.8057, loss=0.4263/acc=0.7974,
 loss=0.4021/acc=0.8131, loss=0.4305/acc=0.8039)
  [gbstump] gbstump estimators=200 lr=0.1 min_leaf=50 -> avg_log_loss=0.4420 (folds: loss=0.4314/acc=0.7877, loss=0.4455/acc=0.7877, loss=0.4816/acc=0.7764, loss=0.4467/acc=0.7864, loss=0.4225/acc=0.8076, loss=0.4399/acc=0.7993, loss=0.4168/a
cc=0.8223, loss=0.4517/acc=0.7910)
  [histgb] histgb trees=100 depth=4 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4273 (folds: loss=0.4197/acc=0.7978, loss=0.4377/acc=0.7812, loss=0.4712/acc=0.7709, loss=0.4326/acc=0.7974, loss=0.4133/acc=0.8112, loss=0.4221/acc=0.7983,
 loss=0.3982/acc=0.8112, loss=0.4233/acc=0.8002)
  [mlp] layers=[16, 256, 32, 16, 32, 32] act=leaky_relu lr=0.0050 batch=128 epochs=30 -> avg_log_loss=0.3900 (folds: loss=0.3763/acc=0.8042, loss=0.4064/acc=0.7987, loss=0.4418/acc=0.7737, loss=0.3928/acc=0.7956, loss=0.3831/acc=0.8214, loss=
0.3873/acc=0.8177, loss=0.3492/acc=0.8389, loss=0.3828/acc=0.8057)
  [mlp] layers=[16, 256, 16, 256, 128, 64] act=leaky_relu lr=0.0050 batch=64 epochs=15 -> avg_log_loss=0.3931 (folds: loss=0.3783/acc=0.8143, loss=0.4156/acc=0.8061, loss=0.4372/acc=0.7728, loss=0.3885/acc=0.8149, loss=0.3875/acc=0.8085, loss
=0.3897/acc=0.8122, loss=0.3543/acc=0.8250, loss=0.3933/acc=0.8131)
  [histgb] histgb trees=300 depth=6 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4386 (folds: loss=0.4301/acc=0.7914, loss=0.4466/acc=0.7748, loss=0.4797/acc=0.7636, loss=0.4397/acc=0.7873, loss=0.4275/acc=0.8085, loss=0.4343/acc=0.7983,
 loss=0.4096/acc=0.8131, loss=0.4414/acc=0.7993)
  [mlp] layers=[16, 64, 16, 256, 256, 32] act=leaky_relu lr=0.0050 batch=256 epochs=30 -> avg_log_loss=0.3906 (folds: loss=0.3866/acc=0.8015, loss=0.4045/acc=0.8042, loss=0.4410/acc=0.7755, loss=0.3907/acc=0.8020, loss=0.3760/acc=0.8195, loss
=0.3812/acc=0.8149, loss=0.3537/acc=0.8435, loss=0.3913/acc=0.8066)
  [mlp] layers=[16, 32, 16, 16, 256, 32] act=leaky_relu lr=0.0047 batch=512 epochs=20 -> avg_log_loss=0.3931 (folds: loss=0.3860/acc=0.7969, loss=0.4089/acc=0.8024, loss=0.4358/acc=0.7728, loss=0.3894/acc=0.8094, loss=0.3825/acc=0.8103, loss=
0.3878/acc=0.8076, loss=0.3574/acc=0.8241, loss=0.3969/acc=0.8066)
  [mlp] layers=[16, 256, 256, 256, 128, 32] act=leaky_relu lr=0.0050 batch=512 epochs=30 -> avg_log_loss=0.3993 (folds: loss=0.3841/acc=0.7950, loss=0.4240/acc=0.8042, loss=0.4561/acc=0.7746, loss=0.3915/acc=0.8048, loss=0.3916/acc=0.8066, lo
ss=0.3881/acc=0.8094, loss=0.3576/acc=0.8324, loss=0.4012/acc=0.8057)
  [mlp] layers=[16, 32, 16, 16, 32, 32] act=leaky_relu lr=0.0041 batch=256 epochs=25 -> avg_log_loss=0.3894 (folds: loss=0.3738/acc=0.7987, loss=0.4067/acc=0.8061, loss=0.4496/acc=0.7682, loss=0.3934/acc=0.8094, loss=0.3769/acc=0.8204, loss=0
.3781/acc=0.8186, loss=0.3509/acc=0.8435, loss=0.3854/acc=0.8131)
  [mlp] layers=[32, 128, 32, 128, 32] act=relu lr=0.0050 batch=128 epochs=15 -> avg_log_loss=0.4023 (folds: loss=0.4021/acc=0.8061, loss=0.4223/acc=0.8051, loss=0.4610/acc=0.7700, loss=0.3936/acc=0.8076, loss=0.3756/acc=0.8158, loss=0.3900/ac
c=0.8149, loss=0.3735/acc=0.8269, loss=0.4004/acc=0.8131)
  [mlp] layers=[256, 256, 16, 16, 32, 32] act=leaky_relu lr=0.0047 batch=64 epochs=25 -> avg_log_loss=0.4122 (folds: loss=0.3919/acc=0.8107, loss=0.4270/acc=0.7987, loss=0.4811/acc=0.7764, loss=0.4179/acc=0.8039, loss=0.4186/acc=0.8076, loss=
0.3807/acc=0.8168, loss=0.3709/acc=0.8444, loss=0.4092/acc=0.8048)
  [histgb] histgb trees=400 depth=4 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4330 (folds: loss=0.4208/acc=0.8015, loss=0.4431/acc=0.7767, loss=0.4730/acc=0.7654, loss=0.4379/acc=0.8103, loss=0.4198/acc=0.8149, loss=0.4274/acc=0.7983,
 loss=0.4083/acc=0.8076, loss=0.4336/acc=0.7937)
  [mlp] layers=[16, 32, 16, 256, 256, 32] act=leaky_relu lr=0.0047 batch=64 epochs=10 -> avg_log_loss=0.3991 (folds: loss=0.3865/acc=0.7969, loss=0.4195/acc=0.8070, loss=0.4386/acc=0.7810, loss=0.3953/acc=0.8057, loss=0.3908/acc=0.8195, loss=
0.3940/acc=0.8186, loss=0.3722/acc=0.8287, loss=0.3958/acc=0.8020)
  [rf] rf trees=300 depth=5 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4336 (folds: loss=0.4313/acc=0.7932, loss=0.4488/acc=0.7748, loss=0.4849/acc=0.7663, loss=0.4326/acc=0.7937, loss=0.4116/acc=0.8057, loss=0.4203/acc=0.8029, loss=0.
4068/acc=0.8177, loss=0.4324/acc=0.8076)
  [mlp] layers=[16, 32, 512, 32, 256, 32] act=leaky_relu lr=0.0038 batch=64 epochs=20 -> avg_log_loss=0.3920 (folds: loss=0.3787/acc=0.8015, loss=0.4083/acc=0.8015, loss=0.4427/acc=0.7700, loss=0.3915/acc=0.8140, loss=0.3820/acc=0.8306, loss=
0.3854/acc=0.8066, loss=0.3566/acc=0.8361, loss=0.3912/acc=0.8122)
  best avg log loss this gen: 0.3880
Generation 13/20
  [mlp] layers=[16, 256, 16, 256, 128, 32] act=leaky_relu lr=0.0050 batch=64 epochs=30 -> avg_log_loss=0.3902 (folds: loss=0.3820/acc=0.8024, loss=0.4056/acc=0.8061, loss=0.4379/acc=0.7810, loss=0.3894/acc=0.8039, loss=0.3790/acc=0.8177, loss
=0.3841/acc=0.8057, loss=0.3537/acc=0.8260, loss=0.3897/acc=0.8204)
  [mlp] layers=[16, 32, 16, 32, 256, 32] act=leaky_relu lr=0.0043 batch=128 epochs=30 -> avg_log_loss=0.3882 (folds: loss=0.3783/acc=0.7969, loss=0.4016/acc=0.8107, loss=0.4385/acc=0.7774, loss=0.3899/acc=0.8002, loss=0.3813/acc=0.8112, loss=
0.3859/acc=0.8168, loss=0.3477/acc=0.8260, loss=0.3823/acc=0.8066)
  [mlp] layers=[16, 32, 16, 32, 256, 32] act=leaky_relu lr=0.0046 batch=512 epochs=25 -> avg_log_loss=0.3913 (folds: loss=0.3813/acc=0.8088, loss=0.4114/acc=0.8051, loss=0.4396/acc=0.7792, loss=0.3943/acc=0.8011, loss=0.3773/acc=0.8232, loss=
0.3814/acc=0.8149, loss=0.3570/acc=0.8195, loss=0.3882/acc=0.8011)
  [mlp] layers=[16, 256, 16, 256, 256, 32] act=leaky_relu lr=0.0050 batch=128 epochs=10 -> avg_log_loss=0.3952 (folds: loss=0.3862/acc=0.7960, loss=0.4193/acc=0.7996, loss=0.4418/acc=0.7728, loss=0.3993/acc=0.8029, loss=0.3740/acc=0.8241, los
s=0.3925/acc=0.8122, loss=0.3594/acc=0.8195, loss=0.3893/acc=0.8011)
  [mlp] layers=[16, 32, 16, 128, 256, 32] act=leaky_relu lr=0.0048 batch=128 epochs=25 -> avg_log_loss=0.3907 (folds: loss=0.3763/acc=0.8079, loss=0.4058/acc=0.8070, loss=0.4457/acc=0.7700, loss=0.3915/acc=0.8029, loss=0.3817/acc=0.8149, loss
=0.3830/acc=0.8177, loss=0.3529/acc=0.8398, loss=0.3886/acc=0.8112)
  [mlp] layers=[16, 32, 128, 32, 256, 32] act=leaky_relu lr=0.0050 batch=128 epochs=30 -> avg_log_loss=0.3870 (folds: loss=0.3774/acc=0.8079, loss=0.4020/acc=0.8061, loss=0.4382/acc=0.7774, loss=0.3908/acc=0.7974, loss=0.3759/acc=0.8204, loss
=0.3858/acc=0.8177, loss=0.3450/acc=0.8370, loss=0.3812/acc=0.8112)
  [rf] rf trees=100 depth=8 max_feat=0.90 sample_ratio=0.60 -> avg_log_loss=0.4327 (folds: loss=0.4334/acc=0.7904, loss=0.4467/acc=0.7831, loss=0.4823/acc=0.7672, loss=0.4310/acc=0.7956, loss=0.4141/acc=0.8112, loss=0.4167/acc=0.8011, loss=0.
4098/acc=0.8122, loss=0.4272/acc=0.8076)
  [mlp] layers=[16, 32, 16, 16, 16, 32] act=leaky_relu lr=0.0037 batch=256 epochs=20 -> avg_log_loss=0.3909 (folds: loss=0.3789/acc=0.8033, loss=0.4081/acc=0.8107, loss=0.4464/acc=0.7783, loss=0.3933/acc=0.8094, loss=0.3819/acc=0.8158, loss=0
.3851/acc=0.8122, loss=0.3494/acc=0.8425, loss=0.3843/acc=0.8149)
  [mlp] layers=[16, 256, 16, 16, 128, 32] act=leaky_relu lr=0.0050 batch=256 epochs=15 -> avg_log_loss=0.3947 (folds: loss=0.3821/acc=0.8061, loss=0.4115/acc=0.7941, loss=0.4384/acc=0.7764, loss=0.3952/acc=0.8020, loss=0.3840/acc=0.8204, loss
=0.3987/acc=0.8140, loss=0.3557/acc=0.8306, loss=0.3923/acc=0.8131)
  [mlp] layers=[16, 32, 16, 128, 256, 32] act=leaky_relu lr=0.0041 batch=128 epochs=15 -> avg_log_loss=0.3955 (folds: loss=0.3842/acc=0.7950, loss=0.4081/acc=0.8079, loss=0.4425/acc=0.7774, loss=0.3939/acc=0.7974, loss=0.3759/acc=0.8186, loss
=0.4027/acc=0.8112, loss=0.3637/acc=0.8333, loss=0.3927/acc=0.7993)
  [mlp] layers=[16, 32, 64, 16, 32, 32] act=leaky_relu lr=0.0041 batch=64 epochs=20 -> avg_log_loss=0.3933 (folds: loss=0.3808/acc=0.8024, loss=0.4105/acc=0.7941, loss=0.4502/acc=0.7764, loss=0.3928/acc=0.7993, loss=0.3714/acc=0.8140, loss=0.
3851/acc=0.8140, loss=0.3632/acc=0.8232, loss=0.3920/acc=0.8103)
  [mlp] layers=[16, 256, 32, 16, 32, 64] act=leaky_relu lr=0.0044 batch=64 epochs=25 -> avg_log_loss=0.3903 (folds: loss=0.3857/acc=0.8097, loss=0.4062/acc=0.8006, loss=0.4440/acc=0.7774, loss=0.3900/acc=0.8103, loss=0.3791/acc=0.8186, loss=0
.3763/acc=0.8186, loss=0.3539/acc=0.8306, loss=0.3871/acc=0.8112)
  [rf] rf trees=100 depth=5 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4199 (folds: loss=0.4129/acc=0.7932, loss=0.4377/acc=0.7665, loss=0.4685/acc=0.7682, loss=0.4206/acc=0.7910, loss=0.3973/acc=0.8112, loss=0.4116/acc=0.8011, loss=0.
3942/acc=0.8131, loss=0.4166/acc=0.8140)
  [mlp] layers=[16, 32, 16, 128, 256, 32] act=leaky_relu lr=0.0042 batch=64 epochs=20 -> avg_log_loss=0.3928 (folds: loss=0.3829/acc=0.8061, loss=0.4074/acc=0.7923, loss=0.4346/acc=0.7856, loss=0.3991/acc=0.8076, loss=0.3869/acc=0.8186, loss=
0.3840/acc=0.8131, loss=0.3556/acc=0.8260, loss=0.3919/acc=0.8103)
  [mlp] layers=[16, 64, 32, 16, 32, 32] act=leaky_relu lr=0.0050 batch=512 epochs=10 -> avg_log_loss=0.3971 (folds: loss=0.3873/acc=0.7978, loss=0.4151/acc=0.8061, loss=0.4454/acc=0.7829, loss=0.3930/acc=0.8029, loss=0.3831/acc=0.8168, loss=0
.3921/acc=0.8112, loss=0.3607/acc=0.8306, loss=0.4001/acc=0.8103)
  [extratrees] extratrees trees=300 depth=7 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.3963 (folds: loss=0.3867/acc=0.7969, loss=0.4166/acc=0.7996, loss=0.4390/acc=0.7810, loss=0.3988/acc=0.8112, loss=0.3783/acc=0.8112, loss=0.3973/acc
=0.8149, loss=0.3656/acc=0.8343, loss=0.3880/acc=0.8204)
  [mlp] layers=[64, 32, 16, 16, 32, 32] act=leaky_relu lr=0.0045 batch=128 epochs=15 -> avg_log_loss=0.3987 (folds: loss=0.3785/acc=0.8061, loss=0.4200/acc=0.8024, loss=0.4509/acc=0.7718, loss=0.3984/acc=0.8002, loss=0.3899/acc=0.8149, loss=0
.3970/acc=0.8094, loss=0.3571/acc=0.8094, loss=0.3980/acc=0.8057)
  [rf] rf trees=300 depth=8 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4313 (folds: loss=0.4286/acc=0.8015, loss=0.4470/acc=0.7849, loss=0.4880/acc=0.7645, loss=0.4290/acc=0.7993, loss=0.4109/acc=0.8094, loss=0.4180/acc=0.8020, loss=0.
4014/acc=0.8122, loss=0.4274/acc=0.8066)
  best avg log loss this gen: 0.3870
Generation 14/20
  [mlp] layers=[16, 32, 128, 32, 256, 32] act=leaky_relu lr=0.0050 batch=128 epochs=30 -> avg_log_loss=0.3928 (folds: loss=0.3813/acc=0.8051, loss=0.4075/acc=0.8088, loss=0.4491/acc=0.7755, loss=0.3860/acc=0.7993, loss=0.3844/acc=0.8195, loss
=0.3870/acc=0.8094, loss=0.3507/acc=0.8407, loss=0.3969/acc=0.8020)
  [mlp] layers=[16, 32, 16, 32, 256, 32] act=leaky_relu lr=0.0043 batch=128 epochs=30 -> avg_log_loss=0.3895 (folds: loss=0.3783/acc=0.8070, loss=0.4034/acc=0.8024, loss=0.4460/acc=0.7755, loss=0.3940/acc=0.8085, loss=0.3738/acc=0.8195, loss=
0.3817/acc=0.8223, loss=0.3508/acc=0.8352, loss=0.3880/acc=0.8029)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.3953 (folds: loss=0.3835/acc=0.7978, loss=0.4174/acc=0.7996, loss=0.4388/acc=0.7783, loss=0.3981/acc=0.8066, loss=0.3779/acc=0.8149, loss=0.3943/acc
=0.8131, loss=0.3651/acc=0.8306, loss=0.3874/acc=0.8186)
  [mlp] layers=[64, 256, 16, 256, 128, 32] act=leaky_relu lr=0.0050 batch=512 epochs=20 -> avg_log_loss=0.4038 (folds: loss=0.3935/acc=0.8033, loss=0.4218/acc=0.7996, loss=0.4688/acc=0.7654, loss=0.3999/acc=0.7993, loss=0.3861/acc=0.8186, los
s=0.3918/acc=0.8094, loss=0.3599/acc=0.8306, loss=0.4083/acc=0.8002)
  [mlp] layers=[64, 256, 32, 16, 32, 64] act=leaky_relu lr=0.0042 batch=512 epochs=15 -> avg_log_loss=0.4019 (folds: loss=0.3879/acc=0.7969, loss=0.4279/acc=0.8006, loss=0.4687/acc=0.7718, loss=0.3958/acc=0.7993, loss=0.3854/acc=0.8232, loss=
0.3949/acc=0.8057, loss=0.3591/acc=0.8269, loss=0.3952/acc=0.8057)
  [mlp] layers=[128, 256, 16, 256, 128, 32] act=leaky_relu lr=0.0050 batch=64 epochs=30 -> avg_log_loss=0.4210 (folds: loss=0.3987/acc=0.8070, loss=0.4307/acc=0.8033, loss=0.5084/acc=0.7626, loss=0.4422/acc=0.7928, loss=0.3963/acc=0.8177, los
s=0.3903/acc=0.8149, loss=0.3786/acc=0.8306, loss=0.4225/acc=0.8029)
  [mlp] layers=[16, 512, 16, 32, 256, 32] act=leaky_relu lr=0.0039 batch=512 epochs=10 -> avg_log_loss=0.3963 (folds: loss=0.3810/acc=0.8006, loss=0.4137/acc=0.7969, loss=0.4369/acc=0.7783, loss=0.3998/acc=0.8002, loss=0.3866/acc=0.8177, loss
=0.3908/acc=0.8103, loss=0.3643/acc=0.8241, loss=0.3976/acc=0.8002)
  [histgb] histgb trees=200 depth=8 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4255 (folds: loss=0.4166/acc=0.8006, loss=0.4358/acc=0.7721, loss=0.4645/acc=0.7580, loss=0.4276/acc=0.7965, loss=0.4147/acc=0.8057, loss=0.4216/acc=0.7974,
 loss=0.3995/acc=0.8158, loss=0.4240/acc=0.8057)
  [mlp] layers=[16, 256, 16, 16, 128, 32] act=leaky_relu lr=0.0050 batch=128 epochs=30 -> avg_log_loss=0.3902 (folds: loss=0.3745/acc=0.8070, loss=0.4030/acc=0.8042, loss=0.4354/acc=0.7783, loss=0.3982/acc=0.8066, loss=0.3801/acc=0.8149, loss
=0.3842/acc=0.8149, loss=0.3533/acc=0.8379, loss=0.3933/acc=0.8085)
  [extratrees] extratrees trees=300 depth=5 max_feat=0.50 sample_ratio=1.00 -> avg_log_loss=0.4046 (folds: loss=0.3960/acc=0.7978, loss=0.4243/acc=0.7978, loss=0.4444/acc=0.7902, loss=0.4056/acc=0.8057, loss=0.3875/acc=0.8195, loss=0.4088/acc
=0.8094, loss=0.3699/acc=0.8315, loss=0.4006/acc=0.8131)
  [mlp] layers=[16, 256, 16, 512, 128, 32] act=leaky_relu lr=0.0050 batch=512 epochs=20 -> avg_log_loss=0.3918 (folds: loss=0.3804/acc=0.7996, loss=0.4023/acc=0.8070, loss=0.4396/acc=0.7884, loss=0.3932/acc=0.8039, loss=0.3831/acc=0.8149, los
s=0.3896/acc=0.8158, loss=0.3588/acc=0.8177, loss=0.3877/acc=0.8131)
  [mlp] layers=[16, 256, 32, 16, 32, 64] act=leaky_relu lr=0.0047 batch=128 epochs=20 -> avg_log_loss=0.3916 (folds: loss=0.3773/acc=0.7932, loss=0.4044/acc=0.7904, loss=0.4453/acc=0.7682, loss=0.3947/acc=0.8020, loss=0.3782/acc=0.8094, loss=
0.3879/acc=0.8149, loss=0.3512/acc=0.8398, loss=0.3935/acc=0.8057)
  [mlp] layers=[16, 16, 16, 32, 256, 32] act=leaky_relu lr=0.0050 batch=512 epochs=30 -> avg_log_loss=0.3897 (folds: loss=0.3816/acc=0.7996, loss=0.3998/acc=0.8033, loss=0.4402/acc=0.7792, loss=0.3876/acc=0.8029, loss=0.3769/acc=0.8250, loss=
0.3823/acc=0.8149, loss=0.3575/acc=0.8343, loss=0.3916/acc=0.7974)
  [gbstump] gbstump estimators=150 lr=0.2 min_leaf=50 -> avg_log_loss=0.4452 (folds: loss=0.4337/acc=0.7822, loss=0.4509/acc=0.7767, loss=0.4848/acc=0.7728, loss=0.4504/acc=0.7845, loss=0.4249/acc=0.8140, loss=0.4438/acc=0.7901, loss=0.4187/a
cc=0.8232, loss=0.4547/acc=0.7864)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=100 -> avg_log_loss=0.4414 (folds: loss=0.4332/acc=0.7831, loss=0.4481/acc=0.7895, loss=0.4780/acc=0.7774, loss=0.4439/acc=0.7947, loss=0.4244/acc=0.8085, loss=0.4421/acc=0.7947, loss=0.4160/
acc=0.8232, loss=0.4455/acc=0.8020)
  [mlp] layers=[16, 256, 32, 16, 64, 64] act=leaky_relu lr=0.0050 batch=128 epochs=30 -> avg_log_loss=0.3892 (folds: loss=0.3743/acc=0.8061, loss=0.4053/acc=0.8033, loss=0.4420/acc=0.7820, loss=0.3915/acc=0.8020, loss=0.3845/acc=0.8204, loss=
0.3829/acc=0.8140, loss=0.3499/acc=0.8389, loss=0.3832/acc=0.8186)
  [mlp] layers=[16, 16, 128, 32, 256, 32] act=leaky_relu lr=0.0050 batch=256 epochs=15 -> avg_log_loss=0.3911 (folds: loss=0.3869/acc=0.7914, loss=0.4046/acc=0.8015, loss=0.4371/acc=0.7810, loss=0.3886/acc=0.8039, loss=0.3841/acc=0.8131, loss
=0.3858/acc=0.8103, loss=0.3531/acc=0.8333, loss=0.3887/acc=0.8149)
  [mlp] layers=[16, 32, 128, 32, 32, 32] act=leaky_relu lr=0.0050 batch=128 epochs=15 -> avg_log_loss=0.3937 (folds: loss=0.3777/acc=0.8051, loss=0.4064/acc=0.8070, loss=0.4415/acc=0.7691, loss=0.3937/acc=0.8039, loss=0.3856/acc=0.8149, loss=
0.3941/acc=0.8131, loss=0.3577/acc=0.8389, loss=0.3932/acc=0.8048)
  best avg log loss this gen: 0.3892
Generation 15/20
  [mlp] layers=[16, 256, 32, 16, 64, 64] act=leaky_relu lr=0.0050 batch=128 epochs=30 -> avg_log_loss=0.3899 (folds: loss=0.3721/acc=0.8088, loss=0.4041/acc=0.8097, loss=0.4423/acc=0.7746, loss=0.3897/acc=0.8057, loss=0.3818/acc=0.8158, loss=
0.3824/acc=0.8131, loss=0.3527/acc=0.8453, loss=0.3941/acc=0.8140)
  [mlp] layers=[16, 32, 16, 32, 256, 32] act=leaky_relu lr=0.0043 batch=128 epochs=30 -> avg_log_loss=0.3898 (folds: loss=0.3819/acc=0.8033, loss=0.3992/acc=0.8070, loss=0.4456/acc=0.7764, loss=0.3901/acc=0.8066, loss=0.3783/acc=0.8195, loss=
0.3809/acc=0.8140, loss=0.3518/acc=0.8416, loss=0.3908/acc=0.8029)
  [rf] rf trees=400 depth=6 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4304 (folds: loss=0.4297/acc=0.7960, loss=0.4478/acc=0.7739, loss=0.4790/acc=0.7709, loss=0.4272/acc=0.7983, loss=0.4118/acc=0.8076, loss=0.4168/acc=0.7993, loss=0.
4034/acc=0.8204, loss=0.4279/acc=0.8131)
  [mlp] layers=[16, 256, 16, 16, 128, 32] act=leaky_relu lr=0.0049 batch=256 epochs=20 -> avg_log_loss=0.3933 (folds: loss=0.3889/acc=0.8051, loss=0.4052/acc=0.8088, loss=0.4379/acc=0.7718, loss=0.3967/acc=0.7993, loss=0.3788/acc=0.8223, loss
=0.3850/acc=0.8112, loss=0.3601/acc=0.8250, loss=0.3937/acc=0.8039)
  [mlp] layers=[16, 32, 256, 32, 256, 32] act=leaky_relu lr=0.0040 batch=128 epochs=20 -> avg_log_loss=0.3931 (folds: loss=0.3866/acc=0.7950, loss=0.4110/acc=0.7996, loss=0.4453/acc=0.7663, loss=0.3944/acc=0.8002, loss=0.3804/acc=0.8066, loss
=0.3901/acc=0.8122, loss=0.3510/acc=0.8352, loss=0.3861/acc=0.8122)
  [gbstump] gbstump estimators=150 lr=0.3 min_leaf=50 -> avg_log_loss=0.4470 (folds: loss=0.4368/acc=0.7868, loss=0.4507/acc=0.7803, loss=0.4844/acc=0.7682, loss=0.4563/acc=0.7855, loss=0.4234/acc=0.8122, loss=0.4462/acc=0.7919, loss=0.4214/a
cc=0.8158, loss=0.4567/acc=0.7928)
  [mlp] layers=[16, 32, 16, 128, 256, 32] act=leaky_relu lr=0.0036 batch=128 epochs=30 -> avg_log_loss=0.3894 (folds: loss=0.3714/acc=0.8051, loss=0.4013/acc=0.8033, loss=0.4438/acc=0.7764, loss=0.3899/acc=0.8057, loss=0.3853/acc=0.8204, loss
=0.3832/acc=0.8076, loss=0.3569/acc=0.8297, loss=0.3837/acc=0.8085)
  [mlp] layers=[16, 256, 16, 64, 128, 32] act=leaky_relu lr=0.0050 batch=512 epochs=15 -> avg_log_loss=0.3918 (folds: loss=0.3788/acc=0.7960, loss=0.4021/acc=0.8033, loss=0.4360/acc=0.7783, loss=0.3997/acc=0.8057, loss=0.3767/acc=0.8177, loss
=0.3843/acc=0.8168, loss=0.3606/acc=0.8389, loss=0.3962/acc=0.8011)
  [mlp] layers=[16, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0041 batch=128 epochs=30 -> avg_log_loss=0.3894 (folds: loss=0.3752/acc=0.8042, loss=0.4045/acc=0.8079, loss=0.4386/acc=0.7718, loss=0.3917/acc=0.8039, loss=0.3825/acc=0.8195, loss=
0.3789/acc=0.8232, loss=0.3557/acc=0.8260, loss=0.3886/acc=0.8085)
  [rf] rf trees=400 depth=6 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4244 (folds: loss=0.4197/acc=0.7987, loss=0.4415/acc=0.7767, loss=0.4744/acc=0.7709, loss=0.4235/acc=0.7983, loss=0.4017/acc=0.8066, loss=0.4138/acc=0.7983, loss=0.
3999/acc=0.8177, loss=0.4210/acc=0.8076)
  [mlp] layers=[16, 16, 32, 32, 256, 32] act=leaky_relu lr=0.0041 batch=64 epochs=10 -> avg_log_loss=0.3977 (folds: loss=0.3843/acc=0.7969, loss=0.4178/acc=0.7978, loss=0.4388/acc=0.7764, loss=0.3959/acc=0.7965, loss=0.3907/acc=0.8214, loss=0
.3928/acc=0.8122, loss=0.3687/acc=0.8306, loss=0.3927/acc=0.8048)
  [mlp] layers=[16, 256, 32, 16, 128, 64] act=leaky_relu lr=0.0050 batch=512 epochs=10 -> avg_log_loss=0.3970 (folds: loss=0.3784/acc=0.7987, loss=0.4148/acc=0.7996, loss=0.4404/acc=0.7682, loss=0.3955/acc=0.7993, loss=0.3937/acc=0.8039, loss
=0.3995/acc=0.8085, loss=0.3578/acc=0.8241, loss=0.3960/acc=0.8029)
  [rf] rf trees=400 depth=8 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4287 (folds: loss=0.4288/acc=0.7978, loss=0.4464/acc=0.7785, loss=0.4787/acc=0.7645, loss=0.4265/acc=0.8020, loss=0.4085/acc=0.8039, loss=0.4149/acc=0.8020, loss=0.
4019/acc=0.8103, loss=0.4242/acc=0.8057)
  [mlp] layers=[16, 16, 512, 32, 256, 32] act=leaky_relu lr=0.0042 batch=256 epochs=30 -> avg_log_loss=0.3911 (folds: loss=0.3816/acc=0.8015, loss=0.4064/acc=0.8097, loss=0.4353/acc=0.7856, loss=0.3931/acc=0.8020, loss=0.3799/acc=0.8195, loss
=0.3901/acc=0.8131, loss=0.3521/acc=0.8398, loss=0.3900/acc=0.8122)
  [rf] rf trees=400 depth=7 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4299 (folds: loss=0.4267/acc=0.7987, loss=0.4437/acc=0.7776, loss=0.4816/acc=0.7672, loss=0.4284/acc=0.7983, loss=0.4149/acc=0.8048, loss=0.4152/acc=0.8029, loss=0.
4025/acc=0.8131, loss=0.4261/acc=0.8066)
  [mlp] layers=[16, 16, 16, 32, 128, 32] act=leaky_relu lr=0.0049 batch=64 epochs=25 -> avg_log_loss=0.3882 (folds: loss=0.3727/acc=0.7978, loss=0.4076/acc=0.7960, loss=0.4359/acc=0.7829, loss=0.3922/acc=0.8011, loss=0.3771/acc=0.8149, loss=0
.3832/acc=0.8223, loss=0.3501/acc=0.8389, loss=0.3871/acc=0.8103)
  [mlp] layers=[128, 32, 16, 32, 256, 32] act=leaky_relu lr=0.0037 batch=256 epochs=25 -> avg_log_loss=0.4000 (folds: loss=0.3855/acc=0.8024, loss=0.4204/acc=0.8006, loss=0.4649/acc=0.7774, loss=0.3974/acc=0.8103, loss=0.3918/acc=0.8149, loss
=0.3818/acc=0.8177, loss=0.3612/acc=0.8379, loss=0.3970/acc=0.8149)
  [mlp] layers=[16, 256, 128, 16, 64, 64] act=leaky_relu lr=0.0044 batch=512 epochs=25 -> avg_log_loss=0.3916 (folds: loss=0.3788/acc=0.8042, loss=0.4090/acc=0.8088, loss=0.4430/acc=0.7700, loss=0.3863/acc=0.8103, loss=0.3793/acc=0.8149, loss
=0.3877/acc=0.8112, loss=0.3554/acc=0.8379, loss=0.3931/acc=0.8039)
  best avg log loss this gen: 0.3882
Generation 16/20
  [mlp] layers=[16, 16, 16, 32, 128, 32] act=leaky_relu lr=0.0049 batch=64 epochs=25 -> avg_log_loss=0.3899 (folds: loss=0.3747/acc=0.8051, loss=0.4064/acc=0.8024, loss=0.4346/acc=0.7764, loss=0.3965/acc=0.8094, loss=0.3812/acc=0.8195, loss=0
.3895/acc=0.8103, loss=0.3525/acc=0.8370, loss=0.3841/acc=0.8112)
  [mlp] layers=[16, 32, 16, 128, 256, 32] act=leaky_relu lr=0.0036 batch=128 epochs=30 -> avg_log_loss=0.3893 (folds: loss=0.3767/acc=0.8061, loss=0.4009/acc=0.8143, loss=0.4410/acc=0.7774, loss=0.3903/acc=0.8103, loss=0.3807/acc=0.8149, loss
=0.3846/acc=0.8085, loss=0.3541/acc=0.8250, loss=0.3862/acc=0.8122)
  [mlp] layers=[16, 16, 16, 32, 128, 32] act=leaky_relu lr=0.0045 batch=128 epochs=15 -> avg_log_loss=0.3972 (folds: loss=0.3912/acc=0.7941, loss=0.4125/acc=0.8006, loss=0.4387/acc=0.7691, loss=0.3981/acc=0.8029, loss=0.3817/acc=0.8204, loss=
0.3977/acc=0.8140, loss=0.3649/acc=0.8168, loss=0.3930/acc=0.8112)
  [rf] rf trees=300 depth=6 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.4179 (folds: loss=0.4124/acc=0.8015, loss=0.4359/acc=0.7748, loss=0.4669/acc=0.7728, loss=0.4171/acc=0.7993, loss=0.3940/acc=0.8103, loss=0.4081/acc=0.8066, loss=0.
3949/acc=0.8168, loss=0.4138/acc=0.8039)
  [mlp] layers=[16, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0040 batch=128 epochs=25 -> avg_log_loss=0.3887 (folds: loss=0.3802/acc=0.8015, loss=0.4033/acc=0.8015, loss=0.4272/acc=0.7847, loss=0.3879/acc=0.8076, loss=0.3820/acc=0.8168, loss=
0.3844/acc=0.8168, loss=0.3527/acc=0.8379, loss=0.3918/acc=0.8002)
  [mlp] layers=[32, 512, 32, 64, 512, 16] act=relu lr=0.0040 batch=128 epochs=30 -> avg_log_loss=0.3926 (folds: loss=0.3842/acc=0.8079, loss=0.4137/acc=0.7914, loss=0.4406/acc=0.7866, loss=0.3885/acc=0.8149, loss=0.3801/acc=0.8195, loss=0.381
4/acc=0.8149, loss=0.3557/acc=0.8343, loss=0.3967/acc=0.8039)
  [mlp] layers=[16, 32, 32, 32, 256, 32] act=leaky_relu lr=0.0041 batch=64 epochs=25 -> avg_log_loss=0.3890 (folds: loss=0.3763/acc=0.7978, loss=0.4015/acc=0.8006, loss=0.4351/acc=0.7801, loss=0.3958/acc=0.8057, loss=0.3772/acc=0.8177, loss=0
.3827/acc=0.8186, loss=0.3504/acc=0.8352, loss=0.3928/acc=0.8131)
  [mlp] layers=[16, 64, 16, 32, 512, 32] act=leaky_relu lr=0.0046 batch=128 epochs=20 -> avg_log_loss=0.3902 (folds: loss=0.3851/acc=0.7987, loss=0.4103/acc=0.7932, loss=0.4398/acc=0.7709, loss=0.3928/acc=0.8020, loss=0.3783/acc=0.8195, loss=
0.3821/acc=0.8250, loss=0.3515/acc=0.8444, loss=0.3818/acc=0.8103)
  [mlp] layers=[16, 32, 16, 32, 256, 16] act=leaky_relu lr=0.0045 batch=256 epochs=25 -> avg_log_loss=0.3884 (folds: loss=0.3729/acc=0.7960, loss=0.4065/acc=0.8088, loss=0.4371/acc=0.7847, loss=0.3892/acc=0.8066, loss=0.3847/acc=0.8214, loss=
0.3838/acc=0.8214, loss=0.3457/acc=0.8333, loss=0.3873/acc=0.8039)
  [mlp] layers=[16, 32, 16, 32, 64, 32] act=leaky_relu lr=0.0049 batch=64 epochs=30 -> avg_log_loss=0.3895 (folds: loss=0.3797/acc=0.8051, loss=0.4068/acc=0.8015, loss=0.4345/acc=0.7792, loss=0.3952/acc=0.8029, loss=0.3730/acc=0.8158, loss=0.
3865/acc=0.8131, loss=0.3540/acc=0.8260, loss=0.3862/acc=0.8011)
  [mlp] layers=[16, 512, 16, 32, 256, 32] act=leaky_relu lr=0.0049 batch=64 epochs=20 -> avg_log_loss=0.3921 (folds: loss=0.3779/acc=0.7969, loss=0.4087/acc=0.8079, loss=0.4383/acc=0.7728, loss=0.3947/acc=0.7965, loss=0.3799/acc=0.8103, loss=
0.3899/acc=0.8094, loss=0.3539/acc=0.8389, loss=0.3938/acc=0.8020)
  [mlp] layers=[16, 64, 16, 128, 256, 32] act=leaky_relu lr=0.0030 batch=512 epochs=25 -> avg_log_loss=0.3926 (folds: loss=0.3821/acc=0.8070, loss=0.4093/acc=0.8006, loss=0.4372/acc=0.7728, loss=0.3948/acc=0.8002, loss=0.3807/acc=0.8158, loss
=0.3879/acc=0.8131, loss=0.3561/acc=0.8278, loss=0.3929/acc=0.8066)
  [mlp] layers=[16, 512, 16, 32, 512, 32] act=leaky_relu lr=0.0036 batch=256 epochs=20 -> avg_log_loss=0.3933 (folds: loss=0.3809/acc=0.7996, loss=0.4039/acc=0.8070, loss=0.4412/acc=0.7709, loss=0.3949/acc=0.8048, loss=0.3824/acc=0.8223, loss
=0.3904/acc=0.8131, loss=0.3605/acc=0.8085, loss=0.3926/acc=0.8057)
  [mlp] layers=[128, 16, 64, 16, 256] act=leaky_relu lr=0.0047 batch=128 epochs=15 -> avg_log_loss=0.3912 (folds: loss=0.3758/acc=0.8006, loss=0.4076/acc=0.8033, loss=0.4403/acc=0.7709, loss=0.3893/acc=0.8177, loss=0.3883/acc=0.8103, loss=0.3
774/acc=0.8168, loss=0.3512/acc=0.8370, loss=0.3998/acc=0.8020)
  [mlp] layers=[128, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0033 batch=128 epochs=20 -> avg_log_loss=0.3943 (folds: loss=0.3805/acc=0.8015, loss=0.4183/acc=0.8079, loss=0.4527/acc=0.7746, loss=0.3935/acc=0.8168, loss=0.3774/acc=0.8214, loss
=0.3821/acc=0.8131, loss=0.3540/acc=0.8324, loss=0.3958/acc=0.8029)
  [mlp] layers=[16, 32, 16, 32, 512, 128] act=leaky_relu lr=0.0048 batch=256 epochs=30 -> avg_log_loss=0.3896 (folds: loss=0.3746/acc=0.8042, loss=0.4060/acc=0.8079, loss=0.4406/acc=0.7755, loss=0.3983/acc=0.8066, loss=0.3771/acc=0.8158, loss
=0.3826/acc=0.8158, loss=0.3546/acc=0.8370, loss=0.3831/acc=0.8177)
  [mlp] layers=[64, 64, 16, 32, 128, 32] act=relu lr=0.0040 batch=256 epochs=15 -> avg_log_loss=0.3936 (folds: loss=0.3832/acc=0.8070, loss=0.4068/acc=0.8079, loss=0.4372/acc=0.7783, loss=0.3924/acc=0.8094, loss=0.3800/acc=0.8131, loss=0.3900
/acc=0.8066, loss=0.3550/acc=0.8324, loss=0.4044/acc=0.7947)
  [mlp] layers=[16, 32, 16, 32, 512, 64] act=leaky_relu lr=0.0045 batch=64 epochs=25 -> avg_log_loss=0.3927 (folds: loss=0.3831/acc=0.7932, loss=0.4095/acc=0.8051, loss=0.4382/acc=0.7774, loss=0.3945/acc=0.8002, loss=0.3774/acc=0.8140, loss=0
.3863/acc=0.8112, loss=0.3597/acc=0.8379, loss=0.3927/acc=0.8076)
  best avg log loss this gen: 0.3884
Generation 17/20
  [mlp] layers=[16, 32, 16, 32, 256, 16] act=leaky_relu lr=0.0045 batch=256 epochs=25 -> avg_log_loss=0.3913 (folds: loss=0.3808/acc=0.8079, loss=0.4050/acc=0.8143, loss=0.4452/acc=0.7764, loss=0.3879/acc=0.8002, loss=0.3824/acc=0.8140, loss=
0.3809/acc=0.8149, loss=0.3602/acc=0.8214, loss=0.3877/acc=0.8039)
  [mlp] layers=[16, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0040 batch=128 epochs=25 -> avg_log_loss=0.3899 (folds: loss=0.3795/acc=0.8024, loss=0.4115/acc=0.8079, loss=0.4382/acc=0.7700, loss=0.3949/acc=0.7993, loss=0.3791/acc=0.8204, loss=
0.3779/acc=0.8214, loss=0.3548/acc=0.8315, loss=0.3835/acc=0.8076)
  [extratrees] extratrees trees=200 depth=5 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4036 (folds: loss=0.3936/acc=0.7923, loss=0.4242/acc=0.8006, loss=0.4457/acc=0.7921, loss=0.4065/acc=0.8076, loss=0.3841/acc=0.8204, loss=0.4057/acc
=0.8103, loss=0.3719/acc=0.8250, loss=0.3975/acc=0.8204)
  [histgb] histgb trees=300 depth=7 max_feat=0.90 sample_ratio=1.00 -> avg_log_loss=0.4234 (folds: loss=0.4143/acc=0.7987, loss=0.4330/acc=0.7748, loss=0.4636/acc=0.7590, loss=0.4247/acc=0.7993, loss=0.4129/acc=0.8048, loss=0.4188/acc=0.8002,
 loss=0.3969/acc=0.8140, loss=0.4227/acc=0.8066)
  [mlp] layers=[16, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0041 batch=128 epochs=20 -> avg_log_loss=0.3901 (folds: loss=0.3806/acc=0.8015, loss=0.4032/acc=0.8061, loss=0.4420/acc=0.7764, loss=0.3917/acc=0.8029, loss=0.3879/acc=0.8168, loss=
0.3787/acc=0.8158, loss=0.3474/acc=0.8407, loss=0.3890/acc=0.7956)
  [rf] rf trees=200 depth=4 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4246 (folds: loss=0.4148/acc=0.7978, loss=0.4413/acc=0.7730, loss=0.4756/acc=0.7737, loss=0.4230/acc=0.7956, loss=0.4021/acc=0.8094, loss=0.4173/acc=0.7974, loss=0.
3978/acc=0.8149, loss=0.4247/acc=0.8066)
  [mlp] layers=[16, 32, 16, 128, 256, 32] act=leaky_relu lr=0.0034 batch=64 epochs=15 -> avg_log_loss=0.3931 (folds: loss=0.3790/acc=0.8033, loss=0.4050/acc=0.8006, loss=0.4386/acc=0.7718, loss=0.3972/acc=0.8020, loss=0.3803/acc=0.8158, loss=
0.3892/acc=0.8085, loss=0.3615/acc=0.8324, loss=0.3944/acc=0.8076)
  [mlp] layers=[16, 32, 16, 256, 256, 16] act=leaky_relu lr=0.0041 batch=256 epochs=10 -> avg_log_loss=0.3955 (folds: loss=0.3898/acc=0.7941, loss=0.4157/acc=0.7895, loss=0.4355/acc=0.7783, loss=0.3972/acc=0.7947, loss=0.3838/acc=0.8204, loss
=0.3869/acc=0.8214, loss=0.3623/acc=0.8306, loss=0.3928/acc=0.8002)
  [mlp] layers=[16, 32, 16, 128, 32, 32] act=leaky_relu lr=0.0039 batch=512 epochs=25 -> avg_log_loss=0.3915 (folds: loss=0.3767/acc=0.7960, loss=0.4121/acc=0.8033, loss=0.4426/acc=0.7737, loss=0.3886/acc=0.8149, loss=0.3828/acc=0.8168, loss=
0.3826/acc=0.8140, loss=0.3564/acc=0.8287, loss=0.3904/acc=0.8048)
  [extratrees] extratrees trees=100 depth=7 max_feat=0.50 sample_ratio=0.80 -> avg_log_loss=0.3979 (folds: loss=0.3877/acc=0.7978, loss=0.4190/acc=0.7960, loss=0.4418/acc=0.7783, loss=0.3981/acc=0.8122, loss=0.3800/acc=0.8158, loss=0.3992/acc
=0.8131, loss=0.3671/acc=0.8333, loss=0.3904/acc=0.8168)
  [mlp] layers=[16, 128, 32, 32, 256, 32] act=leaky_relu lr=0.0045 batch=64 epochs=15 -> avg_log_loss=0.3977 (folds: loss=0.3805/acc=0.7978, loss=0.4163/acc=0.8042, loss=0.4466/acc=0.7709, loss=0.3896/acc=0.7983, loss=0.3787/acc=0.8158, loss=
0.4124/acc=0.8131, loss=0.3620/acc=0.8306, loss=0.3955/acc=0.8103)
  [mlp] layers=[64, 32, 16, 128, 256, 32] act=leaky_relu lr=0.0030 batch=128 epochs=20 -> avg_log_loss=0.3932 (folds: loss=0.3782/acc=0.8042, loss=0.4077/acc=0.8015, loss=0.4513/acc=0.7728, loss=0.3949/acc=0.8057, loss=0.3933/acc=0.8122, loss
=0.3756/acc=0.8177, loss=0.3532/acc=0.8287, loss=0.3913/acc=0.8094)
  [mlp] layers=[16, 512, 32, 32, 256, 32] act=leaky_relu lr=0.0041 batch=128 epochs=20 -> avg_log_loss=0.3926 (folds: loss=0.3783/acc=0.8070, loss=0.4121/acc=0.7904, loss=0.4523/acc=0.7774, loss=0.3907/acc=0.7965, loss=0.3813/acc=0.8177, loss
=0.3839/acc=0.8195, loss=0.3472/acc=0.8389, loss=0.3951/acc=0.8112)
  [gbstump] gbstump estimators=300 lr=0.1 min_leaf=200 -> avg_log_loss=0.4415 (folds: loss=0.4310/acc=0.7849, loss=0.4493/acc=0.7812, loss=0.4780/acc=0.7728, loss=0.4452/acc=0.7910, loss=0.4243/acc=0.8066, loss=0.4408/acc=0.7974, loss=0.4184/
acc=0.8223, loss=0.4448/acc=0.8039)
  [mlp] layers=[16, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0045 batch=128 epochs=25 -> avg_log_loss=0.3894 (folds: loss=0.3786/acc=0.8097, loss=0.4062/acc=0.8061, loss=0.4418/acc=0.7755, loss=0.3879/acc=0.8076, loss=0.3759/acc=0.8195, loss=
0.3788/acc=0.8122, loss=0.3564/acc=0.8287, loss=0.3899/acc=0.8057)
  [mlp] layers=[128, 512, 32] act=leaky_relu lr=0.0043 batch=128 epochs=30 -> avg_log_loss=0.4166 (folds: loss=0.4080/acc=0.8024, loss=0.4386/acc=0.8015, loss=0.5020/acc=0.7617, loss=0.4055/acc=0.8039, loss=0.4056/acc=0.8168, loss=0.3906/acc=
0.8186, loss=0.3610/acc=0.8315, loss=0.4214/acc=0.8011)
  [mlp] layers=[512, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0048 batch=256 epochs=20 -> avg_log_loss=0.4063 (folds: loss=0.3939/acc=0.7978, loss=0.4219/acc=0.8024, loss=0.4822/acc=0.7691, loss=0.4140/acc=0.7983, loss=0.4017/acc=0.8149, loss
=0.3794/acc=0.8131, loss=0.3624/acc=0.8232, loss=0.3951/acc=0.8029)
  [mlp] layers=[16, 32, 16, 128, 512, 32] act=leaky_relu lr=0.0033 batch=512 epochs=15 -> avg_log_loss=0.3962 (folds: loss=0.3856/acc=0.7978, loss=0.4138/acc=0.8024, loss=0.4410/acc=0.7866, loss=0.3927/acc=0.8066, loss=0.3864/acc=0.8223, loss
=0.3912/acc=0.8214, loss=0.3621/acc=0.8260, loss=0.3970/acc=0.7993)
  best avg log loss this gen: 0.3894
Generation 18/20
  [mlp] layers=[16, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0045 batch=128 epochs=25 -> avg_log_loss=0.3892 (folds: loss=0.3764/acc=0.8097, loss=0.4010/acc=0.8088, loss=0.4459/acc=0.7755, loss=0.3900/acc=0.8085, loss=0.3784/acc=0.8140, loss=
0.3847/acc=0.8131, loss=0.3501/acc=0.8306, loss=0.3871/acc=0.8057)
  [mlp] layers=[16, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0040 batch=128 epochs=25 -> avg_log_loss=0.3897 (folds: loss=0.3773/acc=0.8134, loss=0.4051/acc=0.8024, loss=0.4432/acc=0.7764, loss=0.3913/acc=0.8029, loss=0.3816/acc=0.8122, loss=
0.3867/acc=0.8122, loss=0.3454/acc=0.8260, loss=0.3871/acc=0.8131)
  [extratrees] extratrees trees=300 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4105 (folds: loss=0.4022/acc=0.7969, loss=0.4298/acc=0.7969, loss=0.4499/acc=0.7893, loss=0.4121/acc=0.7993, loss=0.3905/acc=0.8149, loss=0.4151/acc
=0.8112, loss=0.3781/acc=0.8269, loss=0.4066/acc=0.8076)
  [mlp] layers=[16, 128, 16, 32, 512, 32] act=leaky_relu lr=0.0039 batch=128 epochs=15 -> avg_log_loss=0.3915 (folds: loss=0.3836/acc=0.7950, loss=0.4078/acc=0.7950, loss=0.4373/acc=0.7764, loss=0.3878/acc=0.8020, loss=0.3809/acc=0.8204, loss
=0.3886/acc=0.8158, loss=0.3605/acc=0.8140, loss=0.3856/acc=0.8232)
  [mlp] layers=[16, 32, 16, 32, 512, 16] act=leaky_relu lr=0.0038 batch=512 epochs=25 -> avg_log_loss=0.3945 (folds: loss=0.3827/acc=0.8070, loss=0.4100/acc=0.8042, loss=0.4429/acc=0.7709, loss=0.3952/acc=0.8103, loss=0.3903/acc=0.8112, loss=
0.3854/acc=0.8140, loss=0.3564/acc=0.8306, loss=0.3928/acc=0.8048)
  [mlp] layers=[16, 32, 16, 32, 256, 16] act=leaky_relu lr=0.0048 batch=256 epochs=20 -> avg_log_loss=0.3905 (folds: loss=0.3801/acc=0.8033, loss=0.4095/acc=0.7978, loss=0.4385/acc=0.7783, loss=0.3884/acc=0.8131, loss=0.3780/acc=0.8158, loss=
0.3860/acc=0.8260, loss=0.3513/acc=0.8416, loss=0.3925/acc=0.8057)
  [mlp] layers=[16, 512, 16, 32, 512, 32] act=leaky_relu lr=0.0043 batch=128 epochs=25 -> avg_log_loss=0.3930 (folds: loss=0.3808/acc=0.8033, loss=0.4121/acc=0.7950, loss=0.4436/acc=0.7847, loss=0.3957/acc=0.8011, loss=0.3811/acc=0.8140, loss
=0.3805/acc=0.8158, loss=0.3541/acc=0.8324, loss=0.3963/acc=0.8103)
  [mlp] layers=[16, 32, 128, 32, 256, 16] act=leaky_relu lr=0.0040 batch=256 epochs=30 -> avg_log_loss=0.3915 (folds: loss=0.3758/acc=0.8015, loss=0.4104/acc=0.8061, loss=0.4450/acc=0.7783, loss=0.3902/acc=0.8177, loss=0.3785/acc=0.8149, loss
=0.3805/acc=0.8223, loss=0.3600/acc=0.8204, loss=0.3921/acc=0.8029)
  [mlp] layers=[16, 32, 16, 32, 32, 32] act=leaky_relu lr=0.0045 batch=128 epochs=10 -> avg_log_loss=0.3991 (folds: loss=0.3890/acc=0.7941, loss=0.4107/acc=0.8015, loss=0.4384/acc=0.7737, loss=0.4105/acc=0.7937, loss=0.3882/acc=0.8158, loss=0
.4026/acc=0.8085, loss=0.3588/acc=0.8315, loss=0.3944/acc=0.8002)
  [rf] rf trees=300 depth=5 max_feat=0.70 sample_ratio=0.60 -> avg_log_loss=0.4254 (folds: loss=0.4170/acc=0.7960, loss=0.4447/acc=0.7739, loss=0.4758/acc=0.7645, loss=0.4235/acc=0.8002, loss=0.4042/acc=0.8094, loss=0.4155/acc=0.8011, loss=0.
3988/acc=0.8149, loss=0.4239/acc=0.8066)
  [mlp] layers=[16, 32, 16, 32, 32, 32] act=leaky_relu lr=0.0045 batch=256 epochs=15 -> avg_log_loss=0.3935 (folds: loss=0.3837/acc=0.7996, loss=0.4102/acc=0.8006, loss=0.4416/acc=0.7737, loss=0.3913/acc=0.8020, loss=0.3812/acc=0.8260, loss=0
.3884/acc=0.8076, loss=0.3602/acc=0.8223, loss=0.3910/acc=0.8029)
  [mlp] layers=[16, 32, 16, 32, 512, 256] act=leaky_relu lr=0.0040 batch=64 epochs=10 -> avg_log_loss=0.3989 (folds: loss=0.3873/acc=0.8079, loss=0.4121/acc=0.7868, loss=0.4422/acc=0.7728, loss=0.4040/acc=0.8048, loss=0.3825/acc=0.8195, loss=
0.3980/acc=0.8149, loss=0.3594/acc=0.8370, loss=0.4060/acc=0.8020)
  [mlp] layers=[256, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0047 batch=256 epochs=30 -> avg_log_loss=0.4113 (folds: loss=0.3983/acc=0.8097, loss=0.4217/acc=0.8042, loss=0.4873/acc=0.7672, loss=0.4219/acc=0.8029, loss=0.3984/acc=0.8057, loss
=0.3806/acc=0.8122, loss=0.3734/acc=0.8287, loss=0.4090/acc=0.8103)
  [mlp] layers=[64, 32, 16, 32, 256, 16] act=leaky_relu lr=0.0042 batch=64 epochs=25 -> avg_log_loss=0.3908 (folds: loss=0.3776/acc=0.8006, loss=0.4096/acc=0.8006, loss=0.4498/acc=0.7691, loss=0.3917/acc=0.8029, loss=0.3780/acc=0.8177, loss=0
.3825/acc=0.8177, loss=0.3536/acc=0.8343, loss=0.3835/acc=0.8048)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=150 -> avg_log_loss=0.4409 (folds: loss=0.4316/acc=0.7831, loss=0.4480/acc=0.7895, loss=0.4781/acc=0.7728, loss=0.4433/acc=0.7937, loss=0.4251/acc=0.8076, loss=0.4407/acc=0.7956, loss=0.4149/
acc=0.8223, loss=0.4457/acc=0.8076)
  [mlp] layers=[16, 128, 16, 32, 512, 32] act=leaky_relu lr=0.0044 batch=256 epochs=15 -> avg_log_loss=0.3926 (folds: loss=0.3817/acc=0.7978, loss=0.4164/acc=0.7969, loss=0.4389/acc=0.7746, loss=0.3957/acc=0.8066, loss=0.3751/acc=0.8177, loss
=0.3895/acc=0.8250, loss=0.3505/acc=0.8435, loss=0.3934/acc=0.8112)
  [mlp] layers=[16, 32, 16, 32, 256, 32] act=leaky_relu lr=0.0043 batch=512 epochs=15 -> avg_log_loss=0.3986 (folds: loss=0.3831/acc=0.8015, loss=0.4076/acc=0.8051, loss=0.4369/acc=0.7829, loss=0.3977/acc=0.8002, loss=0.4047/acc=0.8085, loss=
0.3941/acc=0.8131, loss=0.3653/acc=0.8250, loss=0.3992/acc=0.8011)
  [mlp] layers=[16, 32, 256, 32, 512, 32] act=leaky_relu lr=0.0046 batch=256 epochs=10 -> avg_log_loss=0.3972 (folds: loss=0.3799/acc=0.8015, loss=0.4162/acc=0.7941, loss=0.4494/acc=0.7856, loss=0.3947/acc=0.8048, loss=0.3825/acc=0.8223, loss
=0.3939/acc=0.8195, loss=0.3654/acc=0.8435, loss=0.3957/acc=0.8140)
  best avg log loss this gen: 0.3892
Generation 19/20
  [mlp] layers=[16, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0045 batch=128 epochs=25 -> avg_log_loss=0.3891 (folds: loss=0.3764/acc=0.7969, loss=0.4077/acc=0.8006, loss=0.4427/acc=0.7746, loss=0.3875/acc=0.8029, loss=0.3793/acc=0.8122, loss=
0.3859/acc=0.8066, loss=0.3493/acc=0.8333, loss=0.3840/acc=0.8112)
  [mlp] layers=[16, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0040 batch=128 epochs=25 -> avg_log_loss=0.3909 (folds: loss=0.3794/acc=0.8061, loss=0.4059/acc=0.8107, loss=0.4384/acc=0.7810, loss=0.3958/acc=0.8002, loss=0.3769/acc=0.8186, loss=
0.3825/acc=0.8140, loss=0.3541/acc=0.8287, loss=0.3943/acc=0.8066)
  [mlp] layers=[64, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0041 batch=256 epochs=25 -> avg_log_loss=0.3987 (folds: loss=0.3835/acc=0.7987, loss=0.4170/acc=0.8042, loss=0.4594/acc=0.7792, loss=0.3976/acc=0.8039, loss=0.3800/acc=0.8140, loss=
0.3867/acc=0.8094, loss=0.3605/acc=0.8297, loss=0.4050/acc=0.7983)
  [mlp] layers=[64, 32, 16, 32, 256, 16] act=leaky_relu lr=0.0046 batch=256 epochs=20 -> avg_log_loss=0.3893 (folds: loss=0.3727/acc=0.8024, loss=0.4059/acc=0.8015, loss=0.4428/acc=0.7783, loss=0.3934/acc=0.8085, loss=0.3849/acc=0.8186, loss=
0.3784/acc=0.8177, loss=0.3508/acc=0.8315, loss=0.3857/acc=0.8094)
  [mlp] layers=[16, 32, 16, 32, 512, 16] act=leaky_relu lr=0.0045 batch=128 epochs=10 -> avg_log_loss=0.3957 (folds: loss=0.3799/acc=0.8024, loss=0.4160/acc=0.7932, loss=0.4437/acc=0.7764, loss=0.3967/acc=0.8020, loss=0.3869/acc=0.8131, loss=
0.3865/acc=0.8131, loss=0.3574/acc=0.8379, loss=0.3987/acc=0.8158)
  [extratrees] extratrees trees=100 depth=6 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.3995 (folds: loss=0.3892/acc=0.7987, loss=0.4224/acc=0.7978, loss=0.4430/acc=0.7810, loss=0.4008/acc=0.8122, loss=0.3806/acc=0.8140, loss=0.3998/acc
=0.8140, loss=0.3689/acc=0.8297, loss=0.3915/acc=0.8195)
  [mlp] layers=[64, 32, 16, 32, 256, 32] act=leaky_relu lr=0.0036 batch=512 epochs=30 -> avg_log_loss=0.3932 (folds: loss=0.3817/acc=0.7960, loss=0.4080/acc=0.8024, loss=0.4529/acc=0.7718, loss=0.3978/acc=0.8094, loss=0.3841/acc=0.8168, loss=
0.3795/acc=0.8195, loss=0.3505/acc=0.8352, loss=0.3909/acc=0.7983)
  [mlp] layers=[64, 16, 16, 32, 256, 16] act=leaky_relu lr=0.0034 batch=64 epochs=20 -> avg_log_loss=0.3905 (folds: loss=0.3829/acc=0.7978, loss=0.4073/acc=0.8006, loss=0.4420/acc=0.7691, loss=0.3906/acc=0.8048, loss=0.3787/acc=0.8177, loss=0
.3800/acc=0.8131, loss=0.3539/acc=0.8416, loss=0.3887/acc=0.8094)
  [mlp] layers=[16, 32, 16, 32, 32, 32] act=leaky_relu lr=0.0050 batch=512 epochs=15 -> avg_log_loss=0.3930 (folds: loss=0.3764/acc=0.8024, loss=0.4039/acc=0.8024, loss=0.4389/acc=0.7801, loss=0.3957/acc=0.8039, loss=0.3820/acc=0.8186, loss=0
.3926/acc=0.8112, loss=0.3635/acc=0.8416, loss=0.3911/acc=0.8122)
  [extratrees] extratrees trees=100 depth=4 max_feat=0.70 sample_ratio=1.00 -> avg_log_loss=0.4113 (folds: loss=0.4039/acc=0.7941, loss=0.4307/acc=0.7932, loss=0.4495/acc=0.7884, loss=0.4110/acc=0.7965, loss=0.3919/acc=0.8149, loss=0.4173/acc
=0.8076, loss=0.3774/acc=0.8250, loss=0.4084/acc=0.8029)
  [mlp] layers=[64, 32, 16, 32, 256, 256] act=leaky_relu lr=0.0047 batch=512 epochs=30 -> avg_log_loss=0.3952 (folds: loss=0.3832/acc=0.7987, loss=0.4187/acc=0.8042, loss=0.4554/acc=0.7783, loss=0.3937/acc=0.8066, loss=0.3854/acc=0.8204, loss
=0.3738/acc=0.8140, loss=0.3534/acc=0.8195, loss=0.3978/acc=0.8066)
  [mlp] layers=[64, 64, 16, 32, 256, 16] act=leaky_relu lr=0.0045 batch=512 epochs=30 -> avg_log_loss=0.3894 (folds: loss=0.3770/acc=0.8061, loss=0.4074/acc=0.8006, loss=0.4396/acc=0.7691, loss=0.3969/acc=0.8020, loss=0.3766/acc=0.8195, loss=
0.3775/acc=0.8158, loss=0.3569/acc=0.8260, loss=0.3834/acc=0.8085)
  [histgb] histgb trees=300 depth=7 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4307 (folds: loss=0.4189/acc=0.7950, loss=0.4406/acc=0.7665, loss=0.4781/acc=0.7682, loss=0.4323/acc=0.7956, loss=0.4178/acc=0.8039, loss=0.4248/acc=0.8002,
 loss=0.4004/acc=0.8122, loss=0.4326/acc=0.8029)
  [histgb] histgb trees=400 depth=6 max_feat=0.70 sample_ratio=0.80 -> avg_log_loss=0.4278 (folds: loss=0.4183/acc=0.7941, loss=0.4371/acc=0.7748, loss=0.4712/acc=0.7672, loss=0.4298/acc=0.7947, loss=0.4140/acc=0.8094, loss=0.4234/acc=0.8039,
 loss=0.3990/acc=0.8177, loss=0.4293/acc=0.8011)
  [mlp] layers=[16, 64, 16, 32, 512, 32] act=leaky_relu lr=0.0050 batch=128 epochs=25 -> avg_log_loss=0.3907 (folds: loss=0.3769/acc=0.8033, loss=0.4103/acc=0.7978, loss=0.4438/acc=0.7728, loss=0.3889/acc=0.8094, loss=0.3750/acc=0.8214, loss=
0.3870/acc=0.8158, loss=0.3569/acc=0.8370, loss=0.3863/acc=0.8168)
  [gbstump] gbstump estimators=300 lr=0.2 min_leaf=50 -> avg_log_loss=0.4485 (folds: loss=0.4361/acc=0.7822, loss=0.4531/acc=0.7812, loss=0.4874/acc=0.7645, loss=0.4547/acc=0.7855, loss=0.4235/acc=0.8131, loss=0.4463/acc=0.7882, loss=0.4250/a
cc=0.8149, loss=0.4617/acc=0.7827)
  [gbstump] gbstump estimators=200 lr=0.2 min_leaf=200 -> avg_log_loss=0.4441 (folds: loss=0.4337/acc=0.7858, loss=0.4524/acc=0.7776, loss=0.4785/acc=0.7709, loss=0.4475/acc=0.7928, loss=0.4252/acc=0.8094, loss=0.4443/acc=0.7928, loss=0.4224/
acc=0.8186, loss=0.4484/acc=0.8020)
  [mlp] layers=[16, 32, 512, 32, 512, 32] act=leaky_relu lr=0.0050 batch=256 epochs=20 -> avg_log_loss=0.3901 (folds: loss=0.3808/acc=0.8015, loss=0.4115/acc=0.8006, loss=0.4430/acc=0.7801, loss=0.3927/acc=0.8039, loss=0.3803/acc=0.8149, loss
=0.3818/acc=0.8168, loss=0.3497/acc=0.8297, loss=0.3808/acc=0.8020)
  best avg log loss this gen: 0.3891
Generation 20/20
  [mlp] layers=[16, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0045 batch=128 epochs=25 -> avg_log_loss=0.3910 (folds: loss=0.3783/acc=0.8051, loss=0.4093/acc=0.7996, loss=0.4483/acc=0.7746, loss=0.3906/acc=0.8002, loss=0.3794/acc=0.8214, loss=
0.3781/acc=0.8149, loss=0.3506/acc=0.8425, loss=0.3934/acc=0.8002)
  [mlp] layers=[64, 32, 16, 32, 256, 16] act=leaky_relu lr=0.0046 batch=256 epochs=20 -> avg_log_loss=0.3878 (folds: loss=0.3794/acc=0.8042, loss=0.4080/acc=0.8024, loss=0.4395/acc=0.7792, loss=0.3877/acc=0.8011, loss=0.3720/acc=0.8232, loss=
0.3839/acc=0.8066, loss=0.3478/acc=0.8435, loss=0.3844/acc=0.8140)
  [mlp] layers=[16, 32, 16, 32, 512, 512] act=leaky_relu lr=0.0040 batch=512 epochs=10 -> avg_log_loss=0.4018 (folds: loss=0.4060/acc=0.7960, loss=0.4117/acc=0.8015, loss=0.4462/acc=0.7709, loss=0.3969/acc=0.8103, loss=0.3902/acc=0.8131, loss
=0.3898/acc=0.8149, loss=0.3704/acc=0.8269, loss=0.4030/acc=0.7965)
  [histgb] histgb trees=400 depth=4 max_feat=0.90 sample_ratio=0.80 -> avg_log_loss=0.4316 (folds: loss=0.4231/acc=0.7932, loss=0.4411/acc=0.7803, loss=0.4739/acc=0.7608, loss=0.4331/acc=0.7864, loss=0.4183/acc=0.8112, loss=0.4265/acc=0.8029,
 loss=0.4011/acc=0.8158, loss=0.4356/acc=0.8039)
  [mlp] layers=[256, 32, 512, 32, 512, 32] act=leaky_relu lr=0.0050 batch=128 epochs=25 -> avg_log_loss=0.4060 (folds: loss=0.3971/acc=0.8070, loss=0.4370/acc=0.8006, loss=0.4719/acc=0.7755, loss=0.4032/acc=0.8057, loss=0.3911/acc=0.8140, los
s=0.3834/acc=0.8195, loss=0.3648/acc=0.8297, loss=0.3997/acc=0.8085)
  [mlp] layers=[64, 64, 16, 32, 256, 16] act=leaky_relu lr=0.0050 batch=64 epochs=20 -> avg_log_loss=0.3893 (folds: loss=0.3718/acc=0.8033, loss=0.4075/acc=0.8024, loss=0.4423/acc=0.7856, loss=0.3889/acc=0.8039, loss=0.3803/acc=0.8066, loss=0
.3845/acc=0.8204, loss=0.3543/acc=0.8241, loss=0.3849/acc=0.8122)
  [gbstump] gbstump estimators=200 lr=0.3 min_leaf=100 -> avg_log_loss=0.4457 (folds: loss=0.4350/acc=0.7932, loss=0.4526/acc=0.7785, loss=0.4799/acc=0.7672, loss=0.4505/acc=0.7864, loss=0.4256/acc=0.8094, loss=0.4448/acc=0.7937, loss=0.4271/
acc=0.8158, loss=0.4497/acc=0.7974)
  [mlp] layers=[16, 32, 16, 32, 512, 32] act=leaky_relu lr=0.0050 batch=128 epochs=15 -> avg_log_loss=0.3909 (folds: loss=0.3790/acc=0.7987, loss=0.4087/acc=0.8070, loss=0.4415/acc=0.7764, loss=0.3894/acc=0.8011, loss=0.3774/acc=0.8140, loss=
0.3863/acc=0.8260, loss=0.3528/acc=0.8389, loss=0.3924/acc=0.8094)
  [mlp] layers=[16, 16, 16, 32, 512, 32] act=leaky_relu lr=0.0043 batch=512 epochs=10 -> avg_log_loss=0.4008 (folds: loss=0.3905/acc=0.7960, loss=0.4182/acc=0.7932, loss=0.4446/acc=0.7682, loss=0.3984/acc=0.8057, loss=0.3887/acc=0.8195, loss=
0.4011/acc=0.8085, loss=0.3716/acc=0.8232, loss=0.3937/acc=0.8066)
  [mlp] layers=[64, 64, 16, 128, 256, 16] act=leaky_relu lr=0.0050 batch=64 epochs=10 -> avg_log_loss=0.3952 (folds: loss=0.3850/acc=0.7868, loss=0.4098/acc=0.7987, loss=0.4393/acc=0.7700, loss=0.3898/acc=0.7983, loss=0.3891/acc=0.8140, loss=
0.3931/acc=0.8223, loss=0.3659/acc=0.8112, loss=0.3892/acc=0.8158)
  [mlp] layers=[64, 64, 16, 32, 256, 128] act=leaky_relu lr=0.0039 batch=256 epochs=15 -> avg_log_loss=0.3919 (folds: loss=0.3789/acc=0.8024, loss=0.4113/acc=0.8015, loss=0.4406/acc=0.7709, loss=0.3903/acc=0.8066, loss=0.3817/acc=0.8149, loss
=0.3830/acc=0.8177, loss=0.3556/acc=0.8370, loss=0.3941/acc=0.8029)
  [gbstump] gbstump estimators=100 lr=0.2 min_leaf=200 -> avg_log_loss=0.4409 (folds: loss=0.4318/acc=0.7803, loss=0.4480/acc=0.7868, loss=0.4788/acc=0.7728, loss=0.4430/acc=0.7910, loss=0.4251/acc=0.8076, loss=0.4404/acc=0.7965, loss=0.4148/
acc=0.8232, loss=0.4455/acc=0.8057)
  [mlp] layers=[64, 64, 64, 32, 256, 16] act=leaky_relu lr=0.0037 batch=256 epochs=20 -> avg_log_loss=0.3914 (folds: loss=0.3755/acc=0.8116, loss=0.4107/acc=0.8024, loss=0.4475/acc=0.7746, loss=0.3939/acc=0.8029, loss=0.3840/acc=0.8195, loss=
0.3823/acc=0.8103, loss=0.3423/acc=0.8324, loss=0.3952/acc=0.8066)
  [gbstump] gbstump estimators=200 lr=0.3 min_leaf=100 -> avg_log_loss=0.4457 (folds: loss=0.4350/acc=0.7932, loss=0.4526/acc=0.7785, loss=0.4799/acc=0.7672, loss=0.4505/acc=0.7864, loss=0.4256/acc=0.8094, loss=0.4448/acc=0.7937, loss=0.4271/
acc=0.8158, loss=0.4498/acc=0.7983)
  [mlp] layers=[64, 64, 16, 64, 256, 16] act=leaky_relu lr=0.0037 batch=64 epochs=10 -> avg_log_loss=0.3965 (folds: loss=0.3820/acc=0.7941, loss=0.4128/acc=0.8006, loss=0.4411/acc=0.7755, loss=0.3951/acc=0.7993, loss=0.3928/acc=0.8094, loss=0
.3859/acc=0.8204, loss=0.3684/acc=0.8352, loss=0.3940/acc=0.8140)
  [mlp] layers=[16, 32, 256, 32, 512, 32] act=leaky_relu lr=0.0039 batch=512 epochs=30 -> avg_log_loss=0.3916 (folds: loss=0.3790/acc=0.7978, loss=0.4113/acc=0.8061, loss=0.4473/acc=0.7728, loss=0.3922/acc=0.8002, loss=0.3752/acc=0.8103, loss
=0.3869/acc=0.8223, loss=0.3478/acc=0.8343, loss=0.3929/acc=0.8039)
  [mlp] layers=[16, 32, 32, 32, 512, 32] act=leaky_relu lr=0.0040 batch=256 epochs=10 -> avg_log_loss=0.3983 (folds: loss=0.3868/acc=0.8024, loss=0.4203/acc=0.8024, loss=0.4514/acc=0.7820, loss=0.3991/acc=0.7974, loss=0.3856/acc=0.8149, loss=
0.3875/acc=0.8103, loss=0.3600/acc=0.8269, loss=0.3955/acc=0.8094)
  [mlp] layers=[16, 32, 512, 32, 32, 32] act=leaky_relu lr=0.0040 batch=512 epochs=30 -> avg_log_loss=0.3926 (folds: loss=0.3835/acc=0.7987, loss=0.4091/acc=0.8042, loss=0.4488/acc=0.7691, loss=0.3901/acc=0.8103, loss=0.3772/acc=0.8260, loss=
0.3848/acc=0.8186, loss=0.3525/acc=0.8306, loss=0.3949/acc=0.8057)
  best avg log loss this gen: 0.3878
Retraining top genome #1 (mlp) with val_log_loss=0.3878
Saved model to models/ga_nn_1_1768584855.pt and submission to submissions/ga_nn_1_1768584855.csv
Submitting via Kaggle CLI: kaggle competitions submit -c spaceship-titanic -f submissions/ga_nn_1_1768584855.csv -m "GA rank 1 ts 1768584855 logloss 0.3878"
[warn] Kaggle submission failed:
Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/


Retraining top genome #2 (rf) with val_log_loss=0.3878
Saved model to models/ga_nn_2_1768584856.pt and submission to submissions/ga_nn_2_1768584856.csv
Submitting via Kaggle CLI: kaggle competitions submit -c spaceship-titanic -f submissions/ga_nn_2_1768584856.csv -m "GA rank 2 ts 1768584856 logloss 0.3878"
[warn] Kaggle submission failed:
Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/


Retraining top genome #3 (mlp) with val_log_loss=0.3878
Saved model to models/ga_nn_3_1768584859.pt and submission to submissions/ga_nn_3_1768584859.csv
Submitting via Kaggle CLI: kaggle competitions submit -c spaceship-titanic -f submissions/ga_nn_3_1768584859.csv -m "GA rank 3 ts 1768584859 logloss 0.3878"
[warn] Kaggle submission failed:
Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/


Retraining top genome #4 (mlp) with val_log_loss=0.3878
Saved model to models/ga_nn_4_1768584862.pt and submission to submissions/ga_nn_4_1768584862.csv
Submitting via Kaggle CLI: kaggle competitions submit -c spaceship-titanic -f submissions/ga_nn_4_1768584862.csv -m "GA rank 4 ts 1768584862 logloss 0.3878"
[warn] Kaggle submission failed:
Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/


Retraining top genome #5 (mlp) with val_log_loss=0.3878
Saved model to models/ga_nn_5_1768584865.pt and submission to submissions/ga_nn_5_1768584865.csv
Submitting via Kaggle CLI: kaggle competitions submit -c spaceship-titanic -f submissions/ga_nn_5_1768584865.csv -m "GA rank 5 ts 1768584865 logloss 0.3878"
[warn] Kaggle submission failed:
Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/


Backed up previous summary to models/ga_search_summary_20260116-173425.json
Wrote GA summary to models/ga_search_summary.json
[day5] Copied GA best submission to models/day5_push/day5_ga_best.csv
[day5] $ python scripts/meta_ensemble.py --ga-submission submissions/ga_nn_1_1768584855.csv --test-prepared data/test_prepared.csv --output models/day5_push/day5_meta_lrgb.csv --weights 0.75 0.15 0.1
Wrote meta-ensemble submission to models/day5_push/day5_meta_lrgb.csv
[day5] $ python scripts/ensemble_submissions.py submissions/ga_nn_1_1768584855.csv submissions/day3_stack_meta_submission_trimmed.csv --output models/day5_push/day5_ga_stacker_blend.csv
Wrote ensemble submission to models/day5_push/day5_ga_stacker_blend.csv
[day5] Wrote summary to models/day5_push/day5_summary.json
[day5] Ready to review GA best, meta LR/GB blend, and GA+stacker blend before Kaggle submissions.
root@6a432f30b2e3:/workspace#
root@6a432f30b2e3:/workspace#
root@6a432f30b2e3:/workspace#
root@6a432f30b2e3:/workspace# vim models/day5_push/day5_summary.json
root@6a432f30b2e3:/workspace# cat models/day5_push/day5_summary.json
{
  "best_val_log_loss": 0.3878291710638209,
  "best_submission": "submissions/ga_nn_1_1768584855.csv",
  "meta_ensemble": "models/day5_push/day5_meta_lrgb.csv",
  "ga_stacker_blend": "models/day5_push/day5_ga_stacker_blend.csv"
}root@6a432f30b2e3:/workspace#
root@6a432f30b2e3:/workspace# kaggle competitions submit -c spaceship-titanic \
       -f submissions/ga_nn_1_1768584855.csv \
       -m "Day5 GA best val_log_loss 0.3878"
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.4k/56.4k [00:00<00:00, 102kB/s]
root@6a432f30b2e3:/workspace#  kaggle competitions submit -c spaceship-titanic \ competitions submit -c spaceship-titanic \
    -f models/day5_push/day5_meta_lrgb.csv \
    -m "Day5 GA+LRGB meta"
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.4k/56.4k [00:00<00:00, 100kB/s]
Successfully submitted to Spaceship Titanicroot@6a432f30b2e3:/workspace#
  kaggle competitions submit -c spaceship-titanic \
    -f models/day5_push/day5_ga_stacker_blend.csv \
    -m "Day5 GA+stacker blend"
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.3k/56.3k [00:00<00:00, 101kB/s]
root@6a432f30b2e3:/workspace#  python scripts/meta_ensemble.py \rkspace#  python scripts/meta_ensemble.py \
    --ga-submission submissions/ga_nn_1_1768584855.csv \
    --test-prepared data/test_prepared.csv \
    --output models/day5_push/day5_meta_lrgb_80_10_10.csv \
    --weights 0.8 0.1 0.1
Wrote meta-ensemble submission to models/day5_push/day5_meta_lrgb_80_10_10.csv
root@6a432f30b2e3:/workspace# kaggle competitions submit -c spaceship-titanic \
    -f models/day5_push/day5_meta_lrgb_80_10_10.csv \
    -m "Day5 GA+LRGB meta (80/10/10)"
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.4k/56.4k [00:00<00:00, 104kB/s]
Successfully submitted to Spac^C
root@6a432f30b2e3:/workspace# ^C
root@6a432f30b2e3:/workspace#

