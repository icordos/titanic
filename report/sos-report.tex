\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
%\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{titling}
\usepackage{graphicx} % Required for images
\usepackage{subcaption} 
\usepackage{placeins}
%\usepackage{subfig}
\usepackage{natbib} % For better citation handling

% IBM Plex fonts
\usepackage{plex-serif}   % For main text
\usepackage{plex-sans}    % For sans-serif
\usepackage{plex-mono}    % For monospaced


\title{Spaceship Titanic - Automated neural network architecture generation using genetic algorithms}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Cristian Cordoș\\
  National University of Science and Technology POLITEHNICA Bucharest, Romania\\
  \texttt{ioan.cordos@stud.acs.upb.ro}\\
   \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
In this work we are attempting to resolve and optimize a classical data science problem involving numerical data using deep neural networks automatically fine tuned by genetic algorithms.
\end{abstract}


\section{Introduction}

A few words about neural network architecture generation using genetic algorithms.
\\
\\
We chose a practical problem to research the domain of neural network generation through genetic algorithms. The problem is actually a Kaggle Challenge, Spaceship Titanic \cite{spaceship-titanic}. A newly launched space liner loaded with 13000 passengers encounters a spacetime anomaly that causes half of the passengers to be transported to an alternate dimension. The challenge is to predict which passengers were transported using records recovered from the spaceship's damaged computer system.

\section{Dataset Analysis}

\paragraph{Training Data}

The training data is distributed in tabular form as a csv file. It consists of 14 features and 8693 rows.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{csv.png}
    \caption{Training data view}
    \label{fig:csv}
\end{figure}
\FloatBarrier

The dataset is not perfectly clean, as seen in Figure \ref{fig:missing}, there are around 2\% missing cells per feature.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.22\textwidth]{missing.png}
    \caption{Training data view}
    \label{fig:missing}
\end{figure}
\FloatBarrier

In Figure \ref{fig:features} we can see a summary / statistic of the various features of the training data. We notice the transported feature, the one we need to predict, is pretty balanced (49.63\% vs 50.36\%).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{categorical.png}
        \caption{Categorical features summary}
        \label{fig:categorical}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{numerical.png}
        \caption{Numerical features statistics}
        \label{fig:numerical}
    \end{subfigure}
    \caption{Dataset feature analysis}
    \label{fig:features}
\end{figure}

In the correlation matrix of the numerical features we notice strong correlation between Spa, VRDeck and FoodCourt spending and very weak correlations for the Transported feature.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{correlation.png}
    \caption{Correlation matrix of numerical features}
    \label{fig:correlation}
\end{figure}
\FloatBarrier

\paragraph{Test Data}
The test data is presented in the same format, and mirrors the training data except for the Transported feature, which is the one that needs prediction. It features 4277 rows that need prediction. It maintains approximately the same percentage of missing cells (2\%). Also the data distributions are similar with the ones in the training dataset as shown in Figure \ref{fig:test-features}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{test-categorical.png}
        \caption{Categorical features summary}
        \label{fig:categorical}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{test-numerical.png}
        \caption{Numerical features statistics}
        \label{fig:numerical}
    \end{subfigure}
    \caption{Test dataset feature analysis}
    \label{fig:test-features}
\end{figure}

\section{Feature Engineering}

We apply a number of transformations to the features, as follows:
\begin{itemize}
	\item \textbf{boolean normalization} - CryoSleep and VIP features are transformed from "True"/"False" to 1/0
	\item \textbf{spend clipping} - each spend column (RoomService, FoodCourt, ShoppingMall, Spa and VRDeck) is trimmed at the 99.5th percentile to eliminate the extreme outliers before aggregation or scaling
	\item \textbf{cabin decomposition} - the cabin field (Deck/Num/Side) is split into CabinDeck, CabinNum and CabinSide so the models can learn deck-level effects, longitudinal position and port vs starboard differences separately
	\item \textbf{passenger group features} - parses PassengerId into GroupId, GroupMemberNumber, GroupSize and IsGroupSolo. This exposes group structure (families/friends traveling together), which correlates with survival in the competition narrative
	\item \textbf{family featuers} - extracts surnames from Name, builds FamilySize, and flags IsFamilySolo. It captures broader kinship beyond the PassengerId grouping and differentiates solo travelers from larger families
	\item \textbf{spend aggregations} - creates TotalSpend, LogTotalSpend, NonZeroSpendCount, and AllZeroSpend to summarize how much passengers spend on amenities and whether they spend at all—useful because cryosleep travelers usually show zero spend.
	\item \textbf{per-passenger spend ratios} - adds SpendPerPassenger plus per-amenity usage flags (RoomServiceUsed, etc.) and spend shares (RoomServiceShare, …) so the model knows whether someone spends on specific services and what fraction of their budget each category takes.
	\item \textbf{cabin location cues} - maps CabinDeck to an ordinal DeckRank, encodes IsPortSide/IsStarboardSide, and buckets CabinNum into CabinZone bins (front/mid/rear/deep) to approximate spatial placement
	\item \textbf{group age statistics} - Computes each group’s mean/std/range of Age, adds per-passenger residuals (AgeMinusGroupMean), flags large families, and compares family vs group sizes (FamilyShareOfGroup, GroupMinusFamilySize, GroupOutnumbersFamily). These capture heterogeneity within traveling parties.
	\item \textbf{route feature}	 - concatenates HomePlanet and Destination, keeps the top ROUTE\_TOP\_K combos, and bins others into “Other”. This highlights popular travel routes that correlate with the target.
	\item \textbf{cryo vs spend mismatch} - Flags passengers whose cryosleep status conflicts with spend totals (e.g., CryoSleep=True but nonzero spend), helping the model exploit data inconsistencies that often correlate with outcomes.
\end{itemize}

Then we fill in the missing cells, with various strategies based on the column type:
\begin{itemize}
	\item \textbf{Age} - If HomePlanet is present, fill missing ages with the median age per planet; remaining gaps fall back to the global median
      	\item \textbf{CabinNum} - If CabinDeck exists, fill per-deck medians first, then the global median
      	\item \textbf{Spend columns (RoomService, …)} - If Destination exists, fill per-destination medians, then overall medians
	\item \textbf{Group/family metrics (GroupId, GroupMemberNumber, GroupSize, FamilySize)} - fill with the column median so engineered ratios remain finite.
      	\item \textbf{Boolean-like indicators (CryoSleep, VIP, IsGroupSolo, IsFamilySolo, AllZeroSpend)} - fill zeros to treat missing as “false”
	\item \textbf{String categoricals (CabinDeck, CabinSide, HomePlanet, Destination)} - fill "Unknown" so they still get one-hot encoded
\end{itemize}

After targeted fills, for numeric columns the remaining NaNs are filled with the column median and for categorical columns they filled with the column mode (most frequent value), defaulting to "Unknown" if a mode cannot be computed. In the last step the numerical columns are scaled using Z-score. This transformations, applied both to training and test data, blow the total number of features to 75, thus creating a much richer dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{prepared-correlation.png}
    \caption{Correlation matrix of prepared features}
    \label{fig:prepared-correlation}
\end{figure}
\FloatBarrier

As seen in Figure \ref{fig:prepared-correlation} there is correlation between transported and CryoSleep and AllZeroSpend.
\\
\\
Finally, the last step in data augumentation is to add 2 predictions, one based on logistic regression and the other based on a gradient-boosted stump model. They capture different decision boundaries than the genetic algorithm might find, and their outputs are often well-calibrated. By appending them as features, we let the genetic algorithm models learn from these baseline predictions (e.g., blending them, focusing on where they disagree) without having to run the baselines separately later.

\section{Genetic Algorithm Design}

For population initialization each genome samples a model type (MLP, gradient-boosted stumps, random forest, ExtraTrees) along with its hyperparameters. We can also seed genomes from past runs.
\\
We build either a single holdout or K stratified folds and reuse these splits for all genomes to keep fitness comparisons consistent.
\\
During each generation, we train every genome on each cross-validation fold, record fold accuracies and log losses, and compute the average log loss $\\ell_g$. The fitness is defined as $f_g = -\\ell_g$ so lower loss means higher fitness; we also log the per-fold metrics for transparency.
\\
We sort genomes by fitness, keep the top elites unchanged, and refill the population with mutated copies. Macro mutations can switch model families or re-sample architectures, while micro mutations tweak hyperparameters.
 \\
We iterate for T generations: repeat the evaluate → select → mutate loop for the configured number of generations.
\\
We retrain the best k genomes on the full training set (MLPs via PyTorch, tree models via their custom classes), generate test predictions, and save both the model weights and submission CSVs. Metadata (including $\\ell_g$) is for future restarts or ensembles.

\section{Results}

\section{Conclusions and Future Steps}

\appendix

\section{Code Layout}


\bibliographystyle{plainnat}  % Choose a bibliography style (plainnat, ieee, etc.)
\bibliography{references}  % Load the .bib file (without the .bib extension)

\end{document}