\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
%\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}        % math environments
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{titling}
\usepackage{graphicx} % Required for images
\usepackage{subcaption} 
\usepackage{placeins}
%\usepackage{subfig}
\usepackage{natbib} % For better citation handling

% IBM Plex fonts
\usepackage{plex-serif}   % For main text
\usepackage{plex-sans}    % For sans-serif
\usepackage{plex-mono}    % For monospaced


\title{Spaceship Titanic - Automated neural network architecture generation using genetic algorithms}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  \makebox[\textwidth][c]{%
    \begin{tabular}{c}
      Cristian Cordoș\\
      National University of Science and Technology POLITEHNICA Bucharest, Romania\\
      \texttt{ioan.cordos@stud.acs.upb.ro}
    \end{tabular}
  }
}


\begin{document}


\maketitle


\begin{abstract}
In this work we automatically fine tune deep neural networks using genetic algorithms and ensemble models for the Kaggle Spaceship Titanic competition.
\\
We explore five model families (MLP, Gradient-Boosted Stumps, Random Forest, ExtraTrees, HistGradientBoosting) through evolutionary search, achieving a final Kaggle score of 0.80640.
\end{abstract}


\section{Introduction}

The automated design of neural network architectures remains a fundamental challenge in machine learning. While deep learning has achieved remarkable success across diverse domains, the performance of these models critically depends on architectural choices and hyperparameter configurations that are typically determined through expensive manual tuning or expert intuition. This dependency creates a significant barrier to applying deep learning effectively, particularly for practitioners working with domain-specific tabular datasets where optimal architectures are not well-established.
\\
We chose a practical problem to research the domain of neural network generation through genetic algorithms. The problem is a Kaggle Challenge, Spaceship Titanic \cite{spaceship-titanic}. A newly launched space liner loaded with 13000 passengers encounters a spacetime anomaly that causes half of the passengers to be transported to an alternate dimension. The challenge is to predict which passengers were transported using records recovered from the spaceship's damaged computer system.

\section{State of the Art}

Neural Architecture Search (NAS) automates the design of neural network architectures, traditionally requiring expert knowledge and extensive manual tuning. While NAS has achieved remarkable success in computer vision~\citep{zoph2016neural} and NLP, its application to tabular data remains underexplored despite tabular datasets dominating real-world ML applications in healthcare, finance, and industry.

\subsection{Classical NAS Approaches}

\textbf{Reinforcement Learning-based NAS}~\citep{zoph2016neural,pham2018efficient}: Uses RL agents to generate architectures sequentially. NASNet achieved state-of-the-art ImageNet accuracy but required 2000 GPU-days. ENAS~\cite{pham2018efficient} reduced this to 1 GPU-day via parameter sharing.

\textbf{Evolutionary Methods}~\cite{real2019regularized}: Genetic algorithms evolve populations of architectures. AmoebaNet~\cite{real2019regularized} used tournament selection with 3150 GPU-days. More efficient variants like NSGA-Net employ multi-objective optimization trading accuracy vs. complexity.

\textbf{Gradient-based NAS}~\cite{liu2018darts}: DARTS formulates architecture search as continuous optimization over relaxed architecture spaces. Reduces search to 4 GPU-days but suffers from discretization gaps and instability.

\textbf{Bayesian Optimization}: Uses Gaussian processes or tree-structured Parzen estimators to model architecture performance. BOHB combines BO with Hyperband for efficient search.

\subsection{Tabular-Specific Challenges}

Unlike vision/NLP, tabular data presents unique challenges:
\begin{itemize}
    \item \textbf{Heterogeneous features}: Mixed categorical, numerical, and ordinal types
    \item \textbf{Irregular patterns}: No spatial/sequential structure to exploit
    \item \textbf{Tree-based dominance}: XGBoost/LightGBM often outperform NNs
    \item \textbf{Small-medium scale}: Datasets typically $10^{2}$--$10^{6}$ samples vs. millions in vision
    \item \textbf{Feature engineering}: Critical preprocessing requires domain knowledge
\end{itemize}


\section{Dataset Analysis}

\paragraph{Training Data}

The training data is distributed in tabular form as a csv file. It consists of 14 features and 8693 rows.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{csv.png}
    \caption{Training data view}
    \label{fig:csv}
\end{figure}
\FloatBarrier

The dataset is not perfectly clean, as seen in Figure \ref{fig:missing}, there are around 2\% missing cells per feature.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.22\textwidth]{missing.png}
    \caption{Training data view}
    \label{fig:missing}
\end{figure}
\FloatBarrier

In Figure \ref{fig:features} we can see a summary / statistic of the various features of the training data. We notice the transported feature, the one we need to predict, is pretty balanced (49.63\% vs 50.36\%).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{categorical.png}
        \caption{Categorical features summary}
        \label{fig:categorical}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{numerical.png}
        \caption{Numerical features statistics}
        \label{fig:numerical}
    \end{subfigure}
    \caption{Dataset feature analysis}
    \label{fig:features}
\end{figure}

In the correlation matrix of the numerical features we notice strong correlation between Spa, VRDeck and FoodCourt spending and very weak correlations for the Transported feature.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{correlation.png}
    \caption{Correlation matrix of numerical features}
    \label{fig:correlation}
\end{figure}
\FloatBarrier

\paragraph{Test Data}
The test data is presented in the same format, and mirrors the training data except for the Transported feature, which is the one that needs prediction. It features 4277 rows that need prediction. It maintains approximately the same percentage of missing cells (2\%). Also the data distributions are similar with the ones in the training dataset as shown in Figure \ref{fig:test-features}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{test-categorical.png}
        \caption{Categorical features summary}
        \label{fig:categorical}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{test-numerical.png}
        \caption{Numerical features statistics}
        \label{fig:numerical}
    \end{subfigure}
    \caption{Test dataset feature analysis}
    \label{fig:test-features}
\end{figure}

\section{Feature Engineering}

We apply a number of transformations to the features, as follows:
\begin{itemize}
	\item \textbf{boolean normalization} - CryoSleep and VIP features are transformed from "True"/"False" to 1/0
	\item \textbf{spend clipping} - each spend column (RoomService, FoodCourt, ShoppingMall, Spa and VRDeck) is trimmed at the 99.5th percentile to eliminate the extreme outliers before aggregation or scaling
	\item \textbf{cabin decomposition} - the cabin field (Deck/Num/Side) is split into CabinDeck, CabinNum and CabinSide so the models can learn deck-level effects, longitudinal position and port vs starboard differences separately
	\item \textbf{passenger group features} - parses PassengerId into GroupId, GroupMemberNumber, GroupSize and IsGroupSolo. This exposes group structure (families/friends traveling together), which correlates with survival in the competition narrative
	\item \textbf{family featuers} - extracts surnames from Name, builds FamilySize, and flags IsFamilySolo. It captures broader kinship beyond the PassengerId grouping and differentiates solo travelers from larger families
	\item \textbf{spend aggregations} - creates TotalSpend, LogTotalSpend, NonZeroSpendCount, and AllZeroSpend to summarize how much passengers spend on amenities and whether they spend at all—useful because cryosleep travelers usually show zero spend.
	\item \textbf{per-passenger spend ratios} - adds SpendPerPassenger plus per-amenity usage flags (RoomServiceUsed, etc.) and spend shares (RoomServiceShare, …) so the model knows whether someone spends on specific services and what fraction of their budget each category takes.
	\item \textbf{cabin location cues} - maps CabinDeck to an ordinal DeckRank, encodes IsPortSide/IsStarboardSide, and buckets CabinNum into CabinZone bins (front/mid/rear/deep) to approximate spatial placement
	\item \textbf{group age statistics} - Computes each group’s mean/std/range of Age, adds per-passenger residuals (AgeMinusGroupMean), flags large families, and compares family vs group sizes (FamilyShareOfGroup, GroupMinusFamilySize, GroupOutnumbersFamily). These capture heterogeneity within traveling parties.
	\item \textbf{route feature}	 - concatenates HomePlanet and Destination, keeps the top ROUTE\_TOP\_K combos, and bins others into “Other”. This highlights popular travel routes that correlate with the target.
	\item \textbf{cryo vs spend mismatch} - Flags passengers whose cryosleep status conflicts with spend totals (e.g., CryoSleep=True but nonzero spend), helping the model exploit data inconsistencies that often correlate with outcomes.
\end{itemize}

Then we fill in the missing cells, with various strategies based on the column type:
\begin{itemize}
	\item \textbf{Age} - If HomePlanet is present, fill missing ages with the median age per planet; remaining gaps fall back to the global median
      	\item \textbf{CabinNum} - If CabinDeck exists, fill per-deck medians first, then the global median
      	\item \textbf{Spend columns (RoomService, …)} - If Destination exists, fill per-destination medians, then overall medians
	\item \textbf{Group/family metrics (GroupId, GroupMemberNumber, GroupSize, FamilySize)} - fill with the column median so engineered ratios remain finite.
      	\item \textbf{Boolean-like indicators (CryoSleep, VIP, IsGroupSolo, IsFamilySolo, AllZeroSpend)} - fill zeros to treat missing as “false”
	\item \textbf{String categoricals (CabinDeck, CabinSide, HomePlanet, Destination)} - fill "Unknown" so they still get one-hot encoded
\end{itemize}

After targeted fills, for numeric columns the remaining NaNs are filled with the column median and for categorical columns they filled with the column mode (most frequent value), defaulting to "Unknown" if a mode cannot be computed. In the last step the numerical columns are scaled using Z-score. This transformations, applied both to training and test data, blow the total number of features to 75, thus creating a much richer dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{prepared-correlation.png}
    \caption{Correlation matrix of prepared features}
    \label{fig:prepared-correlation}
\end{figure}
\FloatBarrier

As seen in Figure \ref{fig:prepared-correlation} there is correlation between transported and CryoSleep and AllZeroSpend.
\\
\\
Finally, the last step in data augumentation is to add 2 predictions, one based on logistic regression and the other based on a gradient-boosted stump model. They capture different decision boundaries than the genetic algorithm might find, and their outputs are often well-calibrated. By appending them as features, we let the genetic algorithm models learn from these baseline predictions (e.g., blending them, focusing on where they disagree) without having to run the baselines separately later.

\section{Genetic Algorithm Design}

Our GA explores five genome families, each with its own hyperparameter set:

\begin{enumerate}
    \item \textbf{Multi Layer Perceptron} (\texttt{model\_type="mlp"})
    \begin{itemize}
        \item \textit{Architecture genes:} number of layers (1--6), per-layer widths drawn from [16, 32, 64, 128, 256, 512], activation (relu, gelu, leaky\_relu), dropout rate, batch size, epoch count.
        \item \textit{Optimization genes:} optimizer (adam/adamw), learning rate ($5 \times 10^{-4}$ to $5 \times 10^{-3}$), weight decay (0 to $10^{-3}$).
        \item \textit{Training:} PyTorch sequential net with BatchNorm + optional Dropout per layer, BCEWithLogitsLoss, ReduceLROnPlateau scheduler, predictions via sigmoid.
    \end{itemize}
    
    \item \textbf{Gradient-Boosted Stumps (\citep{friedman2001gradientboosting})} (\texttt{model\_type="gbstump"})
    \begin{itemize}
        \item \textit{Genes:} number of estimators (100--300), learning rate (0.1--0.3), min leaf size (50--200).
        \item \textit{Implementation:} scikit-learn GradientBoostingClassifier forced to depth-1 trees (decision stumps). Outputs are calibrated via CalibratedClassifierCV (3-fold sigmoid) for reliable probabilities.
    \end{itemize}
    
    \item \textbf{Random Forest (\citep{breiman2001randomforest})} (\texttt{model\_type="rf"})
    \begin{itemize}
        \item \textit{Genes:} tree count (100--400), max depth (4--8), max features (0.5--0.9 fraction), min samples split (10--40), max\_samples (0.6--1.0).
        \item \textit{Implementation:} scikit-learn RandomForestClassifier with \texttt{n\_jobs=-1}. Predictions are likewise passed through CalibratedClassifierCV to smooth probabilities.
    \end{itemize}
    
    \item \textbf{ExtraTrees(\citep{geurts2006extratrees}) } (\texttt{model\_type="extratrees"})
    \begin{itemize}
        \item Shares the RF hyperparameters (tree count, depth, max\_features, min\_samples\_split, max\_samples), plus a \texttt{rf\_random\_thresholds} flag set true for this family.
        \item \textit{Implementation:} scikit-learn ExtraTreesClassifier (with bootstrap enabled when max\_samples $< 1.0$) and calibration wrapper.
    \end{itemize}
    
    \item \textbf{HistGradientBoosting (\citep{ke2017lightgbm})} (\texttt{model\_type="histgb"})
    \begin{itemize}
        \item \textit{Genes:} learning rate (0.03--0.1), max depth (3--5 or None), max leaf nodes (31/63/127), min samples leaf (20--100), L2 regularization (0.0--0.6).
        \item \textit{Implementation:} scikit-learn HistGradientBoostingClassifier, also wrapped in CalibratedClassifierCV.
    \end{itemize}
\end{enumerate}

Each genome type contributes different inductive biases: MLPs model high-order interactions, GBDT variants capture step-wise splits, RF/ExtraTrees add bagged diversity, and HistGB offers strong tabular performance with histogram-based splits.

Fitness evaluation uses either a single holdout split or stratified $k$-fold cross-validation ($k \in \{5,8\}$ in the experiments). For each genome $g$ we train on every fold, compute log loss $\ell_{g,i}$ and accuracy $a_{g,i}$, and define fitness as $f_g = -\tfrac{1}{k} \sum_i \ell_{g,i}$ so lower loss yields higher fitness. Tree models are wrapped in \texttt{CalibratedClassifierCV} to ensure well-calibrated probabilities before ensembling; MLPs are trained with BCEWithLogitsLoss, Adam/AdamW optimizers, batch normalization, dropout, and ReduceLROnPlateau schedulers. All fold metrics are logged to the console for transparency and later analysis.

Selection keeps the top $E$ elites every generation and refills the population with mutated descendants. ``Macro'' mutations can switch model families or redraw an entire architecture (e.g., number of layers), while ``micro'' mutations perturb continuous hyperparameters (learning rate, dropout, weight decay) or resample discrete values (trees, depth, min samples split). The evaluate $\rightarrow$ select $\rightarrow$ mutate loop repeats for $T$ generations, steadily improving the population's fitness while preserving diversity.

After evolution, the best $k$ genomes are retrained on the full training set. PyTorch MLPs are serialized via \texttt{state\_dict}, and scikit-learn models are stored directly. Each retrained model produces a Kaggle submission CSV along with metadata describing its validation log loss, fold metrics, submission path, and (when applicable) Kaggle score. These summaries drive subsequent stacking pipelines and allow future GA runs to restart from the strongest known configurations.

\section{Results}

The experimental run that produced the top scoring submission started with a long genetic algorithm sweep of population 15 across 40 generations, each genome being trained with the dataset split over 5 k-folds.

\begin{table}[h]
\centering
\label{tab:ga_results}
\begin{tabular}{|c|l|c|p{6cm}|c|}
\hline
\textbf{Rank} & \textbf{Model Type} & \textbf{Val Log Loss} & \textbf{Architecture Description} & \textbf{Kaggle Score} \\
\hline
1 & ExtraTrees & 0.3887 & 3 layers [512, 64, 128], 300 GB estimators, 0.005 LR & 0.80079 \\
\hline
2 & GB Stump & 0.3887 & 4 layers [256, 32, 16, 256], 100 GB estimators, 0.005 LR & 0.79565 \\
\hline
3 & ExtraTrees & 0.3887 & 3 layers [512, 512, 128], 150 GB estimators, 0.004 LR & 0.79752 \\
\hline
4 & ExtraTrees & 0.3887 & 3 layers [32, 64, 128], 300 GB estimators, 0.0045 LR & 0.79775 \\
\hline
5 & GB Stump & 0.3887 & 1 layer [16], 150 GB estimators, 0.005 LR & 0.79494 \\
\hline
6 & ExtraTrees & 0.3887 & 3 layers [128, 64, 128], 300 GB estimators, 0.005 LR & --- \\
\hline
7 & ExtraTrees & 0.3889 & 3 layers [32, 16, 128], 150 GB estimators, 0.005 LR & --- \\
\hline
8 & ExtraTrees & 0.3889 & 3 layers [32, 16, 64], 300 GB estimators, 0.004 LR & --- \\
\hline
9 & ExtraTrees & 0.3889 & 3 layers [16, 16, 128], 100 GB estimators, 0.0048 LR & --- \\
\hline
10 & Random Forest & 0.3889 & 3 layers [128, 64, 512], 200 RF trees, 0.005 LR & --- \\
\hline
\end{tabular}
\caption{Genetic Algorithm Hyperparameter Search Results}
\end{table}
\FloatBarrier

Then we pulled the best entries from the above run, generated out‑of‑fold predictions for those models and combined them with the two deterministic features (LR\_pred, GB\_pred) already baked into the training dataset. A logistic regression meta‑model (with standardization) was trained on the stacked OOF matrix and produced a new submission that scored 0.80149.
\\
After this we extended the genetic algorithm to include HistGradientBoosting genomes, we selected the previous top 10 models as seed, and we ran across 15 generations with a population of 15, each model trained over 8-folds.

\begin{table}[h]
\centering
\label{tab:ga_results_new}
\begin{tabular}{|c|l|c|p{6cm}|c|}
\hline
\textbf{Rank} & \textbf{Model Type} & \textbf{Val Log Loss} & \textbf{Architecture Description} & \textbf{Kaggle Score} \\
\hline
1 & ExtraTrees & 0.4002 & 3 layers [32, 16, 32], 300 GB estimators, 0.005 LR & 0.80079 \\
\hline
2 & ExtraTrees & 0.4002 & 3 layers [128, 16, 32], 300 GB estimators, 0.005 LR & 0.79845 \\
\hline
3 & ExtraTrees & 0.4002 & 3 layers [32, 64, 32], 150 GB estimators, 0.0045 LR & 0.79869 \\
\hline
\end{tabular}
\caption{Genetic Algorithm Hyperparameter Search Results}
\end{table}
\FloatBarrier

Individual submissions clustered around 0.799 and the best genome and stacker blend reached 0.80266.
\\
For the next step we returned to feature engineering and added:
\begin{itemize}
	\item CabinDeckEmbedSin \ CabinDeckEmbedCos – sine/cosine projections of the deck rank (A–T normalized) so the model sees relative deck position without treating the ordinal index as linear.
      	\item CabinNumEmbedSin / CabinNumEmbedCos – normalized cabin-number projections (sin/cos over the longitudinal position) to capture spatial cycles down the ship.
      	\item CabinDeckSideInteraction – deck rank multiplied by a side indicator (port vs starboard) so asymmetries across the hull can be learned.
	\item Spend-share cleanup: After computing per-service usage flags and shares, we dropped the weak RoomServiceShare/FoodCourtShare/… columns entirely, keeping only the stronger aggregates (TotalSpend, SpendPerPassenger, per-service usage booleans). This removed noisy ratios that weren’t helping the GA while preserving the informative spend signals.
\end{itemize}

  
\begin{table}[h]
\centering
\label{tab:ga_results_mlp}
\begin{tabular}{|c|l|c|p{6cm}|c|}
\hline
\textbf{Rank} & \textbf{Model Type} & \textbf{Val Log Loss} & \textbf{Architecture Description} & \textbf{Kaggle Score} \\
\hline
1 & MLP & 0.3907 & 1 layer [32], 300 GB estimators, 0.0046 LR & 0.80430 \\
\hline
2 & MLP & 0.3907 & 1 layer [128], 200 GB estimators, 0.0041 LR & --- \\
\hline
\end{tabular}
\caption{Genetic Algorithm Hyperparameter Search Results}
\end{table}
\FloatBarrier

Those changes, along with rerunning the pipeline for 10 generations with a population of 12 using 5-folds, gave a richer cabin representation and a cleaner spend feature set, enabling the Day 3 GA run to reach 0.8043.
\\
In the final step we use the top 15 models from previous experiments and we orchestrate a longer GA search of 20 generations with a population of 18 over 8-folds.

\begin{table}[h]
\centering
\label{tab:ga_results_day5}
\begin{tabular}{|c|l|c|p{6cm}|c|}
\hline
\textbf{Rank} & \textbf{Model Type} & \textbf{Val Log Loss} & \textbf{Architecture Description} & \textbf{Kaggle Score} \\
\hline
1 & MLP & 0.3878 & 6 layers [64, 32, 16, 32, 256, 16], 100 GB estimators, 0.0046 LR & 0.80640 \\
\hline
2 & Random Forest & 0.3878 & 4 layers [128, 16, 256, 16], 100 GB estimators, 0.005 LR & --- \\
\hline
3 & MLP & 0.3878 & 6 layers [64, 32, 16, 128, 256, 16], 200 GB estimators, 0.005 LR & --- \\
\hline
4 & MLP & 0.3878 & 6 layers [64, 32, 16, 32, 256, 16], 150 GB estimators, 0.005 LR & --- \\
\hline
5 & MLP & 0.3878 & 6 layers [64, 32, 16, 32, 256, 512], 200 GB estimators, 0.0048 LR & --- \\
\hline
\end{tabular}
\caption{Genetic Algorithm Hyperparameter Search Results}
\end{table}
\FloatBarrier

\subsection{Best Submission - Complete Parameter Specification}

The best Kaggle score is \textbf{0.80640}. The complete parameter specification is as follows:

\subsubsection{Model Overview}

The top-performing model is a \textbf{multi-layer perceptron (MLP)} ensemble that achieved a validation log loss of 0.3878 and a Kaggle public score of 0.80640.

\subsubsection{Neural Network Architecture}

\begin{itemize}
    \item Model type: MLP
    \item Number of layers: 6
    \item Layer widths: [64, 32, 16, 32, 256, 16]
    \item Activation function: Leaky ReLU
    \item Dropout rate: 0.2254 (22.54\%)
\end{itemize}

\subsubsection{Training Configuration}

\begin{itemize}
    \item Optimizer: Adam
    \item Learning rate: 0.004603
    \item Weight decay: 0.0 (no L2 regularization on neural network)
    \item Batch size: 256
    \item Training epochs: 20
\end{itemize}

\subsubsection{Gradient Boosting Parameters}

\begin{itemize}
    \item Number of estimators: 100
    \item Learning rate: 0.1
    \item Minimum samples per leaf: 100
\end{itemize}

\subsubsection{Random Forest Parameters}

\begin{itemize}
    \item Number of trees: 400
    \item Maximum tree depth: 8
    \item Maximum features per split: 0.5 (50\%)
    \item Minimum samples for split: 20
    \item Sample ratio: 0.6 (60\% bootstrap sampling)
    \item Random thresholds: False
\end{itemize}

\subsubsection{Histogram-based Gradient Boosting Parameters}

\begin{itemize}
    \item Learning rate: 0.0341
    \item Maximum depth: 5
    \item Maximum leaf nodes: 31
    \item Minimum samples per leaf: 20
    \item L2 regularization: 0.0
\end{itemize}

In summary, this model represents the convergence of the genetic algorithm search after multiple iterations, with the 6-layer MLP architecture proving most effective when combined with moderate ensemble sizes and carefully tuned regularization through dropout.

The figure below shows how the Kaggle score evolves across the 4 GA runs.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{score-evolution.png}
    \caption{Kaggle score evolution across GA generations}
    \label{fig:csv}
\end{figure}
\FloatBarrier

\section{Discussion}

Examining the progression across our four experimental cycles reveals interesting patterns. The validation log loss remained remarkably stable (ranging only from 0.3878 to 0.3907), yet Kaggle test scores improved from 0.80079 to 0.80640. This apparent discrepancy suggests two phenomena: (1) our cross-validation strategy effectively prevented overfitting, maintaining consistent validation performance, and (2) improvements in test performance came primarily from better generalization rather than simply fitting the training data more tightly.
\\
The genetic algorithm showed clear preferences for certain model families. ExtraTrees and MLPs dominated the top-performing genomes, appearing in 18 of the 23 documented high-fitness solutions. Gradient-Boosted Stumps showed competitive validation metrics but generally weaker test performance, suggesting potential overfitting despite calibration. HistGradientBoosting, introduced in the third experimental cycle, failed to surpass existing solutions, indicating the initial model family selection was already near-optimal for this task.
\\
The best-performing MLP architecture (6 layers: [64, 32, 16, 32, 256, 16]) defies conventional wisdom that deeper or wider networks are necessarily better. The asymmetric widths and the narrow bottleneck at layer 3 (width 16) suggest the network learned to compress information before expanding it again—a pattern reminiscent of autoencoder architectures. This emergent structure was not explicitly designed but discovered through evolutionary search, highlighting the value of exploration over imposed architectural priors.
\\
Tree-based models converged toward moderate ensemble sizes (100-300 estimators) rather than maximizing tree count, with lower learning rates (0.004-0.005) providing better regularization than aggressive boosting. The preference for ExtraTrees over standard Random Forests suggests the additional randomness in split selection improved generalization on this particular dataset.
\\
The iterative refinement of features had substantial impact. The addition of sine/cosine embeddings for spatial cabin features (Day 3) produced a 0.0013 improvement in Kaggle score—modest but consistent across multiple model architectures. Conversely, removing weak spend-share ratios paradoxically improved performance by reducing noise in the feature space, demonstrating that feature engineering is as much about judicious removal as creative addition.
\\
The engineered group and family features proved particularly valuable, with GroupSize, FamilySize, and the various mismatch flags (CryoSpendMismatch) appearing as high-importance features in tree-based models. This validates our hypothesis that the competition's narrative (families traveling together with correlated fates) should be explicitly encoded rather than left for models to discover from raw PassengerId strings.

\section{Conclusions}
This work demonstrates that genetic algorithms provide a viable framework for automated architecture search in tabular data classification tasks. By encoding five distinct model families within a unified genome representation and employing evolutionary operators to explore the joint space of model type, architecture, and hyperparameters, we achieved competitive performance (80.64\% accuracy) on the Kaggle Spaceship Titanic benchmark.
\\
Our iterative methodology, alternating between GA-driven architecture search and human-guided feature engineering, proved effective in progressively improving model performance. The final system combines the complementary strengths of neural networks (learning complex non-linear interactions) and tree-based ensembles (capturing heterogeneous feature importance and robust handling of mixed data types).
\\
The detailed analysis of evolved architectures reveals that the genetic algorithm discovered non-obvious solutions—such as asymmetric bottleneck MLPs and moderate-sized ExtraTrees ensembles—that may not have been prioritized in manual architecture design. This validates the exploration capability of evolutionary methods beyond mere hyperparameter tuning.

\subsection{Limitations}

Several limitations of this work warrant acknowledgment:

\noindent\textbf{1. Single-dataset evaluation}: Our experiments focus exclusively on one Kaggle competition. Generalization to other tabular datasets, particularly those with different characteristics (extreme class imbalance, high-dimensional sparse features, temporal dependencies), remains unvalidated. The architectural preferences discovered here may be task-specific.

\noindent\textbf{2. Absence of baseline comparisons}: We do not compare against alternative automated machine learning approaches (Bayesian optimization, random search, AutoML frameworks like Auto-sklearn or FLAML) or manual expert tuning. Consequently, we cannot definitively claim that genetic algorithms are superior—only that they are sufficient to achieve competitive results.

\noindent\textbf{3. Public leaderboard feedback}: Our iterative refinement process used Kaggle public leaderboard scores to guide feature engineering decisions. This introduces potential overfitting to the public test set and may overestimate true generalization performance. A more rigorous approach would use only cross-validation metrics for all decisions.

\noindent\textbf{4. Limited theoretical insight}: While we demonstrate empirical success, we provide little theoretical understanding of \emph{why} genetic algorithms work well for this problem or \emph{when} they should be preferred over alternatives. The work remains primarily empirical engineering.

\appendix

\section{Code Layout}

The code is publicly available on \href{https://github.com/icordos/titanic}{github}.
The main scripts are:
\begin{itemize}
	\item prepare\_data.py - downloads the training and test data from Kaggle and prepares it
	\item train\_ga\_nn.py - the actual genetic algorithm implementation, run the full ga loop based on training data and produces Kaggle submission files based on test data
\end{itemize}


\bibliographystyle{plainnat}  % Choose a bibliography style (plainnat, ieee, etc.)
\bibliography{references}  % Load the .bib file (without the .bib extension)

\end{document}
