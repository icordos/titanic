\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
%\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}        % math environments
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{titling}
\usepackage{graphicx} % Required for images
\usepackage{subcaption} 
\usepackage{placeins}
%\usepackage{subfig}
\usepackage{natbib} % For better citation handling

% IBM Plex fonts
\usepackage{plex-serif}   % For main text
\usepackage{plex-sans}    % For sans-serif
\usepackage{plex-mono}    % For monospaced


\title{Spaceship Titanic - Automated neural network architecture generation using genetic algorithms}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Cristian Cordoș\\
  National University of Science and Technology POLITEHNICA Bucharest, Romania\\
  \texttt{ioan.cordos@stud.acs.upb.ro}\\
   \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
In this work we are attempting to resolve and optimize a classical data science problem involving numerical data using deep neural networks automatically fine tuned by genetic algorithms.
\end{abstract}


\section{Introduction}

A few words about neural network architecture generation using genetic algorithms.
\\
\\
We chose a practical problem to research the domain of neural network generation through genetic algorithms. The problem is actually a Kaggle Challenge, Spaceship Titanic \cite{spaceship-titanic}. A newly launched space liner loaded with 13000 passengers encounters a spacetime anomaly that causes half of the passengers to be transported to an alternate dimension. The challenge is to predict which passengers were transported using records recovered from the spaceship's damaged computer system.

\section{Dataset Analysis}

\paragraph{Training Data}

The training data is distributed in tabular form as a csv file. It consists of 14 features and 8693 rows.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{csv.png}
    \caption{Training data view}
    \label{fig:csv}
\end{figure}
\FloatBarrier

The dataset is not perfectly clean, as seen in Figure \ref{fig:missing}, there are around 2\% missing cells per feature.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.22\textwidth]{missing.png}
    \caption{Training data view}
    \label{fig:missing}
\end{figure}
\FloatBarrier

In Figure \ref{fig:features} we can see a summary / statistic of the various features of the training data. We notice the transported feature, the one we need to predict, is pretty balanced (49.63\% vs 50.36\%).

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{categorical.png}
        \caption{Categorical features summary}
        \label{fig:categorical}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{numerical.png}
        \caption{Numerical features statistics}
        \label{fig:numerical}
    \end{subfigure}
    \caption{Dataset feature analysis}
    \label{fig:features}
\end{figure}

In the correlation matrix of the numerical features we notice strong correlation between Spa, VRDeck and FoodCourt spending and very weak correlations for the Transported feature.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{correlation.png}
    \caption{Correlation matrix of numerical features}
    \label{fig:correlation}
\end{figure}
\FloatBarrier

\paragraph{Test Data}
The test data is presented in the same format, and mirrors the training data except for the Transported feature, which is the one that needs prediction. It features 4277 rows that need prediction. It maintains approximately the same percentage of missing cells (2\%). Also the data distributions are similar with the ones in the training dataset as shown in Figure \ref{fig:test-features}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{test-categorical.png}
        \caption{Categorical features summary}
        \label{fig:categorical}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{test-numerical.png}
        \caption{Numerical features statistics}
        \label{fig:numerical}
    \end{subfigure}
    \caption{Test dataset feature analysis}
    \label{fig:test-features}
\end{figure}

\section{Feature Engineering}

We apply a number of transformations to the features, as follows:
\begin{itemize}
	\item \textbf{boolean normalization} - CryoSleep and VIP features are transformed from "True"/"False" to 1/0
	\item \textbf{spend clipping} - each spend column (RoomService, FoodCourt, ShoppingMall, Spa and VRDeck) is trimmed at the 99.5th percentile to eliminate the extreme outliers before aggregation or scaling
	\item \textbf{cabin decomposition} - the cabin field (Deck/Num/Side) is split into CabinDeck, CabinNum and CabinSide so the models can learn deck-level effects, longitudinal position and port vs starboard differences separately
	\item \textbf{passenger group features} - parses PassengerId into GroupId, GroupMemberNumber, GroupSize and IsGroupSolo. This exposes group structure (families/friends traveling together), which correlates with survival in the competition narrative
	\item \textbf{family featuers} - extracts surnames from Name, builds FamilySize, and flags IsFamilySolo. It captures broader kinship beyond the PassengerId grouping and differentiates solo travelers from larger families
	\item \textbf{spend aggregations} - creates TotalSpend, LogTotalSpend, NonZeroSpendCount, and AllZeroSpend to summarize how much passengers spend on amenities and whether they spend at all—useful because cryosleep travelers usually show zero spend.
	\item \textbf{per-passenger spend ratios} - adds SpendPerPassenger plus per-amenity usage flags (RoomServiceUsed, etc.) and spend shares (RoomServiceShare, …) so the model knows whether someone spends on specific services and what fraction of their budget each category takes.
	\item \textbf{cabin location cues} - maps CabinDeck to an ordinal DeckRank, encodes IsPortSide/IsStarboardSide, and buckets CabinNum into CabinZone bins (front/mid/rear/deep) to approximate spatial placement
	\item \textbf{group age statistics} - Computes each group’s mean/std/range of Age, adds per-passenger residuals (AgeMinusGroupMean), flags large families, and compares family vs group sizes (FamilyShareOfGroup, GroupMinusFamilySize, GroupOutnumbersFamily). These capture heterogeneity within traveling parties.
	\item \textbf{route feature}	 - concatenates HomePlanet and Destination, keeps the top ROUTE\_TOP\_K combos, and bins others into “Other”. This highlights popular travel routes that correlate with the target.
	\item \textbf{cryo vs spend mismatch} - Flags passengers whose cryosleep status conflicts with spend totals (e.g., CryoSleep=True but nonzero spend), helping the model exploit data inconsistencies that often correlate with outcomes.
\end{itemize}

Then we fill in the missing cells, with various strategies based on the column type:
\begin{itemize}
	\item \textbf{Age} - If HomePlanet is present, fill missing ages with the median age per planet; remaining gaps fall back to the global median
      	\item \textbf{CabinNum} - If CabinDeck exists, fill per-deck medians first, then the global median
      	\item \textbf{Spend columns (RoomService, …)} - If Destination exists, fill per-destination medians, then overall medians
	\item \textbf{Group/family metrics (GroupId, GroupMemberNumber, GroupSize, FamilySize)} - fill with the column median so engineered ratios remain finite.
      	\item \textbf{Boolean-like indicators (CryoSleep, VIP, IsGroupSolo, IsFamilySolo, AllZeroSpend)} - fill zeros to treat missing as “false”
	\item \textbf{String categoricals (CabinDeck, CabinSide, HomePlanet, Destination)} - fill "Unknown" so they still get one-hot encoded
\end{itemize}

After targeted fills, for numeric columns the remaining NaNs are filled with the column median and for categorical columns they filled with the column mode (most frequent value), defaulting to "Unknown" if a mode cannot be computed. In the last step the numerical columns are scaled using Z-score. This transformations, applied both to training and test data, blow the total number of features to 75, thus creating a much richer dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{prepared-correlation.png}
    \caption{Correlation matrix of prepared features}
    \label{fig:prepared-correlation}
\end{figure}
\FloatBarrier

As seen in Figure \ref{fig:prepared-correlation} there is correlation between transported and CryoSleep and AllZeroSpend.
\\
\\
Finally, the last step in data augumentation is to add 2 predictions, one based on logistic regression and the other based on a gradient-boosted stump model. They capture different decision boundaries than the genetic algorithm might find, and their outputs are often well-calibrated. By appending them as features, we let the genetic algorithm models learn from these baseline predictions (e.g., blending them, focusing on where they disagree) without having to run the baselines separately later.

\section{Genetic Algorithm Design}

Our GA explores five genome families, each with its own hyperparameter set:

\begin{enumerate}
    \item \textbf{Multi Layer Perceptron} (\texttt{model\_type="mlp"})
    \begin{itemize}
        \item \textit{Architecture genes:} number of layers (1--6), per-layer widths drawn from [16, 32, 64, 128, 256, 512], activation (relu, gelu, leaky\_relu), dropout rate, batch size, epoch count.
        \item \textit{Optimization genes:} optimizer (adam/adamw), learning rate ($5 \times 10^{-4}$ to $5 \times 10^{-3}$), weight decay (0 to $10^{-3}$).
        \item \textit{Training:} PyTorch sequential net with BatchNorm + optional Dropout per layer, BCEWithLogitsLoss, ReduceLROnPlateau scheduler, predictions via sigmoid.
    \end{itemize}
    
    \item \textbf{Gradient-Boosted Stumps (\citep{friedman2001gradientboosting})} (\texttt{model\_type="gbstump"})
    \begin{itemize}
        \item \textit{Genes:} number of estimators (100--300), learning rate (0.1--0.3), min leaf size (50--200).
        \item \textit{Implementation:} scikit-learn GradientBoostingClassifier forced to depth-1 trees (decision stumps). Outputs are calibrated via CalibratedClassifierCV (3-fold sigmoid) for reliable probabilities.
    \end{itemize}
    
    \item \textbf{Random Forest (\citep{breiman2001randomforest})} (\texttt{model\_type="rf"})
    \begin{itemize}
        \item \textit{Genes:} tree count (100--400), max depth (4--8), max features (0.5--0.9 fraction), min samples split (10--40), max\_samples (0.6--1.0).
        \item \textit{Implementation:} scikit-learn RandomForestClassifier with \texttt{n\_jobs=-1}. Predictions are likewise passed through CalibratedClassifierCV to smooth probabilities.
    \end{itemize}
    
    \item \textbf{ExtraTrees(\citep{geurts2006extratrees}) } (\texttt{model\_type="extratrees"})
    \begin{itemize}
        \item Shares the RF hyperparameters (tree count, depth, max\_features, min\_samples\_split, max\_samples), plus a \texttt{rf\_random\_thresholds} flag set true for this family.
        \item \textit{Implementation:} scikit-learn ExtraTreesClassifier (with bootstrap enabled when max\_samples $< 1.0$) and calibration wrapper.
    \end{itemize}
    
    \item \textbf{HistGradientBoosting (\citep{ke2017lightgbm})} (\texttt{model\_type="histgb"})
    \begin{itemize}
        \item \textit{Genes:} learning rate (0.03--0.1), max depth (3--5 or None), max leaf nodes (31/63/127), min samples leaf (20--100), L2 regularization (0.0--0.6).
        \item \textit{Implementation:} scikit-learn HistGradientBoostingClassifier, also wrapped in CalibratedClassifierCV.
    \end{itemize}
\end{enumerate}

Each genome type contributes different inductive biases: MLPs model high-order interactions, GBDT variants capture step-wise splits, RF/ExtraTrees add bagged diversity, and HistGB offers strong tabular performance with histogram-based splits.

Fitness evaluation uses either a single holdout split or stratified $k$-fold cross-validation ($k \in \{5,8\}$ in the experiments). For each genome $g$ we train on every fold, compute log loss $\ell_{g,i}$ and accuracy $a_{g,i}$, and define fitness as $f_g = -\tfrac{1}{k} \sum_i \ell_{g,i}$ so lower loss yields higher fitness. Tree models are wrapped in \texttt{CalibratedClassifierCV} to ensure well-calibrated probabilities before ensembling; MLPs are trained with BCEWithLogitsLoss, Adam/AdamW optimizers, batch normalization, dropout, and ReduceLROnPlateau schedulers. All fold metrics are logged to the console for transparency and later analysis.

Selection keeps the top $E$ elites every generation and refills the population with mutated descendants. ``Macro'' mutations can switch model families or redraw an entire architecture (e.g., number of layers), while ``micro'' mutations perturb continuous hyperparameters (learning rate, dropout, weight decay) or resample discrete values (trees, depth, min samples split). The evaluate $\rightarrow$ select $\rightarrow$ mutate loop repeats for $T$ generations, steadily improving the population's fitness while preserving diversity.

After evolution, the best $k$ genomes are retrained on the full training set. PyTorch MLPs are serialized via \texttt{state\_dict}, and scikit-learn models are stored directly. Each retrained model produces a Kaggle submission CSV along with metadata describing its validation log loss, fold metrics, submission path, and (when applicable) Kaggle score. These summaries drive subsequent stacking pipelines and allow future GA runs to restart from the strongest known configurations.

\section{Results}

The experimental run that produced the top scoring submission started with a long genetic algorithm sweep of population 15 across 40 generations, each genome being trained with the dataset split over 5 k-folds.

\begin{table}[h]
\centering
\label{tab:ga_results}
\begin{tabular}{|c|l|c|p{6cm}|c|}
\hline
\textbf{Rank} & \textbf{Model Type} & \textbf{Val Log Loss} & \textbf{Architecture Description} & \textbf{Kaggle Score} \\
\hline
1 & ExtraTrees & 0.3887 & 3 layers [512, 64, 128], 300 GB estimators, 0.005 LR & 0.80079 \\
\hline
2 & GB Stump & 0.3887 & 4 layers [256, 32, 16, 256], 100 GB estimators, 0.005 LR & 0.79565 \\
\hline
3 & ExtraTrees & 0.3887 & 3 layers [512, 512, 128], 150 GB estimators, 0.004 LR & 0.79752 \\
\hline
4 & ExtraTrees & 0.3887 & 3 layers [32, 64, 128], 300 GB estimators, 0.0045 LR & 0.79775 \\
\hline
5 & GB Stump & 0.3887 & 1 layer [16], 150 GB estimators, 0.005 LR & 0.79494 \\
\hline
6 & ExtraTrees & 0.3887 & 3 layers [128, 64, 128], 300 GB estimators, 0.005 LR & --- \\
\hline
7 & ExtraTrees & 0.3889 & 3 layers [32, 16, 128], 150 GB estimators, 0.005 LR & --- \\
\hline
8 & ExtraTrees & 0.3889 & 3 layers [32, 16, 64], 300 GB estimators, 0.004 LR & --- \\
\hline
9 & ExtraTrees & 0.3889 & 3 layers [16, 16, 128], 100 GB estimators, 0.0048 LR & --- \\
\hline
10 & Random Forest & 0.3889 & 3 layers [128, 64, 512], 200 RF trees, 0.005 LR & --- \\
\hline
\end{tabular}
\caption{Genetic Algorithm Hyperparameter Search Results}
\end{table}
\FloatBarrier

Then we pulled the best entries from the above run, generated out‑of‑fold predictions for those models and combined them with the two deterministic features (LR\_pred, GB\_pred) already baked into the training dataset. A logistic regression meta‑model (with standardization) was trained on the stacked OOF matrix and produced a new submission that scored 0.80149.
\\
After this we extended the genetic algorithm to include HistGradientBoosting genomes, we selected the previous top 10 models as seed, and we ran across 15 generations with a population of 15, each model trained over 8-folds.

\begin{table}[h]
\centering
\label{tab:ga_results_new}
\begin{tabular}{|c|l|c|p{6cm}|c|}
\hline
\textbf{Rank} & \textbf{Model Type} & \textbf{Val Log Loss} & \textbf{Architecture Description} & \textbf{Kaggle Score} \\
\hline
1 & ExtraTrees & 0.4002 & 3 layers [32, 16, 32], 300 GB estimators, 0.005 LR & 0.80079 \\
\hline
2 & ExtraTrees & 0.4002 & 3 layers [128, 16, 32], 300 GB estimators, 0.005 LR & 0.79845 \\
\hline
3 & ExtraTrees & 0.4002 & 3 layers [32, 64, 32], 150 GB estimators, 0.0045 LR & 0.79869 \\
\hline
\end{tabular}
\caption{Genetic Algorithm Hyperparameter Search Results}
\end{table}
\FloatBarrier

Individual submissions clustered around 0.799 and the best genome and stacker blend reached 0.80266.
\\
For the next step we returned to feature engineering and added:
\begin{itemize}
	\item CabinDeckEmbedSin \ CabinDeckEmbedCos – sine/cosine projections of the deck rank (A–T normalized) so the model sees relative deck position without treating the ordinal index as linear.
      	\item CabinNumEmbedSin / CabinNumEmbedCos – normalized cabin-number projections (sin/cos over the longitudinal position) to capture spatial cycles down the ship.
      	\item CabinDeckSideInteraction – deck rank multiplied by a side indicator (port vs starboard) so asymmetries across the hull can be learned.
	\item Spend-share cleanup: After computing per-service usage flags and shares, we dropped the weak RoomServiceShare/FoodCourtShare/… columns entirely, keeping only the stronger aggregates (TotalSpend, SpendPerPassenger, per-service usage booleans). This removed noisy ratios that weren’t helping the GA while preserving the informative spend signals.
\end{itemize}

  
\begin{table}[h]
\centering
\label{tab:ga_results_mlp}
\begin{tabular}{|c|l|c|p{6cm}|c|}
\hline
\textbf{Rank} & \textbf{Model Type} & \textbf{Val Log Loss} & \textbf{Architecture Description} & \textbf{Kaggle Score} \\
\hline
1 & MLP & 0.3907 & 1 layer [32], 300 GB estimators, 0.0046 LR & 0.80430 \\
\hline
2 & MLP & 0.3907 & 1 layer [128], 200 GB estimators, 0.0041 LR & --- \\
\hline
\end{tabular}
\caption{Genetic Algorithm Hyperparameter Search Results}
\end{table}
\FloatBarrier

Those changes, along with rerunning the pipeline for 10 generations with a population of 12 using 5-folds, gave a richer cabin representation and a cleaner spend feature set, enabling the Day 3 GA run to reach 0.8043.
\\
In the final step we use the top 15 models from previous experiments and we orchestrate a longer GA search of 20 generations with a population of 18 over 8-folds.

\begin{table}[h]
\centering
\label{tab:ga_results_day5}
\begin{tabular}{|c|l|c|p{6cm}|c|}
\hline
\textbf{Rank} & \textbf{Model Type} & \textbf{Val Log Loss} & \textbf{Architecture Description} & \textbf{Kaggle Score} \\
\hline
1 & MLP & 0.3878 & 6 layers [64, 32, 16, 32, 256, 16], 100 GB estimators, 0.0046 LR & 0.80640 \\
\hline
2 & Random Forest & 0.3878 & 4 layers [128, 16, 256, 16], 100 GB estimators, 0.005 LR & --- \\
\hline
3 & MLP & 0.3878 & 6 layers [64, 32, 16, 128, 256, 16], 200 GB estimators, 0.005 LR & --- \\
\hline
4 & MLP & 0.3878 & 6 layers [64, 32, 16, 32, 256, 16], 150 GB estimators, 0.005 LR & --- \\
\hline
5 & MLP & 0.3878 & 6 layers [64, 32, 16, 32, 256, 512], 200 GB estimators, 0.0048 LR & --- \\
\hline
\end{tabular}
\caption{Genetic Algorithm Hyperparameter Search Results}
\end{table}
\FloatBarrier

\subsection{Best Submission - Complete Parameter Specification}

The best Kaggle score is \textbf{0.80640}. The complete parameter specification is as follows:

\subsubsection{Model Overview}

The top-performing model is a \textbf{multi-layer perceptron (MLP)} ensemble that achieved a validation log loss of 0.3878 and a Kaggle public score of 0.80640.

\subsubsection{Neural Network Architecture}

\begin{itemize}
    \item Model type: MLP
    \item Number of layers: 6
    \item Layer widths: [64, 32, 16, 32, 256, 16]
    \item Activation function: Leaky ReLU
    \item Dropout rate: 0.2254 (22.54\%)
\end{itemize}

\subsubsection{Training Configuration}

\begin{itemize}
    \item Optimizer: Adam
    \item Learning rate: 0.004603
    \item Weight decay: 0.0 (no L2 regularization on neural network)
    \item Batch size: 256
    \item Training epochs: 20
\end{itemize}

\subsubsection{Gradient Boosting Parameters}

\begin{itemize}
    \item Number of estimators: 100
    \item Learning rate: 0.1
    \item Minimum samples per leaf: 100
\end{itemize}

\subsubsection{Random Forest Parameters}

\begin{itemize}
    \item Number of trees: 400
    \item Maximum tree depth: 8
    \item Maximum features per split: 0.5 (50\%)
    \item Minimum samples for split: 20
    \item Sample ratio: 0.6 (60\% bootstrap sampling)
    \item Random thresholds: False
\end{itemize}

\subsubsection{Histogram-based Gradient Boosting Parameters}

\begin{itemize}
    \item Learning rate: 0.0341
    \item Maximum depth: 5
    \item Maximum leaf nodes: 31
    \item Minimum samples per leaf: 20
    \item L2 regularization: 0.0
\end{itemize}

In summary, this model represents the convergence of the genetic algorithm search after multiple iterations, with the 6-layer MLP architecture proving most effective when combined with moderate ensemble sizes and carefully tuned regularization through dropout.

The figure below shows how the Kaggle score evolves across the 4 GA runs.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{score-evolution.png}
    \caption{Kaggle score evolution across GA generations}
    \label{fig:csv}
\end{figure}
\FloatBarrier

\section{Conclusions and Future Steps}

\appendix

\section{Code Layout}

The code is publicly available on \href{https://github.com/icordos/titanic}{github}.
The main scripts are:
\begin{itemize}
	\item prepare\_data.py - downloads the training and test data from Kaggle and prepares it
	\item train\_ga\_nn.py - the actual genetic algorithm implementation, run the full ga loop based on training data and produces Kaggle submission files based on test data
\end{itemize}


\bibliographystyle{plainnat}  % Choose a bibliography style (plainnat, ieee, etc.)
\bibliography{references}  % Load the .bib file (without the .bib extension)

\end{document}
